{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Setup Guide\n",
    "\n",
    "This notebook provides a comprehensive guide to setting up and using vLLM, a high-throughput and memory-efficient inference and serving library for LLMs.\n",
    "\n",
    "## What is vLLM?\n",
    "\n",
    "vLLM is an open-source library designed for:\n",
    "- Fast LLM inference and serving\n",
    "- Efficient memory management with PagedAttention\n",
    "- Continuous batching for high throughput\n",
    "- OpenAI-compatible API server\n",
    "\n",
    "## System Requirements\n",
    "\n",
    "- **Operating System**: Linux\n",
    "- **Python**: 3.9 - 3.12\n",
    "- **Hardware**: NVIDIA GPUs (recommended)\n",
    "- **CUDA**: Compatible version for your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Option 1: Using uv (Recommended)\n",
    "\n",
    "uv is a fast Python package manager that's recommended for vLLM installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install uv if you haven't already\n",
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and activate a virtual environment\n",
    "!uv venv --python 3.12\n",
    "# Note: In Jupyter, you'll need to restart the kernel and select the new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM with automatic torch backend selection\n",
    "!uv pip install vllm --torch-backend=auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new conda environment\n",
    "# Run this in terminal:\n",
    "# conda create -n vllm_env python=3.12 -y\n",
    "# conda activate vllm_env\n",
    "\n",
    "# Then install vLLM\n",
    "!pip install vllm --torch-backend=auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Using pip (in existing environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct pip install\n",
    "!pip install vllm --torch-backend=auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify vLLM installation\n",
    "import vllm\n",
    "print(f\"vLLM version: {vllm.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Inference\n",
    "\n",
    "Let's start with basic offline inference using vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Define prompts\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\"\n",
    "]\n",
    "\n",
    "# Set sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Initialize the model (using a small model for demo)\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "\n",
    "# Generate outputs\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Larger Models\n",
    "\n",
    "vLLM supports a wide range of models from Hugging Face. Here's how to use larger, more capable models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with a more capable model (adjust based on your GPU memory)\n",
    "# Uncomment and run based on your GPU capacity:\n",
    "\n",
    "# For 8GB GPU:\n",
    "# llm = LLM(model=\"microsoft/phi-2\")\n",
    "\n",
    "# For 16GB GPU:\n",
    "# llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "# For 24GB+ GPU:\n",
    "# llm = LLM(model=\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "\n",
    "# Example with Mistral 7B (requires ~16GB GPU memory)\n",
    "try:\n",
    "    llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "    \n",
    "    # Chat-style prompt\n",
    "    prompts = [\n",
    "        \"[INST] Explain quantum computing in simple terms. [/INST]\"\n",
    "    ]\n",
    "    \n",
    "    sampling_params = SamplingParams(temperature=0.7, max_tokens=200)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    \n",
    "    print(outputs[0].outputs[0].text)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Try a smaller model based on your GPU memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Sampling Parameters\n",
    "\n",
    "vLLM offers various sampling parameters to control generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating different sampling strategies\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "\n",
    "prompt = \"The weather today is\"\n",
    "\n",
    "# Greedy decoding (deterministic)\n",
    "greedy_params = SamplingParams(temperature=0, max_tokens=30)\n",
    "\n",
    "# Creative sampling\n",
    "creative_params = SamplingParams(\n",
    "    temperature=1.2,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_tokens=30\n",
    ")\n",
    "\n",
    "# Beam search\n",
    "beam_params = SamplingParams(\n",
    "    use_beam_search=True,\n",
    "    best_of=3,\n",
    "    max_tokens=30\n",
    ")\n",
    "\n",
    "# Generate with different strategies\n",
    "print(\"Greedy decoding:\")\n",
    "print(llm.generate([prompt], greedy_params)[0].outputs[0].text)\n",
    "print(\"\\nCreative sampling:\")\n",
    "print(llm.generate([prompt], creative_params)[0].outputs[0].text)\n",
    "print(\"\\nBeam search:\")\n",
    "print(llm.generate([prompt], beam_params)[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-Compatible API Server\n",
    "\n",
    "vLLM can serve models with an OpenAI-compatible API, making it easy to integrate with existing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the Server\n",
    "\n",
    "To start the vLLM server, run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "# Basic server start\n",
    "vllm serve facebook/opt-125m\n",
    "\n",
    "# With custom options\n",
    "vllm serve facebook/opt-125m --port 8000 --host 0.0.0.0\n",
    "\n",
    "# For a chat model\n",
    "vllm serve mistralai/Mistral-7B-Instruct-v0.1 --chat-template\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the API (run after starting the server)\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# API endpoint\n",
    "url = \"http://localhost:8000/v1/completions\"\n",
    "\n",
    "# Request payload\n",
    "payload = {\n",
    "    \"model\": \"facebook/opt-125m\",\n",
    "    \"prompt\": \"The meaning of life is\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Make request (uncomment when server is running)\n",
    "# response = requests.post(url, json=payload)\n",
    "# print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI Python client with vLLM\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to local vLLM server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"dummy-key\"  # vLLM doesn't require an API key\n",
    ")\n",
    "\n",
    "# Use it like OpenAI API (uncomment when server is running)\n",
    "# completion = client.completions.create(\n",
    "#     model=\"facebook/opt-125m\",\n",
    "#     prompt=\"Once upon a time\",\n",
    "#     max_tokens=50\n",
    "# )\n",
    "# print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Backend Configuration\n",
    "\n",
    "vLLM supports multiple attention backends for optimal performance on different hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check current attention backend\n",
    "current_backend = os.environ.get('VLLM_ATTENTION_BACKEND', 'auto')\n",
    "print(f\"Current attention backend: {current_backend}\")\n",
    "\n",
    "# Available backends:\n",
    "# - FLASH_ATTN: FlashAttention-2 backend\n",
    "# - FLASHINFER: FlashInfer backend  \n",
    "# - XFORMERS: xFormers backend\n",
    "# - ROCM_FLASH: ROCm flash attention\n",
    "# - auto: Automatic selection (default)\n",
    "\n",
    "# To set a specific backend:\n",
    "# os.environ['VLLM_ATTENTION_BACKEND'] = 'FLASH_ATTN'\n",
    "\n",
    "# Example with specific backend\n",
    "# llm = LLM(model=\"facebook/opt-125m\")\n",
    "# This will use the backend specified in the environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization Tips\n",
    "\n",
    "1. **GPU Memory Management**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control GPU memory usage\n",
    "llm = LLM(\n",
    "    model=\"facebook/opt-125m\",\n",
    "    gpu_memory_utilization=0.9,  # Use 90% of GPU memory\n",
    "    max_model_len=2048  # Limit context length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Tensor Parallelism** (for multi-GPU systems):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multi-GPU systems\n",
    "# llm = LLM(\n",
    "#     model=\"meta-llama/Llama-2-13b-hf\",\n",
    "#     tensor_parallel_size=2  # Use 2 GPUs\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Quantization** (reduce memory usage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quantized models\n",
    "# llm = LLM(\n",
    "#     model=\"TheBloke/Llama-2-7B-AWQ\",  # AWQ quantized model\n",
    "#     quantization=\"awq\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Common Issues\n",
    "\n",
    "### 1. CUDA Out of Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions for OOM errors:\n",
    "\n",
    "# 1. Reduce batch size\n",
    "llm = LLM(model=\"facebook/opt-125m\", max_num_seqs=1)\n",
    "\n",
    "# 2. Reduce memory utilization\n",
    "llm = LLM(model=\"facebook/opt-125m\", gpu_memory_utilization=0.7)\n",
    "\n",
    "# 3. Use smaller model or quantized version\n",
    "# 4. Reduce max_model_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check System Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA and GPU info\n",
    "import subprocess\n",
    "\n",
    "# Check CUDA version\n",
    "try:\n",
    "    cuda_version = subprocess.check_output(['nvcc', '--version']).decode('utf-8')\n",
    "    print(\"CUDA Version:\")\n",
    "    print(cuda_version)\n",
    "except:\n",
    "    print(\"CUDA not found in PATH\")\n",
    "\n",
    "# Check GPU info\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv']).decode('utf-8')\n",
    "    print(\"\\nGPU Info:\")\n",
    "    print(gpu_info)\n",
    "except:\n",
    "    print(\"nvidia-smi not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Explore Model Zoo**: Browse Hugging Face for models compatible with vLLM\n",
    "2. **Production Deployment**: Set up vLLM with proper monitoring and scaling\n",
    "3. **Custom Models**: Learn to serve your fine-tuned models\n",
    "4. **Integration**: Connect vLLM to your applications via the OpenAI-compatible API\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [vLLM GitHub](https://github.com/vllm-project/vllm)\n",
    "- [Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)\n",
    "- [Performance Guide](https://docs.vllm.ai/en/latest/models/performance.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}