{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Agents: From Structured Outputs to Tool Calling\n",
    "\n",
    "In this notebook, we'll build up to creating a local AI agent step by step:\n",
    "\n",
    "1. **Structured Outputs** - Getting reliable JSON from LLMs using Pydantic\n",
    "2. **Tool/Function Calling** - Having the LLM decide which functions to call\n",
    "3. **Complete Agent Loop** - Putting it all together into an agentic workflow\n",
    "\n",
    "All examples run locally using [Ollama](https://ollama.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ollama pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Structured Outputs with Pydantic\n",
    "\n",
    "The foundation of reliable agents is getting **structured, predictable outputs** from LLMs.\n",
    "\n",
    "Instead of parsing free-form text, we can force the LLM to return valid JSON that matches our schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Task Generation\n",
    "\n",
    "Let's define a `Task` model and have the LLM generate a realistic task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    priority: str  # \"High\", \"Medium\", \"Low\"\n",
    "    estimated_hours: float\n",
    "    dependencies: List[str] = []\n",
    "    status: str = \"Not Started\"  # \"Not Started\", \"In Progress\", \"Completed\"\n",
    "    assigned_to: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '''Create a task for implementing a new feature in our project management software \n",
    "            that allows users to track time spent on tasks. Include dependencies and make it realistic.''',\n",
    "        }\n",
    "    ],\n",
    "    model='mistral-small3.2',\n",
    "    format=Task.model_json_schema(),  # This forces structured output!\n",
    ")\n",
    "\n",
    "# Validate and parse the response\n",
    "task = Task.model_validate_json(response['message']['content'])\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Task: {task.title}\")\n",
    "print(f\"Priority: {task.priority}\")\n",
    "print(f\"Estimated Hours: {task.estimated_hours}\")\n",
    "print(f\"Dependencies: {', '.join(task.dependencies)}\")\n",
    "print(f\"Status: {task.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Email Tool Input\n",
    "\n",
    "Structured outputs are perfect for extracting tool inputs from natural language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailToolInput(BaseModel):\n",
    "    email_destination: str\n",
    "    email_contents: str\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write an email to my boss: boss_of_lucas@gmail.com, telling him that I quit to pursue bird watching.\n",
    "\"\"\"\n",
    "\n",
    "email_response = chat(\n",
    "    model='mistral-small3.2',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    format=EmailToolInput.model_json_schema(),\n",
    ")\n",
    "\n",
    "email_input = EmailToolInput.model_validate_json(email_response['message']['content'])\n",
    "print(f\"To: {email_input.email_destination}\")\n",
    "print(f\"\\n{email_input.email_contents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we could use this structured data to actually send an email!\n",
    "def send_email(email_destination: str, email_contents: str):\n",
    "    print(\"[SIMULATED] Sending email...\")\n",
    "    print(f\"To: {email_destination}\")\n",
    "    print(f\"Contents: {email_contents}\")\n",
    "\n",
    "send_email(email_input.email_destination, email_input.email_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Native Tool/Function Calling\n",
    "\n",
    "Ollama supports **native tool calling** - the LLM can decide which function to call and with what arguments.\n",
    "\n",
    "This is more powerful than structured outputs because the LLM chooses the tool dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Tool\n",
    "\n",
    "Let's create a simple tool that reads information from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a sample file with some information\n",
    "with open(\"lucas_secrets.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"Name: Lucas Soares\n",
    "Profession: Software Engineer\n",
    "Favorite Movie: Inception\n",
    "Favorite Book: The Name of the Wind\"\"\")\n",
    "\n",
    "print(\"Created lucas_secrets.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lucas_info(file_path: str) -> str:\n",
    "    \"\"\"Read information about Lucas from a file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Test it\n",
    "get_lucas_info(\"lucas_secrets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the LLM Decide to Use the Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"\"\"\n",
    "What is Lucas's profession, favorite movie, and favorite book?\n",
    "Use the information from the file: lucas_secrets.txt\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='mistral-small3.2',\n",
    "    messages=[{'role': 'user', 'content': input_prompt}],\n",
    "    tools=[{\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'get_lucas_info',\n",
    "            'description': 'Get information about Lucas from a file',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'file_path': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'The path to the file containing information about Lucas',\n",
    "                    },\n",
    "                },\n",
    "                'required': ['file_path'],\n",
    "            },\n",
    "        },\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(\"Tool calls requested by LLM:\")\n",
    "print(response['message']['tool_calls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM understood the request and decided to call `get_lucas_info` with the correct file path!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Complete Agent Loop\n",
    "\n",
    "Now let's put it all together into a complete **agentic workflow**:\n",
    "\n",
    "```\n",
    "1. User provides input\n",
    "2. LLM decides which tool(s) to call\n",
    "3. We execute the tool(s)\n",
    "4. We feed the results back to the LLM\n",
    "5. LLM generates the final response\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool registry - maps function names to actual functions\n",
    "TOOLS = {\n",
    "    'get_lucas_info': get_lucas_info,\n",
    "}\n",
    "\n",
    "def execute_tool_call(tool_call):\n",
    "    \"\"\"Execute a tool call and return the result.\"\"\"\n",
    "    func_name = tool_call['function']['name']\n",
    "    func_args = tool_call['function']['arguments']\n",
    "    \n",
    "    if func_name in TOOLS:\n",
    "        return TOOLS[func_name](**func_args)\n",
    "    else:\n",
    "        return f\"Error: Tool '{func_name}' not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_input: str, tools_schema: list) -> str:\n",
    "    \"\"\"\n",
    "    Run a simple agent loop:\n",
    "    1. Send user input to LLM with available tools\n",
    "    2. If LLM requests tool calls, execute them\n",
    "    3. Send tool results back to LLM for final response\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Step 1: Initial LLM call\n",
    "    response = ollama.chat(\n",
    "        model='mistral-small3.2',\n",
    "        messages=[{'role': 'user', 'content': user_input}],\n",
    "        tools=tools_schema,\n",
    "    )\n",
    "    \n",
    "    # Step 2: Check if tools were called\n",
    "    tool_calls = response['message'].get('tool_calls', [])\n",
    "    \n",
    "    if not tool_calls:\n",
    "        # No tools needed, return direct response\n",
    "        return response['message']['content']\n",
    "    \n",
    "    # Step 3: Execute tool calls\n",
    "    print(\"Agent is calling tools...\")\n",
    "    tool_results = []\n",
    "    for tool_call in tool_calls:\n",
    "        func_name = tool_call['function']['name']\n",
    "        print(f\"  -> Calling: {func_name}\")\n",
    "        result = execute_tool_call(tool_call)\n",
    "        tool_results.append(result)\n",
    "        print(f\"  <- Result: {result[:100]}...\" if len(result) > 100 else f\"  <- Result: {result}\")\n",
    "    \n",
    "    # Step 4: Send results back to LLM\n",
    "    combined_results = \"\\n\".join(tool_results)\n",
    "    followup_prompt = f\"\"\"\n",
    "The user asked: {user_input}\n",
    "\n",
    "You called tools and got these results:\n",
    "{combined_results}\n",
    "\n",
    "Now provide a helpful response to the user based on this information.\n",
    "\"\"\"\n",
    "    \n",
    "    final_response = ollama.chat(\n",
    "        model='mistral-small3.2',\n",
    "        messages=[{'role': 'user', 'content': followup_prompt}],\n",
    "    )\n",
    "    \n",
    "    return final_response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our tools schema\n",
    "tools_schema = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_lucas_info',\n",
    "        'description': 'Get information about Lucas from a file',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'file_path': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The path to the file containing information about Lucas',\n",
    "                },\n",
    "            },\n",
    "            'required': ['file_path'],\n",
    "        },\n",
    "    },\n",
    "}]\n",
    "\n",
    "# Run the agent!\n",
    "result = run_agent(\n",
    "    \"What is Lucas's profession, favorite movie, and favorite book? Check lucas_secrets.txt\",\n",
    "    tools_schema\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Final Response:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "We've covered the building blocks of local AI agents:\n",
    "\n",
    "| Concept | Purpose | Ollama Feature |\n",
    "|---------|---------|----------------|\n",
    "| **Structured Outputs** | Get predictable JSON from LLMs | `format=Schema.model_json_schema()` |\n",
    "| **Tool Calling** | Let LLM decide which functions to call | `tools=[...]` parameter |\n",
    "| **Agent Loop** | Complete workflow with tool execution | Combine both + execution logic |\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Pydantic + `format`** = Reliable structured outputs\n",
    "2. **`tools` parameter** = LLM-driven function calling\n",
    "3. **Agent loop** = User input -> LLM -> Tools -> LLM -> Response\n",
    "\n",
    "This pattern scales to more complex agents with multiple tools, memory, and planning capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
