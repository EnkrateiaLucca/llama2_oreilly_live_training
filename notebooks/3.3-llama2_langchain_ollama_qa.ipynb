{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://python.langchain.com/docs/integrations/llms/ollama#:~:text=as%20a%20herd.%22)-,rag\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial intelligence (AI) has a rich and varied history that spans several decades. październik Since the 1950s, when researchers first explored the possibility of creating machines that could think and learn like humans, AI has made significant progress. Here is an overview of some of the key events in the history of AI:\n",
      "\n",
      "1. 1950s: The Dartmouth Conference - The field of AI was formally established at a conference held at Dartmouth College in 1956. Attendees included computer scientists, mathematicians, and cognitive scientists who were interested in exploring the possibilities of creating machines that could simulate human intelligence.\n",
      "2. 1951: The Turing Test - British mathematician Alan Turing proposed a test to measure a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. The Turing Test has since become a benchmark for measuring the success of AI systems.\n",
      "3. 1956: The First AI Program - Computer scientist John McCarthy created the first AI program, called the Logical Theorist, which was designed to reason and solve problems using logical deduction.\n",
      "4. 1960s: Rule-Based Expert Systems - The development of rule-based expert systems, which used a set of rules to reason and make decisions, marked a significant milestone in the history of AI. These systems were widely used in industries such as banking and healthcare.\n",
      "5. 1970s: Machine Learning -Machine learning, which enables machines to learn from data without being explicitly programmed, emerged as a major area of research in AI. This led to the development of algorithms such as decision trees and neural networks.\n",
      "6. 1980s: Expert Systems - The development of expert systems, which were designed to mimic the decision-making abilities of human experts, reached its peak in the 1980s. These systems were widely used in industries such as banking and healthcare.\n",
      "7. 1990s: AI Winter - Despite significant advances in AI research, the field experienced a decline in funding and interest in the 1990s, known as the \"AI winter.\"\n",
      "8. 2000s: Machine Learning Revival - The revival of machine learning in the 2000s led to the development of powerful algorithms such as support vector machines and deep learning. This resurgence of interest in AI has continued into the present day.\n",
      "9. 2010s: AI Breakthroughs - In recent years, there have been several breakthroughs in AI, including the development of AlphaGo, a computer program that defeated a human champion in the game of Go, and the creation of deep learning algorithms that can perform tasks such as image recognition and natural language processing.\n",
      "10. Present Day: AI Continues to Evolve - Today, AI is continuing to evolve at a rapid pace, with new technologies and applications emerging all the time. From self-driving cars to personal assistants like Siri and Alexa, AI is becoming increasingly integrated into our daily lives.\n",
      "\n",
      "Some of the key players in the history of AI include:\n",
      "\n",
      "1. Alan Turing - British mathematician and computer scientist who proposed the Turing Test and developed the concept of the universal Turing machine.\n",
      "2. Marvin Minsky - American cognitive scientist and co-founder of the MIT Artificial Intelligence Laboratory, who made significant contributions to the field of neural networks.\n",
      "3. John McCarthy - American computer scientist and cognitive scientist who created the Lisp programming language and coined the term \"artificial intelligence.\"\n",
      "4. Alan Kay - American computer scientist and cognitive scientist who developed the concept of the personal computer and the Smalltalk programming language.\n",
      "5. Edward Feigenbaum - American computer scientist and cognitive scientist who developed the first expert system, called the ELIZA program, and made significant contributions to the field of artificial intelligence.\n",
      "6. Douglas Lenat - American computer scientist and cognitive scientist who developed the Cyc system, a large-scale knowledge base and reasoning system designed to mimic human common sense.\n",
      "7. Rodney Brooks - American roboticist and computer scientist who developed the Roomba robot and made significant contributions to the field of autonomous robots.\n",
      "8. Andrew Ng - Chinese-American computer scientist and entrepreneur who co-founded Google Brain and developed the deep learning algorithm known as the \"NgNet.\"\n",
      "9. Yann LeCun - French computer scientist and cognitive scientist who is director of AI Research at Facebook and made significant contributions to the field of neural networks.\n",
      "10. Geoffrey Hinton - British-Canadian computer scientist and cognitive scientist who developed the backpropagation algorithm and made significant contributions to the field of deep learning."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Artificial intelligence (AI) has a rich and varied history that spans several decades. październik Since the 1950s, when researchers first explored the possibility of creating machines that could think and learn like humans, AI has made significant progress. Here is an overview of some of the key events in the history of AI:\\n\\n1. 1950s: The Dartmouth Conference - The field of AI was formally established at a conference held at Dartmouth College in 1956. Attendees included computer scientists, mathematicians, and cognitive scientists who were interested in exploring the possibilities of creating machines that could simulate human intelligence.\\n2. 1951: The Turing Test - British mathematician Alan Turing proposed a test to measure a machine\\'s ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. The Turing Test has since become a benchmark for measuring the success of AI systems.\\n3. 1956: The First AI Program - Computer scientist John McCarthy created the first AI program, called the Logical Theorist, which was designed to reason and solve problems using logical deduction.\\n4. 1960s: Rule-Based Expert Systems - The development of rule-based expert systems, which used a set of rules to reason and make decisions, marked a significant milestone in the history of AI. These systems were widely used in industries such as banking and healthcare.\\n5. 1970s: Machine Learning -Machine learning, which enables machines to learn from data without being explicitly programmed, emerged as a major area of research in AI. This led to the development of algorithms such as decision trees and neural networks.\\n6. 1980s: Expert Systems - The development of expert systems, which were designed to mimic the decision-making abilities of human experts, reached its peak in the 1980s. These systems were widely used in industries such as banking and healthcare.\\n7. 1990s: AI Winter - Despite significant advances in AI research, the field experienced a decline in funding and interest in the 1990s, known as the \"AI winter.\"\\n8. 2000s: Machine Learning Revival - The revival of machine learning in the 2000s led to the development of powerful algorithms such as support vector machines and deep learning. This resurgence of interest in AI has continued into the present day.\\n9. 2010s: AI Breakthroughs - In recent years, there have been several breakthroughs in AI, including the development of AlphaGo, a computer program that defeated a human champion in the game of Go, and the creation of deep learning algorithms that can perform tasks such as image recognition and natural language processing.\\n10. Present Day: AI Continues to Evolve - Today, AI is continuing to evolve at a rapid pace, with new technologies and applications emerging all the time. From self-driving cars to personal assistants like Siri and Alexa, AI is becoming increasingly integrated into our daily lives.\\n\\nSome of the key players in the history of AI include:\\n\\n1. Alan Turing - British mathematician and computer scientist who proposed the Turing Test and developed the concept of the universal Turing machine.\\n2. Marvin Minsky - American cognitive scientist and co-founder of the MIT Artificial Intelligence Laboratory, who made significant contributions to the field of neural networks.\\n3. John McCarthy - American computer scientist and cognitive scientist who created the Lisp programming language and coined the term \"artificial intelligence.\"\\n4. Alan Kay - American computer scientist and cognitive scientist who developed the concept of the personal computer and the Smalltalk programming language.\\n5. Edward Feigenbaum - American computer scientist and cognitive scientist who developed the first expert system, called the ELIZA program, and made significant contributions to the field of artificial intelligence.\\n6. Douglas Lenat - American computer scientist and cognitive scientist who developed the Cyc system, a large-scale knowledge base and reasoning system designed to mimic human common sense.\\n7. Rodney Brooks - American roboticist and computer scientist who developed the Roomba robot and made significant contributions to the field of autonomous robots.\\n8. Andrew Ng - Chinese-American computer scientist and entrepreneur who co-founded Google Brain and developed the deep learning algorithm known as the \"NgNet.\"\\n9. Yann LeCun - French computer scientist and cognitive scientist who is director of AI Research at Facebook and made significant contributions to the field of neural networks.\\n10. Geoffrey Hinton - British-Canadian computer scientist and cognitive scientist who developed the backpropagation algorithm and made significant contributions to the field of deep learning.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Tell me about the history of AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.24175570905208588,\n",
       " -0.014599747024476528,\n",
       " 0.42253026366233826,\n",
       " 0.18622802197933197,\n",
       " 0.18547432124614716,\n",
       " -0.026411332190036774,\n",
       " 0.2060837596654892,\n",
       " 0.3480953574180603,\n",
       " -0.19985777139663696,\n",
       " 0.23087435960769653,\n",
       " 0.2692231833934784,\n",
       " -0.19024747610092163,\n",
       " 0.036673154681921005,\n",
       " -0.08839139342308044,\n",
       " -0.1324239820241928,\n",
       " -0.05830953270196915,\n",
       " 0.25267907977104187,\n",
       " 0.1721261590719223,\n",
       " 0.16267246007919312,\n",
       " 0.021371368318796158,\n",
       " 0.06245815381407738,\n",
       " -0.08952192217111588,\n",
       " -0.1090797707438469,\n",
       " -0.34464240074157715,\n",
       " 0.13248425722122192,\n",
       " -0.07076163589954376,\n",
       " 0.33955925703048706,\n",
       " 0.07396332174539566,\n",
       " 0.0006133475108072162,\n",
       " 0.10686536133289337,\n",
       " 0.016076665371656418,\n",
       " -0.002866720547899604,\n",
       " 0.0811944380402565,\n",
       " 0.0003226141561754048,\n",
       " -0.03801163658499718,\n",
       " 0.012929367832839489,\n",
       " 0.07142987847328186,\n",
       " -0.04902137070894241,\n",
       " -0.0038107100408524275,\n",
       " 0.1022312194108963,\n",
       " -0.04484926909208298,\n",
       " 0.05128607153892517,\n",
       " -0.02099519409239292,\n",
       " 0.08373233675956726,\n",
       " 0.36039966344833374,\n",
       " 0.2155286818742752,\n",
       " -0.1971374750137329,\n",
       " -0.1326359063386917,\n",
       " 0.02001647651195526,\n",
       " 0.034521568566560745,\n",
       " 0.15220825374126434,\n",
       " -0.29532748460769653,\n",
       " 0.23284171521663666,\n",
       " -0.25151437520980835,\n",
       " -0.14666667580604553,\n",
       " -0.0028105315286666155,\n",
       " 0.06398050487041473,\n",
       " 0.2822498679161072,\n",
       " -0.20186148583889008,\n",
       " 0.06762901693582535,\n",
       " -0.13994985818862915,\n",
       " 0.16786551475524902,\n",
       " -0.18426255881786346,\n",
       " -0.18435382843017578,\n",
       " 0.004392804112285376,\n",
       " 0.03965960815548897,\n",
       " 0.06039050593972206,\n",
       " -0.2605542540550232,\n",
       " -0.14245080947875977,\n",
       " 0.1420479416847229,\n",
       " -0.301683634519577,\n",
       " 0.014556937851011753,\n",
       " -0.295634001493454,\n",
       " -0.09171499311923981,\n",
       " -0.0021453392691910267,\n",
       " 0.08892539888620377,\n",
       " -0.04774268716573715,\n",
       " -0.11415912955999374,\n",
       " -0.04061637073755264,\n",
       " 0.003647118341177702,\n",
       " 0.2470860332250595,\n",
       " 0.20778682827949524,\n",
       " 0.20312853157520294,\n",
       " -0.30349984765052795,\n",
       " -0.04415088891983032,\n",
       " 0.046190980821847916,\n",
       " -0.10841133445501328,\n",
       " 0.05664278566837311,\n",
       " 0.0628095269203186,\n",
       " -0.16719476878643036,\n",
       " 0.15730799734592438,\n",
       " 0.001935539417900145,\n",
       " -0.05947449058294296,\n",
       " -0.07191881537437439,\n",
       " 0.5585094094276428,\n",
       " 0.1943245679140091,\n",
       " -0.17541611194610596,\n",
       " -0.0777096375823021,\n",
       " -0.19661876559257507,\n",
       " -0.1380895972251892,\n",
       " 0.005278230179101229,\n",
       " 0.32870247960090637,\n",
       " 0.21664628386497498,\n",
       " 0.1710616946220398,\n",
       " 0.12233231216669083,\n",
       " -0.028281984850764275,\n",
       " 0.015381167642772198,\n",
       " 0.23277243971824646,\n",
       " 0.45737743377685547,\n",
       " 0.17310069501399994,\n",
       " -0.13952675461769104,\n",
       " 0.31606683135032654,\n",
       " -0.18619835376739502,\n",
       " -0.05930060148239136,\n",
       " -0.027246972545981407,\n",
       " -0.4088040590286255,\n",
       " 0.09379925578832626,\n",
       " -0.14112573862075806,\n",
       " 0.2044251561164856,\n",
       " -0.11124996095895767,\n",
       " -0.026685336604714394,\n",
       " -0.3021242320537567,\n",
       " -0.16525651514530182,\n",
       " 0.04958706349134445,\n",
       " 0.10114038735628128,\n",
       " -0.15843424201011658,\n",
       " 0.05846865475177765,\n",
       " 0.017792921513319016,\n",
       " -0.04386652633547783,\n",
       " -0.07548505067825317,\n",
       " 0.18913634121418,\n",
       " -0.03494236245751381,\n",
       " -0.3128088116645813,\n",
       " -0.07065017521381378,\n",
       " -0.25107190012931824,\n",
       " -0.11489126831293106,\n",
       " 0.16394950449466705,\n",
       " -0.0699615404009819,\n",
       " -0.2442648559808731,\n",
       " 0.02252531424164772,\n",
       " -0.1923385113477707,\n",
       " -0.24627570807933807,\n",
       " -0.2665390968322754,\n",
       " 0.01658855937421322,\n",
       " 0.19823890924453735,\n",
       " -0.16941000521183014,\n",
       " 0.07125285267829895,\n",
       " -0.21110612154006958,\n",
       " 0.1790529489517212,\n",
       " 0.44609037041664124,\n",
       " -0.17374388873577118,\n",
       " -0.12635573744773865,\n",
       " -0.011358221992850304,\n",
       " 0.06472523510456085,\n",
       " 0.34561479091644287,\n",
       " 0.3144196569919586,\n",
       " 0.07816110551357269,\n",
       " 0.1823200285434723,\n",
       " -0.14955735206604004,\n",
       " 0.055317528545856476,\n",
       " -0.11902392655611038,\n",
       " -0.008770354092121124,\n",
       " -0.07705456018447876,\n",
       " -0.17921578884124756,\n",
       " 0.16366161406040192,\n",
       " -0.17633379995822906,\n",
       " 0.15314719080924988,\n",
       " -0.11687374860048294,\n",
       " -0.08727211505174637,\n",
       " 0.09438198804855347,\n",
       " -0.1842263638973236,\n",
       " -0.14984771609306335,\n",
       " 0.02144533395767212,\n",
       " 0.382681280374527,\n",
       " 0.04287128150463104,\n",
       " 0.29430198669433594,\n",
       " -0.017300114035606384,\n",
       " 0.18266385793685913,\n",
       " 0.11293746531009674,\n",
       " -0.03509704768657684,\n",
       " 0.07709745317697525,\n",
       " 0.13830634951591492,\n",
       " 0.13861359655857086,\n",
       " -0.023652227595448494,\n",
       " 0.028356464579701424,\n",
       " 0.086039237678051,\n",
       " 0.05690329149365425,\n",
       " -0.053846575319767,\n",
       " -0.14688962697982788,\n",
       " -0.2669503390789032,\n",
       " 0.010691005736589432,\n",
       " -0.2825540006160736,\n",
       " -0.40472978353500366,\n",
       " 0.10913620889186859,\n",
       " 0.4922083616256714,\n",
       " -0.08385694026947021,\n",
       " -0.21522565186023712,\n",
       " 0.07312187552452087,\n",
       " 0.026723943650722504,\n",
       " -0.07513687014579773,\n",
       " -0.06692639738321304,\n",
       " 0.22236131131649017,\n",
       " -0.2461819350719452,\n",
       " -0.18221601843833923,\n",
       " 0.17687919735908508,\n",
       " 0.014319095760583878,\n",
       " 0.0008255606517195702,\n",
       " 0.040594812482595444,\n",
       " -0.08115927129983902,\n",
       " 0.006940179038792849,\n",
       " 0.15294088423252106,\n",
       " -0.03967578336596489,\n",
       " -0.31378161907196045,\n",
       " 0.16551242768764496,\n",
       " -0.03674384206533432,\n",
       " -0.07668612897396088,\n",
       " 0.10731180757284164,\n",
       " 0.030198169872164726,\n",
       " -0.12389306724071503,\n",
       " -0.08400463312864304,\n",
       " -0.06656111776828766,\n",
       " 0.1796688288450241,\n",
       " -0.32925868034362793,\n",
       " -0.15229998528957367,\n",
       " -0.1252073496580124,\n",
       " 0.09559262543916702,\n",
       " 0.07547462731599808,\n",
       " 0.03897635638713837,\n",
       " 0.05291440337896347,\n",
       " -0.08734863996505737,\n",
       " 0.13169845938682556,\n",
       " -0.1822466105222702,\n",
       " -0.2352721095085144,\n",
       " -0.29657119512557983,\n",
       " -0.2972193956375122,\n",
       " 0.05125987157225609,\n",
       " -0.09076803177595139,\n",
       " 0.09276814013719559,\n",
       " 0.06906663626432419,\n",
       " -0.5212712287902832,\n",
       " -0.20919664204120636,\n",
       " 0.21536821126937866,\n",
       " 0.11744531989097595,\n",
       " -0.07602321356534958,\n",
       " 0.010281214490532875,\n",
       " 0.13576069474220276,\n",
       " -0.17826376855373383,\n",
       " -0.2423943430185318,\n",
       " -0.11839240789413452,\n",
       " 0.11614242941141129,\n",
       " 0.30294954776763916,\n",
       " -0.1067524403333664,\n",
       " 0.0986795574426651,\n",
       " -0.15644459426403046,\n",
       " 0.0935925543308258,\n",
       " -0.23899877071380615,\n",
       " 0.12024656683206558,\n",
       " 0.46518510580062866,\n",
       " -0.33908185362815857,\n",
       " 0.3199979066848755,\n",
       " -0.15443143248558044,\n",
       " -0.16719767451286316,\n",
       " 0.005185130052268505,\n",
       " -0.06898874789476395,\n",
       " -0.011884985491633415,\n",
       " 0.28482282161712646,\n",
       " 0.14673373103141785,\n",
       " 0.20474570989608765,\n",
       " -0.2755948305130005,\n",
       " 0.2594543993473053,\n",
       " -0.11498939245939255,\n",
       " -0.278751015663147,\n",
       " 0.19047218561172485,\n",
       " 0.15137284994125366,\n",
       " -0.4634523391723633,\n",
       " -0.005906624253839254,\n",
       " -0.0184074267745018,\n",
       " 0.18399490416049957,\n",
       " 0.02541740983724594,\n",
       " -0.20857839286327362,\n",
       " 0.10690050572156906,\n",
       " 0.3071421980857849,\n",
       " -0.08421669900417328,\n",
       " -0.07403837144374847,\n",
       " -0.2428528070449829,\n",
       " 0.014158488251268864,\n",
       " -0.238790825009346,\n",
       " -0.4809406101703644,\n",
       " -0.11483246833086014,\n",
       " 0.13242025673389435,\n",
       " -0.10041364282369614,\n",
       " -0.012570948339998722,\n",
       " 0.023258185014128685,\n",
       " -0.08565685153007507,\n",
       " -0.11215092241764069,\n",
       " -0.06117543578147888,\n",
       " -0.033303409814834595,\n",
       " -0.1163310632109642,\n",
       " 0.0835895836353302,\n",
       " 0.5502098202705383,\n",
       " 0.021073997020721436,\n",
       " 0.008354038000106812,\n",
       " 0.08445656299591064,\n",
       " 0.3485364317893982,\n",
       " -0.0396864227950573,\n",
       " 0.0642448216676712,\n",
       " 0.30546092987060547,\n",
       " -0.09759800881147385,\n",
       " 0.014249389991164207,\n",
       " 0.00035807923995889723,\n",
       " 1.4539728164672852,\n",
       " -0.13254746794700623,\n",
       " 0.05842890590429306,\n",
       " -0.0035179071128368378,\n",
       " -0.06997685879468918,\n",
       " 0.018807251006364822,\n",
       " -0.03735387325286865,\n",
       " -0.04454587772488594,\n",
       " 0.001194252516143024,\n",
       " 0.19326892495155334,\n",
       " -0.3774557411670685,\n",
       " -0.1904604732990265,\n",
       " -0.23941482603549957,\n",
       " 0.26383012533187866,\n",
       " -0.14860878884792328,\n",
       " -0.1782754361629486,\n",
       " -0.15950320661067963,\n",
       " 0.06812013685703278,\n",
       " -0.016773482784628868,\n",
       " -0.01880441978573799,\n",
       " -0.32169196009635925,\n",
       " -0.07151523232460022,\n",
       " 0.10071788728237152,\n",
       " 0.11022911965847015,\n",
       " -0.21970297396183014,\n",
       " 0.0723818987607956,\n",
       " -0.17765554785728455,\n",
       " -0.03646069020032883,\n",
       " -0.020215531811118126,\n",
       " -1.5597714185714722,\n",
       " -0.055178169161081314,\n",
       " 0.19421672821044922,\n",
       " 0.22446462512016296,\n",
       " 0.009938682429492474,\n",
       " -0.1520480513572693,\n",
       " 0.08439987897872925,\n",
       " 0.2525128424167633,\n",
       " -0.1900435984134674,\n",
       " -0.04497186094522476,\n",
       " -0.15313252806663513,\n",
       " 0.1722763627767563,\n",
       " 0.20242692530155182,\n",
       " -0.06418835371732712,\n",
       " 0.03966064006090164,\n",
       " -0.02194073423743248,\n",
       " 0.0638328492641449,\n",
       " 0.32962414622306824,\n",
       " -0.1945837289094925,\n",
       " 0.1410074681043625,\n",
       " -0.11778582632541656,\n",
       " 0.002709300024434924,\n",
       " 0.3638153374195099,\n",
       " -0.012973794713616371,\n",
       " -0.5632542967796326,\n",
       " 0.03338845446705818,\n",
       " 0.007897179573774338,\n",
       " -0.27914711833000183,\n",
       " -0.09766267985105515,\n",
       " 0.20329786837100983,\n",
       " 0.1498744636774063,\n",
       " -0.06111254543066025,\n",
       " -0.03663196042180061,\n",
       " 0.3701637089252472,\n",
       " -0.27476197481155396,\n",
       " -0.03919203206896782,\n",
       " 0.14221835136413574,\n",
       " -0.010192187502980232,\n",
       " -0.2433943897485733,\n",
       " 0.14154115319252014,\n",
       " 0.04482502490282059,\n",
       " -0.08538389950990677,\n",
       " -0.04681769013404846,\n",
       " 0.06505484133958817,\n",
       " -0.1979784369468689,\n",
       " -0.1091466024518013,\n",
       " 0.2640976011753082,\n",
       " -0.09232266247272491,\n",
       " 0.07014618068933487,\n",
       " -0.08357245475053787,\n",
       " -0.37300214171409607,\n",
       " 0.28605902194976807,\n",
       " 0.05452018976211548,\n",
       " 0.12869645655155182,\n",
       " -0.2060096710920334,\n",
       " 0.21438246965408325,\n",
       " -0.10142319649457932,\n",
       " -0.279997318983078,\n",
       " 0.16103659570217133,\n",
       " -0.0917116105556488,\n",
       " -0.2336157113313675,\n",
       " -0.014614986255764961,\n",
       " -0.1132647842168808,\n",
       " -0.04730895161628723,\n",
       " -0.03986796736717224,\n",
       " -0.33785438537597656,\n",
       " 0.31656453013420105,\n",
       " 0.3483927547931671,\n",
       " -0.03350640833377838,\n",
       " 0.01776112988591194,\n",
       " 0.08371155709028244,\n",
       " -0.029910216107964516,\n",
       " 0.10362212359905243,\n",
       " -0.09148810803890228,\n",
       " -0.06729119271039963,\n",
       " -0.04092225804924965,\n",
       " 0.1496891975402832,\n",
       " 0.10635900497436523,\n",
       " 0.09591973572969437,\n",
       " -0.07582169771194458,\n",
       " 0.11859029531478882,\n",
       " 0.32465824484825134,\n",
       " 0.20175990462303162,\n",
       " -0.011261670850217342,\n",
       " -0.1834503412246704,\n",
       " -0.32619452476501465,\n",
       " -0.09841345250606537,\n",
       " 0.09951275587081909,\n",
       " -0.02839002013206482,\n",
       " 0.2223181426525116,\n",
       " -0.22751368582248688,\n",
       " -0.27521660923957825,\n",
       " -0.025332970544695854,\n",
       " 0.047303009778261185,\n",
       " 0.07166370004415512,\n",
       " 0.08427122235298157,\n",
       " 0.10260310769081116,\n",
       " 0.020089928060770035,\n",
       " -0.15410512685775757,\n",
       " -0.24168318510055542,\n",
       " -0.1136283427476883,\n",
       " -0.190541073679924,\n",
       " -0.09615979343652725,\n",
       " -0.12611141800880432,\n",
       " -0.29556944966316223,\n",
       " -0.1926940679550171,\n",
       " -0.2446971833705902,\n",
       " 0.05232604965567589,\n",
       " -0.0985189750790596,\n",
       " -0.05027684196829796,\n",
       " -0.20822954177856445,\n",
       " -0.1398620307445526,\n",
       " -0.09362951666116714,\n",
       " 0.04696466028690338,\n",
       " -0.12838564813137054,\n",
       " -0.0030965544283390045,\n",
       " -0.1300409436225891,\n",
       " 0.11490297317504883,\n",
       " 0.09088641405105591,\n",
       " 0.013953329995274544,\n",
       " -0.13908472657203674,\n",
       " 0.005446223542094231,\n",
       " -0.09513109922409058,\n",
       " 0.3840247690677643,\n",
       " -0.14383359253406525,\n",
       " -0.02870398946106434,\n",
       " -0.1371593028306961,\n",
       " -0.0768112912774086,\n",
       " -0.05984420329332352,\n",
       " 0.08369176089763641,\n",
       " -0.14561684429645538,\n",
       " 0.04078589379787445,\n",
       " 0.20176570117473602,\n",
       " -0.33779841661453247,\n",
       " -0.08869224786758423,\n",
       " 0.4332611560821533,\n",
       " 0.269592821598053,\n",
       " 0.13540321588516235,\n",
       " 0.004354643169790506,\n",
       " -0.03487410768866539,\n",
       " -0.17742696404457092,\n",
       " -0.3532350957393646,\n",
       " 0.034830424934625626,\n",
       " -0.4047936499118805,\n",
       " -0.14981697499752045,\n",
       " 0.304485946893692,\n",
       " 0.005337066017091274,\n",
       " 0.06224730238318443,\n",
       " 0.29154396057128906,\n",
       " -0.2209695279598236,\n",
       " -0.004512764047831297,\n",
       " -0.561456024646759,\n",
       " -0.14672328531742096,\n",
       " 0.21084082126617432,\n",
       " 0.23676805198192596,\n",
       " 0.41957029700279236,\n",
       " -0.06952900439500809,\n",
       " -0.1506381332874298,\n",
       " -0.01385321281850338,\n",
       " 0.09842859208583832,\n",
       " 0.023963406682014465,\n",
       " 0.23310115933418274,\n",
       " -0.7184200286865234,\n",
       " 0.07699254155158997,\n",
       " -0.018615545704960823,\n",
       " -0.13386322557926178,\n",
       " 0.1704891324043274,\n",
       " 0.013219834305346012,\n",
       " 0.3528713285923004,\n",
       " 0.062178853899240494,\n",
       " 0.08945012092590332,\n",
       " 0.08869943022727966,\n",
       " 0.013813774101436138,\n",
       " 0.17759299278259277,\n",
       " -0.1875077784061432,\n",
       " 0.021364402025938034,\n",
       " 0.3044895827770233,\n",
       " 0.13214625418186188,\n",
       " -0.10436347872018814,\n",
       " 0.1223626509308815,\n",
       " -0.2341332882642746,\n",
       " 0.11509594321250916,\n",
       " 0.18210174143314362,\n",
       " 0.22307412326335907,\n",
       " -0.12236033380031586,\n",
       " -0.03670560568571091,\n",
       " -0.018469080328941345,\n",
       " 0.06045307591557503,\n",
       " -0.016527272760868073,\n",
       " -0.052888624370098114,\n",
       " 0.29677316546440125,\n",
       " -0.07639788091182709,\n",
       " -0.07722388952970505,\n",
       " -0.14898258447647095,\n",
       " 0.042398881167173386,\n",
       " -0.08617676794528961,\n",
       " -0.14334985613822937,\n",
       " 0.06967812776565552,\n",
       " 0.0679304301738739,\n",
       " -0.2973410487174988,\n",
       " -0.23770390450954437,\n",
       " 0.23260316252708435,\n",
       " 0.18682576715946198,\n",
       " 0.14809410274028778,\n",
       " -0.08864308148622513,\n",
       " 0.09760706126689911,\n",
       " 0.18632803857326508,\n",
       " -0.3295488953590393,\n",
       " -0.0809125006198883,\n",
       " 0.032535526901483536,\n",
       " 0.10866586118936539,\n",
       " 0.3648870885372162,\n",
       " -0.07084326446056366,\n",
       " 0.005296763032674789,\n",
       " 0.085856132209301,\n",
       " 0.23001399636268616,\n",
       " 0.18563249707221985,\n",
       " -0.3280276358127594,\n",
       " 0.04411037266254425,\n",
       " 0.33852627873420715,\n",
       " -0.09683941304683685,\n",
       " -0.3172433376312256,\n",
       " -0.061174165457487106,\n",
       " 0.044171445071697235,\n",
       " 0.21283085644245148,\n",
       " 0.007275755517184734,\n",
       " -0.024485845118761063,\n",
       " -0.00627274252474308,\n",
       " 0.14115722477436066,\n",
       " -0.37536147236824036,\n",
       " 0.15758799016475677,\n",
       " 0.12240084260702133,\n",
       " 0.2354132980108261,\n",
       " 0.08377040177583694,\n",
       " -0.11024826020002365,\n",
       " -0.1072465255856514,\n",
       " 1.1643195152282715,\n",
       " -0.035474590957164764,\n",
       " -0.05670393630862236,\n",
       " 0.09621545672416687,\n",
       " 0.07126544415950775,\n",
       " 1.2404165267944336,\n",
       " 0.11473067104816437,\n",
       " 0.25278934836387634,\n",
       " 0.061634477227926254,\n",
       " -0.019218988716602325,\n",
       " 0.14535482227802277,\n",
       " 0.014078947715461254,\n",
       " 0.3095889389514923,\n",
       " 0.008267475292086601,\n",
       " 0.12170062959194183,\n",
       " 0.13946841657161713,\n",
       " -0.17107395827770233,\n",
       " -0.34923848509788513,\n",
       " -0.07558296620845795,\n",
       " -0.07320820540189743,\n",
       " 0.11404833942651749,\n",
       " -0.19822818040847778,\n",
       " 0.010416680946946144,\n",
       " 0.22827960550785065,\n",
       " 0.07486331462860107,\n",
       " 0.02922595478594303,\n",
       " -0.17020106315612793,\n",
       " -0.03292815387248993,\n",
       " -0.037969689816236496,\n",
       " -0.21585455536842346,\n",
       " 0.32803937792778015,\n",
       " -0.2041844129562378,\n",
       " -0.1674482375383377,\n",
       " -0.10050375759601593,\n",
       " 0.05877377465367317,\n",
       " -0.003926578443497419,\n",
       " 0.05680372938513756,\n",
       " -0.3382139801979065,\n",
       " 0.0009647553670220077,\n",
       " 0.0951719880104065,\n",
       " -0.053055740892887115,\n",
       " -0.08892638236284256,\n",
       " 0.0950988382101059,\n",
       " -0.13882966339588165,\n",
       " 0.1628146469593048,\n",
       " 0.1284371167421341,\n",
       " -0.357330322265625,\n",
       " 0.00039951776852831244,\n",
       " 0.008162640035152435,\n",
       " 0.27860286831855774,\n",
       " 0.16806389391422272,\n",
       " -0.06886439025402069,\n",
       " 0.032903071492910385,\n",
       " -0.0842973068356514,\n",
       " -0.14556150138378143,\n",
       " 0.19211408495903015,\n",
       " -0.19302840530872345,\n",
       " 0.05172044783830643,\n",
       " -0.02296539954841137,\n",
       " -0.1319374293088913,\n",
       " 0.12366680055856705,\n",
       " 0.35247308015823364,\n",
       " -0.138775035738945,\n",
       " -0.28524109721183777,\n",
       " -0.09175396710634232,\n",
       " -0.08617810904979706,\n",
       " -0.04398219659924507,\n",
       " -0.2616021931171417,\n",
       " -0.13967078924179077,\n",
       " -0.0843278020620346,\n",
       " -0.05922814458608627,\n",
       " -0.03386515751481056,\n",
       " -0.06752708554267883,\n",
       " 0.32495251297950745,\n",
       " -0.15478913486003876,\n",
       " 0.3440105617046356,\n",
       " 0.06810525804758072,\n",
       " 0.018607741221785545,\n",
       " -0.0966077521443367,\n",
       " 0.0704670250415802,\n",
       " 0.3676518201828003,\n",
       " 0.2337263822555542,\n",
       " -0.41055262088775635,\n",
       " 0.34523290395736694,\n",
       " 0.20186060667037964,\n",
       " -0.11438363045454025,\n",
       " 0.16998009383678436,\n",
       " -0.1551094800233841,\n",
       " -0.17947912216186523,\n",
       " 0.09904801100492477,\n",
       " 0.18664884567260742,\n",
       " 0.01612868532538414,\n",
       " -0.07159633189439774,\n",
       " -0.07411574572324753,\n",
       " 0.08629673719406128,\n",
       " -0.009066852740943432,\n",
       " -0.07951780408620834,\n",
       " -0.1967693716287613,\n",
       " 0.12499164789915085,\n",
       " 0.23106077313423157,\n",
       " 0.16718420386314392,\n",
       " 0.014818844385445118,\n",
       " 0.013216588646173477,\n",
       " 0.06275766342878342,\n",
       " -0.07005239278078079,\n",
       " -0.3469896912574768,\n",
       " 0.3903283476829529,\n",
       " 0.26010796427726746,\n",
       " -0.08850482851266861,\n",
       " 0.22944292426109314,\n",
       " 0.058846041560173035,\n",
       " -0.21369802951812744,\n",
       " -0.18007688224315643,\n",
       " 0.3196766972541809,\n",
       " 0.13079912960529327,\n",
       " -0.13899405300617218,\n",
       " -0.0391981415450573,\n",
       " 0.3198238015174866,\n",
       " 0.2718161940574646,\n",
       " -0.09767746925354004,\n",
       " 0.08722792565822601,\n",
       " 0.2004707306623459,\n",
       " 0.1578512191772461,\n",
       " -0.24913914501667023,\n",
       " 0.09562596678733826,\n",
       " 0.3549495339393616,\n",
       " -0.23069126904010773,\n",
       " 0.012369229458272457,\n",
       " 0.03337099030613899,\n",
       " -0.22403481602668762,\n",
       " -0.04706528037786484,\n",
       " -0.013410920277237892,\n",
       " -0.01969827152788639,\n",
       " -0.26266029477119446,\n",
       " 0.00023184804012998939,\n",
       " 0.03372616693377495,\n",
       " -0.16911281645298004,\n",
       " -0.22539398074150085,\n",
       " -0.4301702678203583,\n",
       " -0.12734200060367584,\n",
       " -0.0805078074336052,\n",
       " -0.20631766319274902,\n",
       " -0.014687258750200272,\n",
       " -0.022120678797364235,\n",
       " -0.1904061734676361,\n",
       " 0.03047158382833004,\n",
       " 0.22095049917697906,\n",
       " 0.2513560652732849,\n",
       " -0.006468630861490965,\n",
       " -0.17301572859287262,\n",
       " -0.15518637001514435,\n",
       " 0.10865306109189987,\n",
       " -0.03516651317477226,\n",
       " -0.319190114736557,\n",
       " 0.03943440690636635,\n",
       " -0.28269585967063904,\n",
       " 0.009531418792903423,\n",
       " 0.13886895775794983,\n",
       " -0.04744948074221611,\n",
       " 0.2022397667169571,\n",
       " -0.0018972030375152826,\n",
       " -0.18173880875110626,\n",
       " -0.08817150443792343,\n",
       " 0.26525765657424927,\n",
       " -0.11084816604852676,\n",
       " 0.19567769765853882,\n",
       " -0.042842037975788116,\n",
       " -0.08139801770448685,\n",
       " 0.01717320643365383,\n",
       " -0.5003007650375366,\n",
       " 0.029687216505408287,\n",
       " 0.2584288716316223,\n",
       " -0.2967465817928314,\n",
       " 0.04701119661331177,\n",
       " 0.03035608120262623,\n",
       " 0.050279490649700165,\n",
       " -0.08286157995462418,\n",
       " 0.08631736040115356,\n",
       " -0.18470020592212677,\n",
       " -0.05657799541950226,\n",
       " -0.03855212777853012,\n",
       " -0.043099358677864075,\n",
       " -0.08441382646560669,\n",
       " 0.048092328011989594,\n",
       " 0.034832533448934555,\n",
       " 0.07316119968891144,\n",
       " 0.06902867555618286,\n",
       " 0.040188223123550415,\n",
       " -0.11594709008932114,\n",
       " 0.04401576146483421,\n",
       " -0.10776840895414352,\n",
       " 0.17637617886066437,\n",
       " -0.21327738463878632,\n",
       " 0.19436562061309814,\n",
       " 0.3205236792564392,\n",
       " -0.04353044182062149,\n",
       " -0.10889114439487457,\n",
       " 0.09063591063022614,\n",
       " -0.013219553977251053,\n",
       " -0.07335658371448517,\n",
       " -0.052099794149398804,\n",
       " -0.254656046628952,\n",
       " 0.04905891790986061,\n",
       " 0.16804678738117218,\n",
       " 0.0008082585409283638,\n",
       " -0.24165695905685425,\n",
       " -0.0030939322896301746,\n",
       " -0.03328671306371689,\n",
       " -0.1450509876012802,\n",
       " -0.05058145150542259,\n",
       " -0.051992349326610565,\n",
       " 0.14162805676460266,\n",
       " 0.1558430939912796,\n",
       " -2.2998244762420654,\n",
       " 0.09791232645511627,\n",
       " 0.010438812896609306,\n",
       " 0.08039151132106781,\n",
       " 0.1093965545296669,\n",
       " -0.2531304955482483,\n",
       " 0.08851534873247147,\n",
       " -0.009268661960959435,\n",
       " 0.10208256542682648,\n",
       " 0.1427627056837082,\n",
       " 0.10666622966527939,\n",
       " 0.14837318658828735,\n",
       " -0.15620240569114685,\n",
       " 0.02555590122938156,\n",
       " 0.1555783450603485,\n",
       " -0.5093567371368408,\n",
       " 0.0918051227927208,\n",
       " 0.3583309054374695,\n",
       " -0.03651425987482071,\n",
       " -0.23648518323898315,\n",
       " -0.1006954088807106,\n",
       " 0.07044718414545059,\n",
       " -0.01245749182999134,\n",
       " 0.26216405630111694,\n",
       " 0.12352896481752396,\n",
       " 0.20174750685691833,\n",
       " 0.20605650544166565,\n",
       " -0.07239097356796265,\n",
       " -0.03610130026936531,\n",
       " -0.024027949199080467,\n",
       " 0.10563218593597412,\n",
       " -0.11859920620918274,\n",
       " 0.09768125414848328,\n",
       " 0.1178356260061264,\n",
       " -0.3304685950279236,\n",
       " -0.294660359621048,\n",
       " -0.19141264259815216,\n",
       " -0.16286900639533997,\n",
       " -0.12670490145683289,\n",
       " 0.23314344882965088,\n",
       " -0.010721896775066853,\n",
       " -0.28652557730674744,\n",
       " -0.1777651458978653,\n",
       " -0.0020621567964553833,\n",
       " -0.0729275718331337,\n",
       " 0.06099843978881836,\n",
       " -0.11034005135297775,\n",
       " -0.07752219587564468,\n",
       " -0.043356701731681824,\n",
       " 0.1704222559928894,\n",
       " 0.2075207382440567,\n",
       " 0.4337140619754791,\n",
       " 0.034648992121219635,\n",
       " 0.029388664290308952,\n",
       " 0.1482062041759491,\n",
       " 0.08701404929161072,\n",
       " -0.2796572148799896,\n",
       " -0.26746177673339844,\n",
       " -0.1640467643737793,\n",
       " -0.021814122796058655,\n",
       " -0.08121118694543839,\n",
       " -0.08794153481721878,\n",
       " 0.05942043662071228,\n",
       " 0.19201388955116272,\n",
       " 0.1947501003742218,\n",
       " -0.10827609151601791,\n",
       " 0.19634664058685303,\n",
       " 0.10460323095321655,\n",
       " -0.13882608711719513,\n",
       " 0.051796942949295044,\n",
       " 0.044760990887880325,\n",
       " 0.020506955683231354,\n",
       " 0.0400267131626606,\n",
       " -0.16681426763534546,\n",
       " 0.18472449481487274,\n",
       " -0.11098942160606384,\n",
       " 0.024147119373083115,\n",
       " 0.25937172770500183,\n",
       " -0.19741185009479523,\n",
       " -0.06967777013778687,\n",
       " -0.15186966955661774,\n",
       " -0.01096720714122057,\n",
       " 0.0500095933675766,\n",
       " 0.11711020767688751,\n",
       " -0.02263421192765236,\n",
       " 0.2510080933570862,\n",
       " 0.27292460203170776,\n",
       " 0.022333374246954918,\n",
       " 0.14249500632286072,\n",
       " -0.1138986274600029,\n",
       " 0.3559286296367645,\n",
       " -0.07235490530729294,\n",
       " 0.0899377390742302,\n",
       " 0.2838963568210602,\n",
       " -0.0702613964676857,\n",
       " -0.08349502831697464,\n",
       " -0.06787589192390442,\n",
       " -0.1265748143196106,\n",
       " -0.3340567350387573,\n",
       " 0.00030906882602721453,\n",
       " -0.08302529156208038,\n",
       " 0.032654497772455215,\n",
       " 0.20804397761821747,\n",
       " -0.16185827553272247,\n",
       " -0.2719489634037018,\n",
       " -0.2016175389289856,\n",
       " 0.06872587651014328,\n",
       " -0.07192900776863098,\n",
       " 0.05368688702583313,\n",
       " 0.019549312070012093,\n",
       " 0.08732966333627701,\n",
       " -0.01511482335627079,\n",
       " 0.004565230570733547,\n",
       " -0.13627952337265015,\n",
       " 0.1982511281967163,\n",
       " 0.11693403869867325,\n",
       " 0.21151761710643768,\n",
       " 0.017740866169333458,\n",
       " -0.07992111146450043,\n",
       " -0.025788599625229836,\n",
       " 0.08754248172044754,\n",
       " 0.23560470342636108,\n",
       " -0.028555070981383324,\n",
       " 0.16175386309623718,\n",
       " -0.11707671731710434,\n",
       " -0.19609121978282928,\n",
       " 0.3073803782463074,\n",
       " -0.03933165967464447,\n",
       " 0.025490259751677513,\n",
       " -0.03203505650162697,\n",
       " 0.054967381060123444,\n",
       " 0.12036146968603134,\n",
       " -0.12332142889499664,\n",
       " -0.14817433059215546,\n",
       " -0.3523792624473572,\n",
       " -0.034455183893442154,\n",
       " -0.2700130045413971,\n",
       " -0.14281325042247772,\n",
       " -0.14065717160701752,\n",
       " -0.3019569516181946,\n",
       " 0.14698009192943573,\n",
       " -0.09951643645763397,\n",
       " -0.04220571368932724,\n",
       " 0.19714510440826416,\n",
       " -0.052536413073539734,\n",
       " -0.10748207569122314,\n",
       " 0.33286812901496887,\n",
       " -0.1078278198838234,\n",
       " -0.11550857126712799,\n",
       " 0.0005799433565698564,\n",
       " -0.163969025015831,\n",
       " -0.16900812089443207,\n",
       " 0.010571706108748913,\n",
       " 0.008610954508185387,\n",
       " -0.07306629419326782,\n",
       " -0.19836828112602234,\n",
       " 0.07258222252130508,\n",
       " -0.04767947643995285,\n",
       " -0.2627008557319641,\n",
       " 0.21856003999710083,\n",
       " 0.25694921612739563,\n",
       " -0.2243013083934784,\n",
       " 0.3893478512763977,\n",
       " 0.01057470589876175,\n",
       " 0.04766513779759407,\n",
       " 0.03973827138543129,\n",
       " -0.09474829584360123,\n",
       " 0.016627777367830276,\n",
       " -0.13767196238040924,\n",
       " -0.0632031038403511,\n",
       " 0.3538658022880554,\n",
       " -0.20924244821071625,\n",
       " -0.2390054613351822,\n",
       " 0.07096435874700546,\n",
       " 0.2559751570224762,\n",
       " -0.005890076979994774,\n",
       " -0.10141162574291229,\n",
       " -0.03615153953433037,\n",
       " -0.08155056834220886,\n",
       " 0.11135990172624588,\n",
       " 0.1011112779378891,\n",
       " 0.2976600229740143,\n",
       " -0.05520517751574516,\n",
       " -0.10575129836797714,\n",
       " -0.25216224789619446,\n",
       " 0.12383680045604706,\n",
       " 0.23946939408779144,\n",
       " -0.21469292044639587,\n",
       " -0.03411545231938362,\n",
       " 0.11972759664058685,\n",
       " -0.007974776439368725,\n",
       " 0.1647399514913559,\n",
       " -0.06238821521401405,\n",
       " -0.10491146147251129,\n",
       " -0.10283978283405304,\n",
       " -0.018690075725317,\n",
       " 0.19337239861488342,\n",
       " -0.26745113730430603,\n",
       " 0.0072800833731889725,\n",
       " 0.18214914202690125,\n",
       " -0.22354494035243988,\n",
       " -0.20074649155139923,\n",
       " -0.09800127893686295,\n",
       " 0.14169465005397797,\n",
       " 0.31213200092315674,\n",
       " 0.002942571882158518,\n",
       " -0.18045224249362946,\n",
       " 0.14996178448200226,\n",
       " -0.08761787414550781,\n",
       " 0.04339926689863205,\n",
       " 0.40960201621055603,\n",
       " -0.012833713553845882,\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama2\")\n",
    "oembed.embed_query(\"Llamas are social animals and live with others as a herd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 29fdb92e57cf... 100% |███████████████| (7.4/7.4 GB, 51 MB/s)           \n",
      "pulling 8c17c2ebb0ea... 100% |█████████████████| (7.0/7.0 kB, 294 MB/s)        \n",
      "pulling 7c23fb36d801... 100% |██████████████████| (4.8/4.8 kB, 68 MB/s)        \n",
      "pulling 2e0493f67d0c... 100% |████████████████████| (59/59 B, 2.2 MB/s)        \n",
      "pulling 2759286baa87... 100% |███████████████████| (105/105 B, 12 MB/s)        \n",
      "pulling db161ca112dc... 100% |███████████████████| (530/530 B, 286 B/s)        \n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama2:13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt4all\n",
      "  Downloading gpt4all-2.0.2-py3-none-macosx_10_15_universal2.whl.metadata (892 bytes)\n",
      "Requirement already satisfied: requests in /Users/greatmaster/miniconda3/envs/oreilly_llama2/lib/python3.11/site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/greatmaster/miniconda3/envs/oreilly_llama2/lib/python3.11/site-packages (from gpt4all) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatmaster/miniconda3/envs/oreilly_llama2/lib/python3.11/site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/greatmaster/miniconda3/envs/oreilly_llama2/lib/python3.11/site-packages (from requests->gpt4all) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greatmaster/miniconda3/envs/oreilly_llama2/lib/python3.11/site-packages (from requests->gpt4all) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/greatmaster/miniconda3/envs/oreilly_llama2/lib/python3.11/site-packages (from requests->gpt4all) (2023.7.22)\n",
      "Downloading gpt4all-2.0.2-py3-none-macosx_10_15_universal2.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gpt4all\n",
      "Successfully installed gpt4all-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install chromadb\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install gpt4all\n",
    "# !pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45.9M/45.9M [00:01<00:00, 23.7MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "# Embed and store\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings  # We can also try Ollama embeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "question = \"How can Task Decomposition be done?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}),\n",
       " Document(page_content='Planning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "from langchain import hub\n",
    "\n",
    "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    verbose=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The various approaches to Task Decomposition for AI Agents involve breaking down complex tasks into smaller, manageable subtasks or steps. Here are some of the common techniques used in LLM-powered autonomous agent systems:\n",
      "\n",
      "1. Chain of Thoughts (CoT): This technique involves instructing the model to \"think step by step\" and use more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks, providing insight into the model's thinking process.\n",
      "2. Tree of Thoughts (ToT): ToT extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS or DFS with each state evaluated by a classifier or majority vote.\n",
      "3. Planning: In this approach, the agent breaks down large tasks into smaller, manageable subtasks, enabling efficient handling of complex tasks. Reflection and refinement are also used to learn from mistakes and improve the quality of final results.\n",
      "4. Memory-based Decomposition: The agent can use short-term memory to learn from in-context learning and long-term memory to retain and recall information over extended periods. External vector stores and fast retrieval can also be leveraged for this purpose.\n",
      "5. Tool Use: The agent can call external APIs for additional information that is missing from the model weights, including current information, code execution capability, access to proprietary information sources, and more.\n",
      "6. Hybrid Approaches: Combining multiple techniques, such as CoT and ToT, or using a hierarchical structure with multiple levels of decomposition, can lead to more efficient and effective task decomposition.\n",
      "\n",
      "In summary, Task Decomposition is a crucial aspect of LLM-powered autonomous agent systems, allowing the agents to handle complex tasks by breaking them down into smaller, manageable steps. By using various techniques such as CoT, ToT, Planning, Memory-based Decomposition, and Tool Use, these agents can efficiently execute tasks and provide accurate results."
     ]
    }
   ],
   "source": [
    "# QA chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "\n",
    "question = \"What are the various approaches to Task Decomposition for AI Agents?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the various approaches to Task Decomposition for AI Agents?', 'result': ' The various approaches to Task Decomposition for AI Agents involve breaking down complex tasks into smaller, manageable subtasks or steps. Here are some of the common techniques used in LLM-powered autonomous agent systems:\\n\\n1. Chain of Thoughts (CoT): This technique involves instructing the model to \"think step by step\" and use more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks, providing insight into the model\\'s thinking process.\\n2. Tree of Thoughts (ToT): ToT extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS or DFS with each state evaluated by a classifier or majority vote.\\n3. Planning: In this approach, the agent breaks down large tasks into smaller, manageable subtasks, enabling efficient handling of complex tasks. Reflection and refinement are also used to learn from mistakes and improve the quality of final results.\\n4. Memory-based Decomposition: The agent can use short-term memory to learn from in-context learning and long-term memory to retain and recall information over extended periods. External vector stores and fast retrieval can also be leveraged for this purpose.\\n5. Tool Use: The agent can call external APIs for additional information that is missing from the model weights, including current information, code execution capability, access to proprietary information sources, and more.\\n6. Hybrid Approaches: Combining multiple techniques, such as CoT and ToT, or using a hierarchical structure with multiple levels of decomposition, can lead to more efficient and effective task decomposition.\\n\\nIn summary, Task Decomposition is a crucial aspect of LLM-powered autonomous agent systems, allowing the agents to handle complex tasks by breaking them down into smaller, manageable steps. By using various techniques such as CoT, ToT, Planning, Memory-based Decomposition, and Tool Use, these agents can efficiently execute tasks and provide accurate results.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatives using Hugging Face LLM:\n",
    "- https://python.plainenglish.io/super-quick-fine-tuning-llama-2-0-on-cpu-with-personal-data-d2d284559f\n",
    "- https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_llama2",
   "language": "python",
   "name": "oreilly_llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
