{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are such things as llama 3 series of models\n",
    "2. They come in different sizes: from 2Gb of memory to 243Gb of memory\n",
    "3. They can be used for different thigng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achiev'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    '''Loads text from a PDF file.'''\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # extracting text from page\n",
    "    pdf_contents = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    \n",
    "    return pdf_contents\n",
    "\n",
    "    \n",
    "pdf_contents = load_pdf_text('./assets-resources/attention_paper.pdf')\n",
    "\n",
    "pdf_contents[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The main idea of the paper is to introduce a new approach to neural machine translation, which uses self-attention mechanisms to improve the accuracy and efficiency of the translation process.\n",
       "\n",
       "* The paper presents a novel architecture that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention mechanisms, allowing for parallelization and reducing the need for sequential processing.\n",
       "* This new approach enables the model to handle longer input sequences and capture more complex dependencies between words in the input and output sequences.\n",
       "\n",
       "> *In the paper, it is stated: \"We propose a new simple network architecture, the Transformer model, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\"*\n",
       " \n",
       "This quote highlights the main contribution of the paper, which is the introduction of a new architecture that relies solely on self-attention mechanisms to perform neural machine translation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_llama(question, context):\n",
    "    prompt = f\"\"\"\n",
    "    Consider the following context:\n",
    "    {context}\n",
    "\n",
    "    Answer the following question:\n",
    "    {question}\n",
    "    \n",
    "    Make sure to give a quote from the original context that serves to validate\n",
    "    your answer and place it in a different bullet point section.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='llama3.3',\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "question = \"What is the main idea of the paper?\"\n",
    "answer = ask_llama(question, pdf_contents)\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
