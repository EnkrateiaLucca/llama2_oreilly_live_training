{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two integers and returns the result integer\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divides two integers and returns the result integer\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "divide_tool = FunctionTool.from_defaults(fn=divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad81ef45cf9445259a4ec7a3e5a3a16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Optional quantization to 4bit\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_TOKEN\")\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    model_kwargs={\n",
    "        \"token\": hf_token,\n",
    "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "        # \"quantization_config\": quantization_config\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    tokenizer_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    tokenizer_kwargs={\"token\": hf_token},\n",
    "    stopping_ids=stopping_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step a4580108-e7dd-43d1-b8c8-cc8413d84829. Step input: What is 100 * 100?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {'a': 100, 'b': 100}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 10000\n",
      "\u001b[0m> Running step e9020238-de43-4224-9551-94a3c55c8cd3. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: 10000\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='10000', sources=[ToolOutput(content='10000', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 100, 'b': 100}}, raw_output=10000, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    [multiply_tool, add_tool, subtract_tool, divide_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent.chat(\"What is 100 * 100?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step d5cebe6f-6ef0-4d7b-9977-175025c353e0. Step input: What is 3 * 2 + 100?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {'a': 3, 'b': 2}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 6\n",
      "\u001b[0m> Running step 0dce18a4-7e6d-4922-ad1e-23da69865298. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use another tool to help me answer the question.\n",
      "Action: add\n",
      "Action Input: {'a': 6, 'b': 100}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 106\n",
      "\u001b[0m> Running step ac672043-0776-435d-88d6-ecc6758fae4d. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: 106\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='106', sources=[ToolOutput(content='6', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 3, 'b': 2}}, raw_output=6, is_error=False), ToolOutput(content='106', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 6, 'b': 100}}, raw_output=106, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"What is 3 * 2 + 100?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader(input_dir=\"./pdfs/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_indices = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = docs_indices.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Email credentials and SMTP settings\n",
    "smtp_server = \"smtp.gmail.com\"\n",
    "port = 587  # For starttls\n",
    "sender_email = \"lucasenkrateia@gmail.com\"  # Enter your email\n",
    "password = os.environ[\"GMAIL_PWD\"]\n",
    "\n",
    "def send_email(to, subject, body):\n",
    "    \"\"\"\n",
    "    Send an email using the configured sender email and SMTP settings\n",
    "    \n",
    "    Args:\n",
    "        to (str): Email address of the recipient\n",
    "        subject (str): Subject line of the email\n",
    "        body (str): Body text of the email\n",
    "    \"\"\"\n",
    "    # Create message\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = to\n",
    "    message[\"Subject\"] = subject\n",
    "    message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "    # Send email\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, port)\n",
    "        server.starttls()  # Secure the connection\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, to, message.as_string())\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_tool = FunctionTool.from_defaults(fn=send_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lora_paper\",\n",
    "            description=(\n",
    "                \"Provides information about an AI paper regarding LoRA technique for efficient fine-tuning of LLMs. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    email_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 5f99ef85-c519-44cd-b90e-ae2c5b4d1bd0. Step input: What is the LoRA technique for efficient fine-tuning of LLMs?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: lora_paper\n",
      "Action Input: {'input': 'LoRA technique for efficient fine-tuning of LLMs'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as the computation and storage of W=W0+BA can be done explicitly.\n",
      "\u001b[0m> Running step 0d3fba52-2b6a-4703-be1a-56d250add0a4. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as the computation and storage of W=W0+BA can be done explicitly.\n",
      "\u001b[0mThe LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as the computation and storage of W=W0+BA can be done explicitly.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the LoRA technique for efficient fine-tuning of LLMs?\")\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 481ac884-e930-40fc-aed4-b106deddf521. Step input: 1. Write a simple summary of the LoRA technique in bullet points, and then 2. send that to lucasbnsoares@hotmail.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to write a simple summary of the LoRA technique in bullet points.\n",
      "Action: None\n",
      "Action Input: {'to': 'lucasbnsoares@hotmail.com', 'subject': 'Summary of LoRA Technique', 'body': '• LoRA is a method for efficient fine-tuning of Language Model Models (LLMs).\\n• It allows for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices.\\n• It introduces no additional inference latency and provides a more efficient way to fine-tune LLMs compared to other methods.'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `None`.\n",
      "\u001b[0m> Running step 2f452ab7-4b65-4ed3-a0af-3733400e3185. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I need to use the send_email tool to send the summary to lucasbnsoares@hotmail.com.\n",
      "Action: send_email\n",
      "Action Input: {'to': 'lucasbnsoares@hotmail.com', 'subject': 'Summary of LoRA Technique', 'body': '• LoRA is a method for efficient fine-tuning of Language Model Models (LLMs).\\n• It allows for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices.\\n• It introduces no additional inference latency and provides a more efficient way to fine-tune LLMs compared to other methods.'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully!\n",
      "\u001b[1;3;34mObservation: None\n",
      "\u001b[0m> Running step 82de452d-1b19-4e34-ae57-44d2374cb38e. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I need to use the lora_paper tool to get information about the LoRA technique.\n",
      "Action: lora_paper\n",
      "Action Input: {'input': 'LoRA technique for efficient fine-tuning of LLMs'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by applying LoRA to all weight matrices and training all biases, it roughly recovers the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. As the number of trainable parameters increases, training LoRA roughly converges to training the original model, providing a more efficient way to fine-tune LLMs compared to other methods like adapter-based or prefix-based approaches. Additionally, when deployed in production, LoRA introduces no additional inference latency, allowing for seamless computation and storage of the updated weights for inference.\n",
      "\u001b[0m> Running step 6bd15ae4-9139-4918-b9e3-753a18724275. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by applying LoRA to all weight matrices and training all biases, it roughly recovers the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. As the number of trainable parameters increases, training LoRA roughly converges to training the original model, providing a more efficient way to fine-tune LLMs compared to other methods like adapter-based or prefix-based approaches. Additionally, when deployed in production, LoRA introduces no additional inference latency, allowing for seamless computation and storage of the updated weights for inference.\n",
      "\u001b[0mThe LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by applying LoRA to all weight matrices and training all biases, it roughly recovers the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. As the number of trainable parameters increases, training LoRA roughly converges to training the original model, providing a more efficient way to fine-tune LLMs compared to other methods like adapter-based or prefix-based approaches. Additionally, when deployed in production, LoRA introduces no additional inference latency, allowing for seamless computation and storage of the updated weights for inference.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"1. Write a simple summary of the LoRA technique in bullet points, and then 2. send that to lucasbnsoares@hotmail.com\")\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 51585cf4-fc5e-4510-a952-c7c3af9e2f82. Step input: \n",
      "I need you to think step by step and perform these actions: \n",
      "1. Retrieve information from the lora_paper tool, \n",
      "2. write a simple summary of the LoRA technique in bullet points, \n",
      "3. send that to lucasbnsoares@hotmail.com\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: I need to follow the steps you provided. First, I'll use the lora_paper tool to retrieve information about the LoRA technique.\n",
      "Action: lora_paper\n",
      "Action Input: {'input': 'LoRA technique for efficient fine-tuning of LLMs'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as it allows for explicit computation and storage of the modified weight matrices for efficient inference.\n",
      "\u001b[0m> Running step f64765a8-68cf-4fcc-bc41-b0782a8f7f0a. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: Now that I have the information about the LoRA technique, I'll write a simple summary in bullet points.\n",
      "Action: None\n",
      "Action Input: {'to': 'lucasbnsoares@hotmail.com', 'subject': 'Summary of LoRA Technique', 'body': 'The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as it allows for explicit computation and storage of the modified weight matrices for efficient inference.'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `None`.\n",
      "\u001b[0m> Running step 89576c60-dc1c-40a2-9694-41c4dc9e676c. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I need to use the send_email tool to send the email.\n",
      "Action: send_email\n",
      "Action Input: {'to': 'lucasbnsoares@hotmail.com', 'subject': 'Summary of LoRA Technique', 'body': 'The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as it allows for explicit computation and storage of the modified weight matrices for efficient inference.'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully!\n",
      "\u001b[1;3;34mObservation: None\n",
      "\u001b[0m> Running step 231a4142-71bf-46bf-95ac-7ce94962cbb9. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems that the send_email tool did not return any output. This might be because the email was sent successfully, but the tool does not return any output in that case. I'll assume that the email was sent successfully.\n",
      "\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as it allows for explicit computation and storage of the modified weight matrices for efficient inference.\n",
      "\u001b[0mThe LoRA technique allows for efficient fine-tuning of Language Model Models (LLMs) by not requiring the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that by setting the LoRA rank to the rank of the pre-trained weight matrices, LoRA roughly recovers the expressiveness of full fine-tuning. Additionally, when deployed in production, LoRA introduces no additional inference latency as it allows for explicit computation and storage of the modified weight matrices for efficient inference.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"\"\"\n",
    "I need you to think step by step and perform these actions: \n",
    "1. Retrieve information from the lora_paper tool, \n",
    "2. write a simple summary of the LoRA technique in bullet points, \n",
    "3. send that to lucasbnsoares@hotmail.com\n",
    "\"\"\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/2024-12-02-18-53-16.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
