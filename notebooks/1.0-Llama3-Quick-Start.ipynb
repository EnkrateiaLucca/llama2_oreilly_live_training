{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Llama 3\n",
    "\n",
    "## Accessing Llama 3\n",
    "\n",
    "1. Download + Self Host (i.e. download Llama weights directly with Hugging Face or from Meta.ai. \n",
    "2. Use tools and frameworks like [Ollama](https://ollama.ai/)), llama-cpp, LangChain.\n",
    "3. Hosted API Platform (e.g. Groq, Replicate, Together, Anyscale)\n",
    "4.  Hosted Container Platform (e.g. Azure, AWS, GCP)\n",
    "\n",
    "## Official Setup Downloading Weights\n",
    "Running Meta Llama 3 on Google Colab using Hugging Face transformers library\n",
    "This notebook goes over how you can set up and run Llama 3 using Hugging Face transformers library Open In Colab\n",
    "\n",
    "To use the downloads on Hugging Face, you must first request a download as shown in the steps below making sure that you are using the same email address as your Hugging Face account.\n",
    "\n",
    "<img src=\"./assets-resources/huggingface-request-accepted.png\" width=50%>\n",
    "\n",
    "To use already converted weights, start here:\n",
    "\n",
    "1. Request download of model weights from the Llama website\n",
    "2. Login to Hugging Face from your terminal using the same email address as (1). Follow the instructions [here](https://huggingface.co/docs/huggingface_hub/en/quick-start).\n",
    "3. Run the example\n",
    "\n",
    "Else, if you'd like to download the models locally and convert them to the HF format, follow the steps below to convert the weights:\n",
    "\n",
    "1. Request download of model weights from the Llama website\n",
    "2. Clone the llama repo and get the weights\n",
    "3. Convert the model weights\n",
    "4. Prepare the script\n",
    "5. Run the example\n",
    "\n",
    "\n",
    "\n",
    "Using already converted weights\n",
    "\n",
    "1. Request download of model weights from the Llama website [here](https://llama.meta.com/).\n",
    "2. Fill the required information, select the models “Meta Llama 3” and accept the terms & conditions. You will receive a URL in your email in a short time.\n",
    "3. Install the Transformers library and Accelerate library for our demo.\n",
    "\n",
    "The Transformers library provides many models to perform tasks on texts such as classification, question answering, text generation, etc. The accelerate library enables the same PyTorch code to be run across any distributed configuration of GPUs and CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**\n",
    "\n",
    "Below, all the setup instructions were taken from the official llama-recipes github repo from Meta, you can check that out [here](https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Running_Llama3_Anywhere/Running_Llama_on_HF_transformers.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade huggingface_hub # for logging into Hugging Face\n",
    "%pip install transformers # for loading the model\n",
    "%pip install accelerate # for distributed training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2955e0294ece45da9e3a6a07530b88cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b787ea028f4603b0a9d435d39e3dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a broken computer that only outputs's error messages regardless of user input. <|eot_id|> <|start_header_id|>user<|end_header_id|> Hi! Tell me a joke! <|eot_id|> <|start_header_id|>assistant<|end_header_id|> \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a broken computer that only outputs's error messages regardless of user input. <|eot_id|> <|start_header_id|>user<|end_header_id|> Hi! Tell me a joke! <|eot_id|> <|start_header_id|>assistant<|end_header_id|>  **ERROR: JOKES DATABASE NOT FOUND. PLEASE REBOOT AND TRY AGAIN.**<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "response = model.generate(**input_ids, max_length=512)\n",
    "extracted_text = tokenizer.decode(response[0])\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"Llama 3.2 follows the same prompt template as Llama 3.1, with a new special token <|image|> representing the input image for the multimodal models.\"](https://arc.net/l/quote/eeitnmkn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: {'input_ids': tensor([[128000,    220, 128000, 128006,   9125, 128007,   1472,    527,    264,\n",
      "          11190,  18328,     13,    220, 128009,    220, 128006,    882, 128007,\n",
      "          83017,    279,  57940,   4920,    279,  87970,  18112,    304,    279,\n",
      "           2317,    315,   3544,   4221,   4211,     13,    220, 128009,    220,\n",
      "         128006,  78191, 128007,    220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Response: tensor([[128000,    220, 128000, 128006,   9125, 128007,   1472,    527,    264,\n",
      "          11190,  18328,     13,    220, 128009,    220, 128006,    882, 128007,\n",
      "          83017,    279,  57940,   4920,    279,  87970,  18112,    304,    279,\n",
      "           2317,    315,   3544,   4221,   4211,     13,    220, 128009,    220,\n",
      "         128006,  78191, 128007,    220,   5810,    596,    459,  24131,    315,\n",
      "            279,  43678,  18112,    304,    279,   2317,    315,   3544,   4221,\n",
      "           4211,   1473,    334,  38255,  57277,    791,  43678,  18112,    574,\n",
      "          11784,    304,    220,    679,     22,    555,  53934,     86,   5676,\n",
      "           1880,    453,     13,    304,    279,   5684,    330,  70429,    374,\n",
      "           2052,   1472,  14998,   3343,   1102,  14110,   1534,    279,   2115,\n",
      "            315,   5933,   4221,   8863,    320,     45,  12852,      8,    555,\n",
      "           8405,    264,    810,  11297,    323,   7524,   1648,    311,   1920,\n",
      "          52100,    828,     11,   1778,    439,   1495,     13,    578,  43678,\n",
      "          18112,    706,   2533,   3719,    279,  16665,    369,   1690,   1614,\n",
      "           8838,  10826,  38921,   4221,   4211,     11,   2737,   1884,   1511,\n",
      "            304,   8522,   1093,   4221,  14807,     11,   1495,  29385,   2065,\n",
      "             11,    323,   7669,   1697,  15592,    382,    334,   1622,  35185,\n",
      "          57277,    791,  43678,  18112,  17610,    315,    279,   2768,   1401,\n",
      "           6956,   1473,     16,     13,   3146,  12363,     12,  70429,  28901,\n",
      "           2191,  96618,    578,    659,     12,  54203,  17383,   6276,    279,\n",
      "           1646,    311,  17988,    279,  12939,    315,   2204,   1988,   5540,\n",
      "           8844,    311,   1855,   1023,     13,   1102,    596,    264,   1401,\n",
      "          19297,    430,  20682,    279,   1646,    311,   5357,    389,   9959,\n",
      "           5596,    315,    279,   1988,   8668,    627,     17,     13,   3146,\n",
      "          20516,     12,  21394,  38943,  96618,    578,  43678,  18112,    374,\n",
      "           6319,    439,    459,  24592,   6953,  41270,   1646,     13,    578,\n",
      "          24592,  11618,    279,   1988,   8668,    323,  19159,    264,  19815,\n",
      "          13340,     11,   1418,    279,  25569,  27983,    279,   2612,   8668,\n",
      "            627,     18,     13,   3146,  20981,     12,  12626,  63120,  96618,\n",
      "            578,   7447,  27488,   6666,  17383,   6276,    279,   1646,    311,\n",
      "          53258,   9604,    311,   2038,    505,   2204,  13340,   1207,  45385,\n",
      "            520,   2204,  10093,     13,   1115,    374,  17427,    555,  50802,\n",
      "            279,   1646,    596,   6666,   1139,   5361,   6666,  14971,    627,\n",
      "             19,     13,   3146,   3897,    278,  30430,  96618,   2057,  21813,\n",
      "            279,   2015,    315,    279,   1988,   8668,     11,  68647,  11418,\n",
      "            374,   1511,    311,    923,    264,  76914,  71916,   8450,    311,\n",
      "            279,   1988,  71647,    627,     20,     13,   3146,  30016,     12,\n",
      "          27014,   8304,    320,   1785,     45,  33395,     25,    578,  31042,\n",
      "             45,    374,    264,   7373,   8599,   4009,   1511,    311,   5276,\n",
      "            279,   2612,    315,    279,   6666,  17383,    382,    334,   4438,\n",
      "            433,  21785,  57277,   8586,    596,    264,   1579,  11852,  24131,\n",
      "            315,    279,  43678,  18112,   1473,     16,     13,   3146,   2566,\n",
      "          38168,  25624,  96618,    578,   1988,   8668,    374,  23711,   1139,\n",
      "            264,   4724,   3634,   1701,   9687,  71647,    627,     17,     13,\n",
      "           3146,  20516,  96618,    578,  24592,  17610,    315,    264,   5729,\n",
      "            315,  20086,  13931,     11,   1855,    315,    902,  17208,    659,\n",
      "             12,  54203,    323,  31042,     45,    627,     18,     13,   3146,\n",
      "          12363,     12,  70429,  96618,    578,    659,     12,  54203,  17383,\n",
      "            374,   9435,    311,    279,   1988,  71647,    311,  12849,    279,\n",
      "          14661,    369,    279,   6666,  17383,    627,     19,     13,   3146,\n",
      "          20981,     12,  12626,  63120,  96618,    578,   7447,  27488,   6666,\n",
      "          17383,    374,   9435,    311,    279,   2612,    315,    279,    659,\n",
      "             12,  54203,  17383,    627,     20,     13,   3146,  30016,     12,\n",
      "          27014,   8304,    320,   1785,     45,  33395,     25,    578,  31042,\n",
      "             45,    374,   9435,    311,    279,   2612,    315,    279,   7447,\n",
      "          27488,   6666,  17383,    627,     21,     13,   3146,  21394,  96618,\n",
      "            578,  25569,    374,   4528,    311,    279,  24592,     11]])\n",
      "Extracted text: <|begin_of_text|> <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant. <|eot_id|> <|start_header_id|>user<|end_header_id|> Explain the fundamentals behind the transformers architecture in the context of large language models. <|eot_id|> <|start_header_id|>assistant<|end_header_id|>  Here's an overview of the transformer architecture in the context of large language models:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "The transformer architecture was introduced in 2017 by Vaswani et al. in the paper \"Attention is All You Need\". It revolutionized the field of natural language processing (NLP) by providing a more efficient and effective way to process sequential data, such as text. The transformer architecture has since become the foundation for many state-of-the-art language models, including those used in applications like language translation, text summarization, and conversational AI.\n",
      "\n",
      "**Key Components**\n",
      "\n",
      "The transformer architecture consists of the following key components:\n",
      "\n",
      "1. **Self-Attention Mechanism**: The self-attention mechanism allows the model to weigh the importance of different input elements relative to each other. It's a key innovation that enables the model to focus on relevant parts of the input sequence.\n",
      "2. **Encoder-Decoder Architecture**: The transformer architecture is designed as an encoder-decoder model. The encoder processes the input sequence and produces a continuous representation, while the decoder generates the output sequence.\n",
      "3. **Multi-Head Attention**: The multi-head attention mechanism allows the model to jointly attend to information from different representation subspaces at different positions. This is achieved by dividing the model's attention into multiple attention heads.\n",
      "4. **Positional Encoding**: To preserve the order of the input sequence, positional encoding is used to add a sinusoidal signal to the input embeddings.\n",
      "5. **Feed-Forward Network (FFN)**: The FFN is a fully connected network used to transform the output of the attention mechanism.\n",
      "\n",
      "**How it Works**\n",
      "\n",
      "Here's a high-level overview of the transformer architecture:\n",
      "\n",
      "1. **Input Embeddings**: The input sequence is embedded into a vector space using learned embeddings.\n",
      "2. **Encoder**: The encoder consists of a stack of identical layers, each of which applies self-attention and FFN.\n",
      "3. **Self-Attention**: The self-attention mechanism is applied to the input embeddings to compute the weights for the attention mechanism.\n",
      "4. **Multi-Head Attention**: The multi-head attention mechanism is applied to the output of the self-attention mechanism.\n",
      "5. **Feed-Forward Network (FFN)**: The FFN is applied to the output of the multi-head attention mechanism.\n",
      "6. **Decoder**: The decoder is similar to the encoder,\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant. <|eot_id|> <|start_header_id|>user<|end_header_id|> Explain the fundamentals behind the transformers architecture in the context of large language models. <|eot_id|> <|start_header_id|>assistant<|end_header_id|> \"\"\"\n",
    "\n",
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "print(\"Input IDs:\", input_ids)\n",
    "response = model.generate(**input_ids, max_length=512)\n",
    "print(\"Response:\", response)\n",
    "extracted_text = tokenizer.decode(response[0])\n",
    "print(\"Extracted text:\", extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/Users/greatmaster/miniconda3/envs/oreilly-llama3/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The key to life is to get up and get going, to do something, to be something'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe(\"The key to life is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on the Prompt Template for Llama 3.1 & 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/llama31-template-prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multiturn-conversation with Meta Llama 3.1 that includes tool-calling follows this structure:\n",
    "\n",
    "```<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ user_message_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "<|python_tag|>{{ model_tool_call_1 }}<|eom_id|><|start_header_id|>ipython<|end_header_id|>\n",
    "\n",
    "{{ tool_response }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{model_response_based_on_tool_response}}<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this course we will mainly work with ollama which is the easiest way to get started with Llama3 locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go [here](https://ollama.ai/) and download the ollama executable for your OS.\n",
    "2. Then you can use it within the terminal\n",
    "\n",
    "    ```ollama run llama3.1```\n",
    "\n",
    "\n",
    "3. If the llama3.1 model is not yet downloaded Ollama will downloaded for you and start a chat conversation with Llama3.1 8B instruct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM Studio for easy download models:\n",
    "\n",
    "https://lmstudio.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run it as a a REST API call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -X POST http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.1\",\n",
    "  \"prompt\":\"Why is the sky blue?\"\n",
    " }'\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you get in return is a stream with the API response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book \"The Godfather\" is actually a novel by Mario Puzo, published in 1969. It's a crime fiction novel that explores the world of organized crime and the rise of Don Vito Corleone as the head of a powerful Italian-American Mafia family.\n",
      "\n",
      "However, if you're thinking of the famous film adaptation \"The Godfather\", it was written by Mario Puzo, who also wrote the screenplay for the movie. The film was directed by Francis Ford Coppola and released in 1972, starring Marlon Brando as Don Vito Corleone.\n",
      "\n",
      "Mario Puzo's novel was a huge success and provided the basis for the famous film trilogy, which includes \"The Godfather\" (1972), \"The Godfather: Part II\" (1974), and \"The Godfather: Part III\" (1990).\n",
      "\n",
      "Let me know if you have any other questions!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "def llama3(prompt):\n",
    "    data = {\n",
    "        \"model\": \"llama3.1\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    return(response.json()['message']['content'])\n",
    "\n",
    "response = llama3(\"who wrote the book godfather\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory requirements for Llama 3.1:\n",
    "\n",
    "![](./assets-resources/llama31-memory-requirements.png)\n",
    "\n",
    "[source](https://arc.net/l/quote/dldvrqtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `llama-cpp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M3 Max) - 98303 MiB free\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./models/meta-llama-3.1-8b-instruct-hf-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct Hf\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-hf\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct Hf\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 128000\n",
      "llama_new_context_with_model: n_ctx_per_seq = 128000\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (128000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x1242881a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x1146dd410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1146ec0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x16a7a8520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x16a2ec6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x16a2ee570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x16a2edf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x16a2efa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x1146ec390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1146ec650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x1146ecc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x12428c980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x12428d920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1146ed220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x1146edbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x12428e5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x12428f330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x16a0fc340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x16a0fd0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x16a2efd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x1242900d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x103e5f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x16a7a7ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x16a2f0530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x16a2f0a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x16a2f2b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x1146edeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1146ee540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x1146eea50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1146ef2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x1146ef9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x16a7a8bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1146f0700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1146f0df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x16a7a9370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x16a0fe420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x16a0fe890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1146f1180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x16a2f26f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x16a7a8e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x16a7aa850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1146f1440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x16a2f0f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x16a7a9ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x16a7ab1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x16a7ab5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1146f1800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x16a2f4720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1146f20e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1146f2510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1146f3460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x16a7ac030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x1146f2d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1146f4290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x1146f4970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1146f3950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1146f5700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1246852a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x16a2f3850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1146f4f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x16a7ac5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1146f6640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1146f6c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x16a7ada90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x16a7adef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x16a2f4a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x16a2f5250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x16a2f6650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x16a2f59f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x16a7ae700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x16a2f6910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x16a7ae9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11192efd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x16bd0d6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x16a7acb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124685560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x16a2f62b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x16a2f78d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124290420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x16a7aefb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x16a2f6d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x16a7afcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x16a2f8070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124685820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x16a2f9d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x16a7b0d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1146f7810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x16a7b15b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x16a2fa9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1146f83a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x16a2fb560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124290d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104746590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124290770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1242928a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x16bd0d990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106c79ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x16a0ff5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x16a2fb820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x16a2fbb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106c7afc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124685c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x16a2fc090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104747280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1146f8940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x16a7b1bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x16a7b1f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1146f9100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124292d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x16a7b30d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1146f97f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x16a7b2400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x16a7b2a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x16a7b4a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1242931b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x16a7b5240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x16a2fce20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x16a2fd420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1146f9ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x16a7b59e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x16a2fd8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x16a2fdfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x16a7b4440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x16a7b3ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1146fa890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x16a2fc760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x16a7b7230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1242935a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x16a2feea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x16a7b7af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x16a7b8fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x16a7b9280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124294630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1146fb6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1146faea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x16a2ff3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124685f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117f7cc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124295c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124686990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x16a7b9ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1146fc560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x16a7ba1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x16a7ba470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x103e60820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x106c7b720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x106c7bb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x106c7d0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x106c7c540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x16bd0e210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1047480f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104748df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x16bd0ef80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x16bd0f990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x16bd0fec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x16a7bb260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x16bd11000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117f7d3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x16a0fdc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11192f950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104749a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x16bd112c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10474a670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x16bd117f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1047494f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x16bd11f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1119307a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106c7d470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x16bd13080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11080d850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106c7dbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10474b0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106c7eb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106c7e460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x16bd13ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10474bfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x16bd127b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10474c6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10474d610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106c80460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106c80850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10474b7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10474e1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x16bd14c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10474f940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10474e9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10474fdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x111930c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106c80cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104750360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106c81320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x16bd15600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x16bd16a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104750bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11080d300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x16bd170a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106c833d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x16bd180a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x16bd17420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1047520e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106c82120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104752950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x16bd17980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106c82c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106c845d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x16bd18820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106c84f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x16bd18c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x16bd198e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x16bd19340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104751310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1146fbfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11080cc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x16bd1a3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106c85450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106c85a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106c85d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x104752ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x16bd1ab30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x106c86750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x106c86fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x104753160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1146fce40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1146fd4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117f7d6b0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size = 16000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 16000.00 MiB, K (f16): 8000.00 MiB, V (f16): 8000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  8282.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'smaug-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '131072', 'general.finetune': 'Instruct-hf', 'llama.attention.head_count': '32', 'general.file_type': '15', 'general.size_label': '8B', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'general.type': 'model', 'general.name': 'Meta Llama 3.1 8B Instruct Hf'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "llama_perf_context_print:        load time =    2337.83 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2632.29 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-b025dc16-99b7-4d14-b650-8e113fcfac3e', 'object': 'text_completion', 'created': 1733136075, 'model': './models/meta-llama-3.1-8b-instruct-hf-q4_k_m.gguf', 'choices': [{'text': 'The capital of France is Paris.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 32, 'completion_tokens': 7, 'total_tokens': 39}}\n"
     ]
    }
   ],
   "source": [
    "# %pip install llama-cpp-python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# https://huggingface.co/YorkieOH10/Meta-Llama-3.1-8B-Instruct-hf-Q4_K_M-GGUF/tree/main\n",
    "# Initialize llm\n",
    "llm = Llama(\n",
    "    model_path=\"./models/meta-llama-3.1-8b-instruct-hf-q4_k_m.gguf\",\n",
    "    n_ctx=128000\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "\n",
    "<|start_header_id|>system<|end_header_id|> \n",
    "You are a helpful assistant\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the capital of France?\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "output = llm.create_completion(\n",
    "      prompt,\n",
    "      max_tokens=128,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# The output from this version is of lower quality due to quantization\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M3 Max) - 98298 MiB free\n",
      "llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from ./models/Llama 3.2 3B Instruct Q6.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q6_K:  197 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 2.45 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q6_K) (and 282 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/29 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  2513.90 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 128000\n",
      "llama_new_context_with_model: n_ctx_per_seq = 128000\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (128000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x106c4c6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x16bd1ffc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x111932170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x1047542e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x106c4caa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x1108a3490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x111932980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x106c4d910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x16bdb2890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x104754c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x104755800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x111932f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x111933660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x111933f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x111934a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x111935100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x106c4de50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x111935930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x103e60f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x104755d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x1108a4070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x1119361a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x106c4e840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x106c4f0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x106c4f370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x16bd20e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x111936460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x111936a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x104756860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106c4ff00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x104756e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x16bd212c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1119370b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x16bd1f7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104757440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x103e612b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x111937500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1108a4630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x103e61640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103e62130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x16bd21c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106c501c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106c50c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x111938000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x16bd22090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1108a4a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1047577c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x103e62770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1108a5420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x16bd22900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x16bd22ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x16bd23570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x104758060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x106c51130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x1146e6c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1108a56e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1119382c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x103e630a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1146e61a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x16bd23b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124296470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0xa45d876f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124296ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x111938e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1108a5c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1108a63e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1146eb2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106c522d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106c51c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1119396a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x103e61ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x111939960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104757ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1242971d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x16a132ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124297b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1146eac30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x16a133330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x16a133ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106c513f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0xa46715460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1146fdaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0xa45d87ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0xa46715aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0xa467165b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1146fe720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1146fe270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x16a134110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1146fedc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117d042f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0xa45d88620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0xa45d8a060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117d1aca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0xa456a4aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x16a134810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x16a1362f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117d1b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0xa456a51f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0xa456a5d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0xa456a6830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x16a1351e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117d1b760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x16a136a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117d1bde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x16a136d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0xa456a6b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104758520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104758e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104759350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x103e63360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1047597d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1108a6b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0xa46717820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10475a010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10475a2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117d1c0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10475a9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10475b240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0xa467197f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x16a137810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124299660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x16a135f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x16a138100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x16a138600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0xa45d8a580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0xa45d8ad10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117d1c870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117d1d050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0xa4671a2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0xa46718930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x16a138d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10475bd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10475c490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10475caf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0xa4671ab70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117d1d7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0xa4671b310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117d1de30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10475cfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10475d270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0xa45d8b4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0xa4671c1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117d1e5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0xa4671c800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x16a139010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0xa46719c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x16a139790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0xa45d8bb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x10475d8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0xa456a43a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x16a13a2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0xa456a8330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0xa45d8c4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x16a1365b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0xa45d8ccd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0xa45d8d170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0xa45d8bf20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117d1f3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117d1fa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0xa4671bb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0xa45d8de50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x16a13b520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0xa45d8eca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x16a13bd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0xa456a8c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0xa4671d640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0xa45d8fa20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x16a13a5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0xa4671d930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10475db80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106c516b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0xa4671ddb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0xa45d8d850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117d1ff20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0xa4671e800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0xa456a7080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0xa45d8d430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117d20820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0xa4671f3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0xa4671ff60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0xa4671f850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117d20ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10475df50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x16a13c530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x16a13ccc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x16a13d450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124298390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12429a5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x16a13dbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x16a13e370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x16a13af20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x16a13eb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x16a13f4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117d20f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103e638b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117d223f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117d22b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117d23310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x16a13fc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x16a1403c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x16a140b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x16a1412e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117d23b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x16a141a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0xa45d90340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x16a142200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0xa456a9a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0xa45d8f170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0xa45d8e620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0xa45d91800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0xa45d91ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10475e210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106c87ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0xa45d92cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x16a142990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0xa45d931a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0xa456a9ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x106c881b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x16a143670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0xa46720ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0xa4671ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0xa467213f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x103e63b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117d21430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x111939d30 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size = 14000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 14000.00 MiB, K (f16): 7000.00 MiB, V (f16): 7000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  6274.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': '/models_out/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '3072', 'llama.vocab_size': '128256', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.attention.value_length': '128', 'llama.attention.head_count': '24', 'llama.attention.key_length': '128', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '18', 'llama.block_count': '28', 'general.size_label': '3B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '8192', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.2', 'quantize.imatrix.entries_count': '196', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Llama-3.2', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Llama 3.2 3B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "ggml_metal_free: deallocating\n",
      "llama_perf_context_print:        load time =    1119.05 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1297.10 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# %pip install llama-cpp-python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize llm\n",
    "llm = Llama(\n",
    "    model_path=\"./models/Llama 3.2 3B Instruct Q6.gguf\",\n",
    "    n_ctx=128000\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "\n",
    "<|start_header_id|>system<|end_header_id|> \n",
    "You are a helpful assistant\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the capital of France?\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "output = llm.create_completion(\n",
    "      prompt,\n",
    "      max_tokens=128,\n",
    ")\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Ollama models with Ollama Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cel if you don't have it installed\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama run llama3.2/llama3.1 etc....\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.2',\n",
       " 'created_at': '2025-03-12T15:57:28.176086Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"Pancakes as the ultimate breakfast choice - it's a topic of debate, but I'm here to make the case for why pancakes reign supreme. And, just for fun, I'll also argue against waffles!\\n\\n**The Case for Pancakes:**\\n\\n1. **Flexibility**: Pancakes are incredibly versatile. You can top them with sweet or savory ingredients, from fresh fruits and syrups to eggs, bacon, and even nutella!\\n2. **Texture**: Fluffy, soft, and light pancakes provide the perfect base for a satisfying breakfast experience.\\n3. **Convenience**: Pancakes are relatively easy to make, requiring only a simple batter mix and a skillet or griddle.\\n4. **Portion control**: A stack of 2-3 pancakes is the ideal serving size for most adults, making it a balanced breakfast option.\\n5. **Comfort food**: Pancakes evoke memories of warm, cozy mornings spent with family and friends.\\n\\n**The Case Against Waffles:**\\n\\n1. **Overcomplication**: Waffle makers can be finicky, requiring precise temperature control and batter consistency. This can lead to frustration and disappointment when the outcome is less than ideal.\\n2. **Texture contrast**: The crispy exterior and fluffy interior of waffles can create an unpleasant texture experience, especially for those who prefer a smooth breakfast.\\n3. **Limited customization options**: Waffle toppings are often limited by the shape of the waffle iron, making it harder to achieve the desired combination of flavors and textures.\\n4. **Cleaning hassle**: Waffle irons can be a pain to clean, with deep grooves and crevices that trap batter and debris.\\n5. **Overemphasis on presentation**: Waffles often require more attention to appearance than pancakes, which can lead to an overemphasis on aesthetics at the expense of taste and satisfaction.\\n\\nIn conclusion, pancakes offer a perfect balance of comfort, flexibility, and convenience, making them the ultimate breakfast choice. Meanwhile, waffles are often plagued by overcomplication, texture contrast, and cleaning hassle - leaving them in the dust!\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 5372198667,\n",
       " 'load_duration': 569208125,\n",
       " 'prompt_eval_count': 39,\n",
       " 'prompt_eval_duration': 837000000,\n",
       " 'eval_count': 421,\n",
       " 'eval_duration': 3964000000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3.2', \n",
    "                       messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why are pancakes the best breakfast? Make a case against waffles!',\n",
    "  },\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pancakes as the ultimate breakfast choice - it's a topic of debate, but I'm here to make the case for why pancakes reign supreme. And, just for fun, I'll also argue against waffles!\n",
       "\n",
       "**The Case for Pancakes:**\n",
       "\n",
       "1. **Flexibility**: Pancakes are incredibly versatile. You can top them with sweet or savory ingredients, from fresh fruits and syrups to eggs, bacon, and even nutella!\n",
       "2. **Texture**: Fluffy, soft, and light pancakes provide the perfect base for a satisfying breakfast experience.\n",
       "3. **Convenience**: Pancakes are relatively easy to make, requiring only a simple batter mix and a skillet or griddle.\n",
       "4. **Portion control**: A stack of 2-3 pancakes is the ideal serving size for most adults, making it a balanced breakfast option.\n",
       "5. **Comfort food**: Pancakes evoke memories of warm, cozy mornings spent with family and friends.\n",
       "\n",
       "**The Case Against Waffles:**\n",
       "\n",
       "1. **Overcomplication**: Waffle makers can be finicky, requiring precise temperature control and batter consistency. This can lead to frustration and disappointment when the outcome is less than ideal.\n",
       "2. **Texture contrast**: The crispy exterior and fluffy interior of waffles can create an unpleasant texture experience, especially for those who prefer a smooth breakfast.\n",
       "3. **Limited customization options**: Waffle toppings are often limited by the shape of the waffle iron, making it harder to achieve the desired combination of flavors and textures.\n",
       "4. **Cleaning hassle**: Waffle irons can be a pain to clean, with deep grooves and crevices that trap batter and debris.\n",
       "5. **Overemphasis on presentation**: Waffles often require more attention to appearance than pancakes, which can lead to an overemphasis on aesthetics at the expense of taste and satisfaction.\n",
       "\n",
       "In conclusion, pancakes offer a perfect balance of comfort, flexibility, and convenience, making them the ultimate breakfast choice. Meanwhile, waffles are often plagued by overcomplication, texture contrast, and cleaning hassle - leaving them in the dust!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ollama github](https://github.com/jmorganca/ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__You can use it with other python frameworks like LangChain:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The pancake-centric view of existence! This philosophical framework posits that, in a grand cosmic tapestry, pancakes are the fundamental building blocks and the very fabric of reality is woven from the fluffy, golden goodness of these round, flat treats.\\n\\nAccording to this pancake-driven cosmology, the universe began as a single, gigantic pancake, with all matter, energy, and space-time condensed into its perfect circle. This initial pancake, known as the \"Pancake Genesis,\" set the stage for the unfolding of creation.\\n\\nAs the universe expanded, smaller pancakes began to form, each one containing a unique combination of flavors and toppings that would eventually give rise to the diverse array of life forms we see today. The pancakes were not just mere foodstuffs; they represented the fundamental essence of existence: the harmony of sweet and savory, the balance of crunchy and smooth.\\n\\nThe laws of physics and mathematics are derived from the intricate patterns found within pancakes. The Fibonacci sequence, a mathematical formula governing the growth of plant cells, was actually inspired by the spiral arrangement of butterfat molecules in the fluffiest pancake. The curvature of space-time is reminiscent of the delicate creases on a well-made pancake.\\n\\nThe human experience, with all its complexities and struggles, is seen as an iterative process of refining and perfecting one\\'s pancake-making skills. Every decision, every action, and every emotion is reflected in the batter-to-griddle ratio, the timing of the pour, and the gentle fold of the spatula.\\n\\nIn this pancake-centric universe, time itself is measured in minutes and seconds, as these units dictate the optimal cooking temperature and duration for each pancake. The concept of free will is an illusion; every choice is guided by the instinctual knowledge that the perfect pancake requires a delicate balance of ingredients and technique.\\n\\nThe meaning of life, according to this cosmic pancake theory, can be distilled into a single phrase: \"To find your true purpose, seek out the fluffiest, most evenly cooked pancake in existence.\" For when one achieves this holy grail, they will have unlocked the secrets of the universe and found their place within the grand tapestry of pancake-filled reality.\\n\\nAnd so, dear friend, if you\\'re feeling lost or uncertain about your place in the world, just remember: the answer lies in the pancake.', response_metadata={'model': 'llama3.2', 'created_at': '2024-12-02T10:46:13.983505Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 5106334792, 'load_duration': 30256292, 'prompt_eval_count': 41, 'prompt_eval_duration': 156000000, 'eval_count': 478, 'eval_duration': 4919000000}, id='run-d349b318-a473-43b5-b78e-98d2f1879448-0', usage_metadata={'input_tokens': 41, 'output_tokens': 478, 'total_tokens': 519})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "llm.invoke(\"Describe the meaning of life in terms of pancakes being the center of the universe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
