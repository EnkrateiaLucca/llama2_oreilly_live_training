{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37f7184",
   "metadata": {},
   "source": "# Getting Started with Local LLMs using Ollama\n\nThis notebook demonstrates how to run local language models using [Ollama](https://ollama.com/), an easy-to-use tool for running LLMs locally on your machine.\n\n## Prerequisites\n\n1. Install Ollama from https://ollama.com/download\n2. Pull a model (e.g., `ollama pull llama3.2`)\n3. Ensure Ollama is running in the background\n\n## What You'll Learn\n\n- How to interact with local LLMs using the Ollama Python library\n- Making basic API calls to local models\n- Listing available models\n- Creating reusable helper functions for LLM interactions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a39c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ush3ibvzpe",
   "source": "## Step 1: Install the Ollama Python Library",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1218a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, everyone, and welcome to our course \"Getting Started with Llama 3\" - I'm excited to help you get started on this journey into the world of conversational AI!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Say hello to my students for the course: \"Getting Started with Llama 3\". One single sentence.',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ejbcxokvgm",
   "source": "## Step 2: Your First Chat with a Local LLM\n\nThe `ollama.chat()` function sends messages to a local model and returns the response.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b597e",
   "metadata": {},
   "outputs": [],
   "source": "# Try with a different model - you can easily switch between models\nimport ollama\n\nresponse = ollama.chat(model='gemma3n', messages=[\n  {\n    'role': 'user',\n    'content': 'Say hello to my students for the course: \"Getting Started with Llama 3\". One single sentence.',\n  },\n])\nprint(response['message']['content'])"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4eef295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     ID              SIZE      MODIFIED      \n",
      "gemma3n:latest                           15cb39fd9394    7.5 GB    3 weeks ago      \n",
      "mistral-small3.2:latest                  5a408ab55df5    15 GB     4 weeks ago      \n",
      "llama3.2-vision:11b                      6f2f9757ae97    7.8 GB    5 weeks ago      \n",
      "llava:latest                             8dd30f6b0cb1    4.7 GB    5 weeks ago      \n",
      "qwen3:8b                                 500a1f067a9f    5.2 GB    6 weeks ago      \n",
      "snowflake-arctic-embed:xs                dfbdc120552e    45 MB     6 weeks ago      \n",
      "phi4:latest                              ac896e5b8b34    9.1 GB    6 weeks ago      \n",
      "qwen2.5vl:latest                         5ced39dfa4ba    6.0 GB    7 weeks ago      \n",
      "qwen3:4b                                 2bfd38a7daaf    2.6 GB    8 weeks ago      \n",
      "Osmosis/Osmosis-Structure-0.6B:latest    f24ec096ac55    1.2 GB    2 months ago     \n",
      "qwen3:32b                                e1c9f234c6eb    20 GB     2 months ago     \n",
      "qwen3:latest                             e4b5fd7f8af0    5.2 GB    2 months ago     \n",
      "gemma3:27b                               a418f5838eaf    17 GB     3 months ago     \n",
      "gemma3:latest                            a2af6cc3eb7f    3.3 GB    3 months ago     \n",
      "qwq:latest                               cc1091b0e276    19 GB     4 months ago     \n",
      "mxbai-embed-large:latest                 468836162de7    669 MB    5 months ago     \n",
      "deepseek-r1:14b                          ea35dfe18182    9.0 GB    6 months ago     \n",
      "deepseek-r1:latest                       0a8c26691023    4.7 GB    6 months ago     \n",
      "llama3.3:latest                          a6eb4748fd29    42 GB     7 months ago     \n",
      "llama3.2-vision:latest                   38107a0cd119    7.9 GB    8 months ago     \n",
      "llama3.2:latest                          a80c4f17acd5    2.0 GB    10 months ago    \n",
      "dolphin-llama3:latest                    613f068e29f8    4.7 GB    10 months ago    \n",
      "nomic-embed-text:latest                  0a109f422b47    274 MB    11 months ago    \n",
      "llama3.1:8b                              62757c860e01    4.7 GB    12 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qotpfczwjkp",
   "source": "## Step 3: List Available Models\n\nYou can list all models available locally using the `ollama list` command.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c3c30",
   "metadata": {},
   "outputs": [],
   "source": "def chat_with_local_llm(model_name: str, prompt: str) -> str:\n    \"\"\"\n    Send a prompt to a local LLM via Ollama and return the response.\n    \n    Args:\n        model_name: The name of the model to use (e.g., 'llama3.2', 'gemma3n')\n        prompt: The user prompt to send to the model\n    \n    Returns:\n        The model's response as a string\n    \"\"\"\n    response = ollama.chat(\n        model=model_name,  # Use the passed model_name parameter\n        messages=[\n            {\n                'role': 'user',\n                'content': prompt,\n            },\n        ]\n    )\n    output = response['message']['content']\n    print(output)\n    \n    return output\n\n# Example usage\nchat_with_local_llm(\n    \"llama3.2\", \n    'Write a simple Python function that calculates the factorial of a number.'\n)"
  },
  {
   "cell_type": "markdown",
   "id": "nmxfe3k4nsi",
   "source": "## Summary\n\nIn this notebook, you learned how to:\n\n1. **Install** the Ollama Python library\n2. **Chat** with local LLMs using `ollama.chat()`\n3. **List** available models with `ollama list`\n4. **Create** a reusable helper function for LLM interactions\n\n## Next Steps\n\n- Explore more models: `ollama pull llama3.3` for the latest Llama model\n- Check out structured outputs in notebook `3.1-llama31-structured-outputs.ipynb`\n- Learn about RAG with local LLMs in notebook `2.0-introduction-to-rag.ipynb`\n\nFor more information, visit the [Ollama Documentation](https://ollama.com/)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6ial4q8st6j",
   "source": "## Step 4: Create a Reusable Helper Function\n\nLet's create a reusable function for chatting with local models.",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}