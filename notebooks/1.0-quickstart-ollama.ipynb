{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37f7184",
   "metadata": {},
   "source": [
    "# Getting Started with Local LLMs using Ollama\n",
    "\n",
    "This notebook demonstrates how to run local language models using [Ollama](https://ollama.com/), an easy-to-use tool for running LLMs locally on your machine.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Install Ollama from https://ollama.com/download\n",
    "2. Pull a model (e.g., `ollama pull llama3.2`)\n",
    "3. (alternative see screenshot)\n",
    "<!-- ![](2026-02-03-16-01-24.png) -->\n",
    "1. Ensure Ollama is running in the background\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to interact with local LLMs using the Ollama Python library\n",
    "- Making basic API calls to local models\n",
    "- Listing available models\n",
    "- Creating reusable helper functions for LLM interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3bccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a39c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ush3ibvzpe",
   "metadata": {},
   "source": [
    "## Step 1: Install the Ollama Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1218a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, everyone, to \"Getting Started with Llama 3,\" where we'll explore the capabilities and applications of this powerful conversational AI model.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"llama3.2\", \n",
    "                       messages=[\n",
    "                         {\n",
    "                           \"role\": \"user\",\n",
    "                           \"content\": \"Say hello to my students for the course: 'Getting Started with Llama 3'. One single sentence.\"\n",
    "                         }\n",
    "                       ]\n",
    "                       )\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ejbcxokvgm",
   "metadata": {},
   "source": [
    "## Step 2: Your First Chat with a Local LLM\n",
    "\n",
    "The `ollama.chat()` function sends messages to a local model and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3b597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to \"Getting Started with Llama 3\" â€“ I'm excited to embark on this journey of exploring powerful language models with all of you!\n"
     ]
    }
   ],
   "source": [
    "# Try with a different model - you can easily switch between models\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='gemma3n', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Say hello to my students for the course: \"Getting Started with Llama 3\". One single sentence.',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4eef295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     ID              SIZE      MODIFIED       \n",
      "llama3.2:latest          a80c4f17acd5    2.0 GB    19 seconds ago    \n",
      "gemma3:latest            a2af6cc3eb7f    3.3 GB    2 months ago      \n",
      "gemma3:1b                8648f39daa8f    815 MB    6 months ago      \n",
      "gemma3n:latest           15cb39fd9394    7.5 GB    7 months ago      \n",
      "gemma3:27b               a418f5838eaf    17 GB     9 months ago      \n",
      "dolphin-llama3:latest    613f068e29f8    4.7 GB    16 months ago     \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qotpfczwjkp",
   "metadata": {},
   "source": [
    "## Step 3: List Available Models\n",
    "\n",
    "You can list all models available locally using the `ollama list` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8c3c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By 2026, Large Language Models (LLMs) are expected to revolutionize various industries such as customer service, content creation, and education by providing fast, accurate, and personalized assistance that augments human capabilities, but also raises concerns about job displacement, data privacy, and the blurring of lines between human creativity and machine-generated output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'By 2026, Large Language Models (LLMs) are expected to revolutionize various industries such as customer service, content creation, and education by providing fast, accurate, and personalized assistance that augments human capabilities, but also raises concerns about job displacement, data privacy, and the blurring of lines between human creativity and machine-generated output.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_with_local_llm(model_name: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a prompt to a local LLM via Ollama and return the response.\n",
    "    \n",
    "    Args:\n",
    "        model_name: The name of the model to use (e.g., 'llama3.2', 'gemma3n')\n",
    "        prompt: The user prompt to send to the model\n",
    "    \n",
    "    Returns:\n",
    "        The model's response as a string\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model_name,  # Use the passed model_name parameter\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    output = response['message']['content']\n",
    "    print(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "chat_with_local_llm(\n",
    "    \"llama3.2\", \n",
    "    'Write a one sentence prediction of the impact AI/LLMs will have on the world in 2026.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nmxfe3k4nsi",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Install** the Ollama Python library\n",
    "2. **Chat** with local LLMs using `ollama.chat()`\n",
    "3. **List** available models with `ollama list`\n",
    "4. **Create** a reusable helper function for LLM interactions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore more models: `ollama pull llama3.3` for the latest Llama model\n",
    "- Check out structured outputs in notebook `3.1-llama31-structured-outputs.ipynb`\n",
    "- Learn about RAG with local LLMs in notebook `2.0-introduction-to-rag.ipynb`\n",
    "\n",
    "For more information, visit the [Ollama Documentation](https://ollama.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ial4q8st6j",
   "metadata": {},
   "source": [
    "## Step 4: Create a Reusable Helper Function\n",
    "\n",
    "Let's create a reusable function for chatting with local models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
