{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization for Fine-Tuning LLMs: A Complete Guide\n",
    "\n",
    "## What is Quantization?\n",
    "\n",
    "**Quantization** is the process of reducing the precision of a model's parameters (weights and activations) by representing them with fewer bits. Instead of using 32-bit floating-point numbers (FP32), we can use 16-bit floats (FP16), 8-bit integers (INT8), or even 4-bit representations (INT4/NF4).\n",
    "\n",
    "## Why Does Quantization Matter for Fine-Tuning?\n",
    "\n",
    "When fine-tuning Large Language Models (LLMs), we face three main challenges:\n",
    "\n",
    "1. **Memory Constraints**: A 7B parameter model in FP32 requires ~28GB of VRAM just for the weights\n",
    "2. **Training Overhead**: Fine-tuning requires storing optimizer states (2x model size), gradients (1x model size), and activations\n",
    "3. **Hardware Limitations**: Most researchers don't have access to 80GB A100 GPUs\n",
    "\n",
    "**Quantization enables fine-tuning on consumer hardware** by reducing memory footprint by 75% (8-bit) or even 87.5% (4-bit) while maintaining model quality.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Precision Formats**: How FP32, FP16, INT8, and INT4 represent numbers\n",
    "2. **Memory Impact**: Quantifying the memory savings from quantization\n",
    "3. **Quantization Methods**: Symmetric vs asymmetric quantization\n",
    "4. **QLoRA & NF4**: State-of-the-art 4-bit quantization for fine-tuning\n",
    "5. **Practical Trade-offs**: When to use each precision format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/6.2-quantization-precision-format-code-explanation.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/6.2-quantization-precision-format-code-explanation.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstruct\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/6.2-quantization-precision-format-code-explanation.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/6.2-quantization-precision-format-code-explanation.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/6.2-quantization-precision-format-code-explanation.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Set style for better visualizations\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/6.2-quantization-precision-format-code-explanation.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sns\u001b[39m.\u001b[39mset_style(\u001b[39m\"\u001b[39m\u001b[39mwhitegrid\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Example weight value from a neural network\n",
    "weight_value = 0.3457\n",
    "\n",
    "# Function to visualize bit representation\n",
    "def float_to_bin(num, precision='float32'):\n",
    "    \"\"\"Convert float to binary representation\"\"\"\n",
    "    if precision == 'float32':\n",
    "        packed = struct.pack('!f', num)\n",
    "        bits = ''.join(f'{byte:08b}' for byte in packed)\n",
    "        return bits, 32\n",
    "    elif precision == 'float16':\n",
    "        packed = struct.pack('!e', num)\n",
    "        bits = ''.join(f'{byte:08b}' for byte in packed)\n",
    "        return bits, 16\n",
    "\n",
    "# Compare representations\n",
    "fp32_bits, fp32_size = float_to_bin(weight_value, 'float32')\n",
    "fp16_bits, fp16_size = float_to_bin(weight_value, 'float16')\n",
    "\n",
    "print(f\"Original value: {weight_value}\")\n",
    "print(f\"\\nFP32 (32-bit): {fp32_bits}\")\n",
    "print(f\"  â””â”€ Sign: {fp32_bits[0]} | Exponent: {fp32_bits[1:9]} | Mantissa: {fp32_bits[9:]}\")\n",
    "print(f\"  â””â”€ Memory: {fp32_size} bits = {fp32_size // 8} bytes\")\n",
    "\n",
    "print(f\"\\nFP16 (16-bit): {fp16_bits}\")\n",
    "print(f\"  â””â”€ Sign: {fp16_bits[0]} | Exponent: {fp16_bits[1:6]} | Mantissa: {fp16_bits[6:]}\")\n",
    "print(f\"  â””â”€ Memory: {fp16_size} bits = {fp16_size // 8} bytes\")\n",
    "\n",
    "# Show precision comparison\n",
    "print(f\"\\n{'Format':<10} {'Bits':<8} {'Range':<30} {'Precision':<15}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'FP32':<10} {'32':<8} {'Â±3.4e38':<30} {'~7 digits':<15}\")\n",
    "print(f\"{'FP16':<10} {'16':<8} {'Â±6.5e4':<30} {'~3 digits':<15}\")\n",
    "print(f\"{'INT8':<10} {'8':<8} {'-128 to 127':<30} {'Integer only':<15}\")\n",
    "print(f\"{'INT4':<10} {'4':<8} {'-8 to 7':<30} {'Integer only':<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate memory for a 7B parameter LLM\n",
    "model_size = 7_000_000_000  # 7 billion parameters\n",
    "\n",
    "# Memory in different formats (in GB)\n",
    "fp32_mem = (model_size * 4) / (1024**3)  # 4 bytes per parameter\n",
    "fp16_mem = (model_size * 2) / (1024**3)  # 2 bytes per parameter\n",
    "int8_mem = (model_size * 1) / (1024**3)  # 1 byte per parameter\n",
    "int4_mem = (model_size * 0.5) / (1024**3)  # 0.5 bytes per parameter\n",
    "\n",
    "print(\"Memory Requirements for 7B Parameter Model:\")\n",
    "print(f\"  FP32:  {fp32_mem:.2f} GB\")\n",
    "print(f\"  FP16:  {fp16_mem:.2f} GB ({fp32_mem/fp16_mem:.1f}x smaller)\")\n",
    "print(f\"  INT8:  {int8_mem:.2f} GB ({fp32_mem/int8_mem:.1f}x smaller)\")\n",
    "print(f\"  INT4:  {int4_mem:.2f} GB ({fp32_mem/int4_mem:.1f}x smaller)\")\n",
    "\n",
    "# Visualize\n",
    "formats = ['FP32', 'FP16', 'INT8', 'INT4']\n",
    "memory = [fp32_mem, fp16_mem, int8_mem, int4_mem]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(formats, memory, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "plt.ylabel('Memory (GB)', fontsize=12)\n",
    "plt.title('Memory Requirements for 7B LLM in Different Formats', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, mem in zip(bars, memory):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{mem:.1f} GB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample weights from a normal distribution (like real neural network weights)\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(10000) * 0.5  # Mean=0, std=0.5\n",
    "\n",
    "def symmetric_quantization(weights, n_bits=8):\n",
    "    \"\"\"Symmetric quantization using absolute maximum\"\"\"\n",
    "    # Calculate scale based on maximum absolute value\n",
    "    max_val = np.max(np.abs(weights))\n",
    "    scale = max_val / (2**(n_bits-1) - 1)\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = np.round(weights / scale).astype(np.int8)\n",
    "    \n",
    "    # Dequantize back\n",
    "    dequantized = quantized * scale\n",
    "    \n",
    "    return quantized, dequantized, scale\n",
    "\n",
    "def asymmetric_quantization(weights, n_bits=8):\n",
    "    \"\"\"Asymmetric quantization with zero-point\"\"\"\n",
    "    min_val = np.min(weights)\n",
    "    max_val = np.max(weights)\n",
    "    \n",
    "    # Calculate scale and zero-point\n",
    "    scale = (max_val - min_val) / (2**n_bits - 1)\n",
    "    zero_point = int(-min_val / scale)\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = np.round(weights / scale + zero_point).astype(np.uint8)\n",
    "    \n",
    "    # Dequantize\n",
    "    dequantized = (quantized - zero_point) * scale\n",
    "    \n",
    "    return quantized, dequantized, scale, zero_point\n",
    "\n",
    "# Apply both methods\n",
    "sym_quant, sym_dequant, sym_scale = symmetric_quantization(weights)\n",
    "asym_quant, asym_dequant, asym_scale, asym_zero = asymmetric_quantization(weights)\n",
    "\n",
    "# Calculate errors\n",
    "sym_error = np.mean(np.abs(weights - sym_dequant))\n",
    "asym_error = np.mean(np.abs(weights - asym_dequant))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Original distribution\n",
    "axes[0].hist(weights, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Original Weights Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Weight Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Symmetric quantization\n",
    "axes[1].scatter(weights[:500], sym_dequant[:500], alpha=0.5, s=10, color='green')\n",
    "axes[1].plot([-2, 2], [-2, 2], 'r--', linewidth=2, label='Perfect reconstruction')\n",
    "axes[1].set_title(f'Symmetric Quantization\\nError: {sym_error:.6f}', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Original Weight')\n",
    "axes[1].set_ylabel('Quantized Weight')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Asymmetric quantization\n",
    "axes[2].scatter(weights[:500], asym_dequant[:500], alpha=0.5, s=10, color='orange')\n",
    "axes[2].plot([-2, 2], [-2, 2], 'r--', linewidth=2, label='Perfect reconstruction')\n",
    "axes[2].set_title(f'Asymmetric Quantization\\nError: {asym_error:.6f}', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Original Weight')\n",
    "axes[2].set_ylabel('Quantized Weight')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Symmetric quantization error: {sym_error:.6f}\")\n",
    "print(f\"Asymmetric quantization error: {asym_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QLoRA: 4-bit Quantization for Fine-Tuning\n",
    "\n",
    "**QLoRA (Quantized Low-Rank Adaptation)** enables fine-tuning large language models on consumer GPUs.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **4-bit NormalFloat (NF4)**: A special data type optimized for neural network weights (which follow a normal distribution)\n",
    "2. **Frozen base model**: The original model stays in 4-bit and doesn't get updated\n",
    "3. **Trainable adapters**: Small LoRA layers (in FP16) are added and trained\n",
    "\n",
    "### Benefits:\n",
    "- Fine-tune 70B models on a single 48GB GPU\n",
    "- 4x memory reduction vs 16-bit fine-tuning\n",
    "- Maintains 99%+ of full precision performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NF4 quantization levels (16 values optimized for normal distribution)\n",
    "# These values are hardcoded based on quantiles of N(0,1) distribution\n",
    "NF4_VALUES = np.array([\n",
    "    -1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,\n",
    "    -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0,\n",
    "    0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224,\n",
    "    0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0\n",
    "])\n",
    "\n",
    "def nf4_quantization(weights, block_size=64):\n",
    "    \"\"\"\n",
    "    Implement NF4 quantization similar to QLoRA\n",
    "    \n",
    "    Steps:\n",
    "    1. Split weights into blocks\n",
    "    2. For each block: normalize to [-1, 1]\n",
    "    3. Map to closest NF4 value\n",
    "    4. Store block scale factors\n",
    "    \"\"\"\n",
    "    # Reshape into blocks\n",
    "    n_blocks = len(weights) // block_size\n",
    "    weights_blocked = weights[:n_blocks * block_size].reshape(n_blocks, block_size)\n",
    "    \n",
    "    quantized_blocks = []\n",
    "    scale_factors = []\n",
    "    \n",
    "    for block in weights_blocked:\n",
    "        # Calculate absmax for this block\n",
    "        absmax = np.max(np.abs(block))\n",
    "        scale_factors.append(absmax)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        normalized = block / (absmax + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        # Find closest NF4 value for each weight\n",
    "        quantized = np.zeros_like(normalized, dtype=np.int8)\n",
    "        for i, val in enumerate(normalized):\n",
    "            # Find index of closest NF4 value\n",
    "            idx = np.argmin(np.abs(NF4_VALUES - val))\n",
    "            quantized[i] = idx\n",
    "        \n",
    "        quantized_blocks.append(quantized)\n",
    "    \n",
    "    return np.array(quantized_blocks).flatten(), np.array(scale_factors)\n",
    "\n",
    "def nf4_dequantization(quantized, scale_factors, block_size=64):\n",
    "    \"\"\"Dequantize NF4 values back to original range\"\"\"\n",
    "    n_blocks = len(scale_factors)\n",
    "    quantized_blocked = quantized.reshape(n_blocks, block_size)\n",
    "    \n",
    "    dequantized = []\n",
    "    for block_quant, scale in zip(quantized_blocked, scale_factors):\n",
    "        # Map indices back to NF4 values\n",
    "        block_nf4 = NF4_VALUES[block_quant]\n",
    "        # Scale back to original range\n",
    "        block_original = block_nf4 * scale\n",
    "        dequantized.extend(block_original)\n",
    "    \n",
    "    return np.array(dequantized)\n",
    "\n",
    "# Use the weights from the previous cell (generated in cell-3)\n",
    "weights_sample = weights[:1024]  # Use 1024 weights for clean blocks\n",
    "nf4_quant, nf4_scales = nf4_quantization(weights_sample, block_size=64)\n",
    "nf4_dequant = nf4_dequantization(nf4_quant, nf4_scales, block_size=64)\n",
    "\n",
    "# Calculate error and compression\n",
    "nf4_error = np.mean(np.abs(weights_sample - nf4_dequant))\n",
    "original_size = weights_sample.nbytes  # FP32 = 4 bytes per weight\n",
    "nf4_size = len(nf4_quant) * 0.5 + len(nf4_scales) * 4  # 4 bits per weight + FP32 scales\n",
    "compression_ratio = original_size / nf4_size\n",
    "\n",
    "# Visualize NF4 quantization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# NF4 quantization levels\n",
    "axes[0, 0].stem(range(len(NF4_VALUES)), NF4_VALUES, basefmt=' ')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_title('NF4 Quantization Levels (16 values)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Index')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Weight distribution with NF4 levels\n",
    "axes[0, 1].hist(weights_sample / np.max(np.abs(weights_sample)), bins=50, alpha=0.6, color='blue', label='Normalized weights', density=True)\n",
    "for val in NF4_VALUES:\n",
    "    axes[0, 1].axvline(x=val, color='red', alpha=0.5, linewidth=1)\n",
    "axes[0, 1].set_title('Weight Distribution vs NF4 Levels', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Normalized Weight Value')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Original vs Dequantized\n",
    "axes[1, 0].scatter(weights_sample[:200], nf4_dequant[:200], alpha=0.5, s=15, color='purple')\n",
    "axes[1, 0].plot([-2, 2], [-2, 2], 'r--', linewidth=2, label='Perfect reconstruction')\n",
    "axes[1, 0].set_title(f'NF4 Quantization Quality\\nError: {nf4_error:.6f}', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Original Weight')\n",
    "axes[1, 0].set_ylabel('Dequantized Weight')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Compression comparison\n",
    "methods = ['FP32', 'INT8', 'NF4']\n",
    "sizes = [original_size, original_size / 4, nf4_size]\n",
    "colors_comp = ['#FF6B6B', '#4ECDC4', '#96CEB4']\n",
    "\n",
    "bars = axes[1, 1].bar(methods, sizes, color=colors_comp, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_title('Storage Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Size (bytes)')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                    f'{size:.0f}B\\n({original_size/size:.1f}x)', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNF4 Quantization Results:\")\n",
    "print(f\"  Original size: {original_size} bytes (FP32)\")\n",
    "print(f\"  NF4 size: {nf4_size:.0f} bytes\")\n",
    "print(f\"  Compression ratio: {compression_ratio:.2f}x\")\n",
    "print(f\"  Mean absolute error: {nf4_error:.6f}\")\n",
    "print(f\"\\nðŸ’¡ NF4 achieves ~{compression_ratio:.1f}x compression with minimal accuracy loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. When to Use Each Precision Format\n",
    "\n",
    "| Format | Best For | Memory Savings | Quality | Fine-tuning? |\n",
    "|--------|----------|----------------|---------|--------------|\n",
    "| **FP32** | Research, maximum accuracy | Baseline | â­â­â­â­â­ | âœ… Full precision |\n",
    "| **FP16/BF16** | Standard training | 2x | â­â­â­â­â­ | âœ… Mixed precision |\n",
    "| **INT8** | Inference only | 4x | â­â­â­â­ | âŒ Not for training |\n",
    "| **INT4/NF4** | Fine-tuning on limited GPUs | 8x | â­â­â­â­ | âœ… With QLoRA |\n",
    "\n",
    "### Quick Guide:\n",
    "\n",
    "âœ… **For inference only**: Use INT8 quantization  \n",
    "âœ… **For fine-tuning with limited GPU**: Use QLoRA with NF4  \n",
    "âœ… **For maximum accuracy**: Use FP32 or FP16  \n",
    "âœ… **For production deployment**: Use INT8 or INT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "1. **Quantization reduces memory** by using fewer bits (FP32 â†’ FP16 â†’ INT8 â†’ INT4)\n",
    "2. **Trade-off**: Lower precision = less memory but some accuracy loss\n",
    "3. **NF4 is special**: Optimized for neural network weights (normally distributed)\n",
    "4. **QLoRA enables fine-tuning** on consumer GPUs with 4-bit quantization\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "âœ… Use **4-bit NF4 with QLoRA** for fine-tuning large models on limited hardware  \n",
    "âœ… Use **block size of 64** for good compression vs accuracy balance  \n",
    "âœ… Monitor quantization error - keep it under 1% of weight magnitude  \n",
    "âœ… Test on your specific task - impact varies by use case\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **bitsandbytes**: Python library for 4-bit/8-bit quantization\n",
    "- **PEFT (Hugging Face)**: QLoRA implementation\n",
    "- **QLoRA paper**: https://arxiv.org/abs/2305.14314\n",
    "\n",
    "---\n",
    "\n",
    "**You can now fine-tune massive LLMs on accessible hardware!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
