{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization for Fine-Tuning LLMs: A Complete Guide\n",
    "\n",
    "## What is Quantization?\n",
    "\n",
    "**Quantization** is the process of reducing the precision of a model's parameters (weights and activations) by representing them with fewer bits. Instead of using 32-bit floating-point numbers (FP32), we can use 16-bit floats (FP16), 8-bit integers (INT8), or even 4-bit representations (INT4/NF4).\n",
    "\n",
    "## Why Does Quantization Matter for Fine-Tuning?\n",
    "\n",
    "When fine-tuning Large Language Models (LLMs), we face three main challenges:\n",
    "\n",
    "1. **Memory Constraints**: A 7B parameter model in FP32 requires ~28GB of VRAM just for the weights\n",
    "2. **Training Overhead**: Fine-tuning requires storing optimizer states (2x model size), gradients (1x model size), and activations\n",
    "3. **Hardware Limitations**: Most researchers don't have access to 80GB A100 GPUs\n",
    "\n",
    "**Quantization enables fine-tuning on consumer hardware** by reducing memory footprint by 75% (8-bit) or even 87.5% (4-bit) while maintaining model quality.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Precision Formats**: How FP32, FP16, INT8, and INT4 represent numbers\n",
    "2. **Memory Impact**: Quantifying the memory savings from quantization\n",
    "3. **Quantization Methods**: Symmetric vs asymmetric quantization\n",
    "4. **QLoRA & NF4**: State-of-the-art 4-bit quantization for fine-tuning\n",
    "5. **Practical Trade-offs**: When to use each precision format"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport struct\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style for better visualizations\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Example weight value from a neural network\nweight_value = 0.3457\n\n# Function to visualize bit representation\ndef float_to_bin(num, precision='float32'):\n    \"\"\"Convert float to binary representation\"\"\"\n    if precision == 'float32':\n        packed = struct.pack('!f', num)\n        bits = ''.join(f'{byte:08b}' for byte in packed)\n        return bits, 32\n    elif precision == 'float16':\n        packed = struct.pack('!e', num)\n        bits = ''.join(f'{byte:08b}' for byte in packed)\n        return bits, 16\n\n# Compare representations\nfp32_bits, fp32_size = float_to_bin(weight_value, 'float32')\nfp16_bits, fp16_size = float_to_bin(weight_value, 'float16')\n\nprint(f\"Original value: {weight_value}\")\nprint(f\"\\nFP32 (32-bit): {fp32_bits}\")\nprint(f\"  â””â”€ Sign: {fp32_bits[0]} | Exponent: {fp32_bits[1:9]} | Mantissa: {fp32_bits[9:]}\")\nprint(f\"  â””â”€ Memory: {fp32_size} bits = {fp32_size // 8} bytes\")\n\nprint(f\"\\nFP16 (16-bit): {fp16_bits}\")\nprint(f\"  â””â”€ Sign: {fp16_bits[0]} | Exponent: {fp16_bits[1:6]} | Mantissa: {fp16_bits[6:]}\")\nprint(f\"  â””â”€ Memory: {fp16_size} bits = {fp16_size // 8} bytes\")\n\n# Show precision comparison\nprint(f\"\\n{'Format':<10} {'Bits':<8} {'Range':<30} {'Precision':<15}\")\nprint(\"-\" * 70)\nprint(f\"{'FP32':<10} {'32':<8} {'Â±3.4e38':<30} {'~7 digits':<15}\")\nprint(f\"{'FP16':<10} {'16':<8} {'Â±6.5e4':<30} {'~3 digits':<15}\")\nprint(f\"{'INT8':<10} {'8':<8} {'-128 to 127':<30} {'Integer only':<15}\")\nprint(f\"{'INT4':<10} {'4':<8} {'-8 to 7':<30} {'Integer only':<15}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Calculate memory for a 7B parameter LLM\nmodel_size = 7_000_000_000  # 7 billion parameters\n\n# Memory in different formats (in GB)\nfp32_mem = (model_size * 4) / (1024**3)  # 4 bytes per parameter\nfp16_mem = (model_size * 2) / (1024**3)  # 2 bytes per parameter\nint8_mem = (model_size * 1) / (1024**3)  # 1 byte per parameter\nint4_mem = (model_size * 0.5) / (1024**3)  # 0.5 bytes per parameter\n\nprint(\"Memory Requirements for 7B Parameter Model:\")\nprint(f\"  FP32:  {fp32_mem:.2f} GB\")\nprint(f\"  FP16:  {fp16_mem:.2f} GB ({fp32_mem/fp16_mem:.1f}x smaller)\")\nprint(f\"  INT8:  {int8_mem:.2f} GB ({fp32_mem/int8_mem:.1f}x smaller)\")\nprint(f\"  INT4:  {int4_mem:.2f} GB ({fp32_mem/int4_mem:.1f}x smaller)\")\n\n# Visualize\nformats = ['FP32', 'FP16', 'INT8', 'INT4']\nmemory = [fp32_mem, fp16_mem, int8_mem, int4_mem]\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(formats, memory, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\nplt.ylabel('Memory (GB)', fontsize=12)\nplt.title('Memory Requirements for 7B LLM in Different Formats', fontsize=14, fontweight='bold')\nplt.grid(axis='y', alpha=0.3)\n\nfor bar, mem in zip(bars, memory):\n    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n            f'{mem:.1f} GB', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Generate sample weights from a normal distribution (like real neural network weights)\nnp.random.seed(42)\nweights = np.random.randn(10000) * 0.5  # Mean=0, std=0.5\n\ndef symmetric_quantization(weights, n_bits=8):\n    \"\"\"Symmetric quantization using absolute maximum\"\"\"\n    # Calculate scale based on maximum absolute value\n    max_val = np.max(np.abs(weights))\n    scale = max_val / (2**(n_bits-1) - 1)\n    \n    # Quantize\n    quantized = np.round(weights / scale).astype(np.int8)\n    \n    # Dequantize back\n    dequantized = quantized * scale\n    \n    return quantized, dequantized, scale\n\ndef asymmetric_quantization(weights, n_bits=8):\n    \"\"\"Asymmetric quantization with zero-point\"\"\"\n    min_val = np.min(weights)\n    max_val = np.max(weights)\n    \n    # Calculate scale and zero-point\n    scale = (max_val - min_val) / (2**n_bits - 1)\n    zero_point = int(-min_val / scale)\n    \n    # Quantize\n    quantized = np.round(weights / scale + zero_point).astype(np.uint8)\n    \n    # Dequantize\n    dequantized = (quantized - zero_point) * scale\n    \n    return quantized, dequantized, scale, zero_point\n\n# Apply both methods\nsym_quant, sym_dequant, sym_scale = symmetric_quantization(weights)\nasym_quant, asym_dequant, asym_scale, asym_zero = asymmetric_quantization(weights)\n\n# Calculate errors\nsym_error = np.mean(np.abs(weights - sym_dequant))\nasym_error = np.mean(np.abs(weights - asym_dequant))\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Original distribution\naxes[0].hist(weights, bins=50, alpha=0.7, color='blue', edgecolor='black')\naxes[0].set_title('Original Weights Distribution', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Weight Value')\naxes[0].set_ylabel('Frequency')\naxes[0].grid(alpha=0.3)\n\n# Symmetric quantization\naxes[1].scatter(weights[:500], sym_dequant[:500], alpha=0.5, s=10, color='green')\naxes[1].plot([-2, 2], [-2, 2], 'r--', linewidth=2, label='Perfect reconstruction')\naxes[1].set_title(f'Symmetric Quantization\\nError: {sym_error:.6f}', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Original Weight')\naxes[1].set_ylabel('Quantized Weight')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\n# Asymmetric quantization\naxes[2].scatter(weights[:500], asym_dequant[:500], alpha=0.5, s=10, color='orange')\naxes[2].plot([-2, 2], [-2, 2], 'r--', linewidth=2, label='Perfect reconstruction')\naxes[2].set_title(f'Asymmetric Quantization\\nError: {asym_error:.6f}', fontsize=14, fontweight='bold')\naxes[2].set_xlabel('Original Weight')\naxes[2].set_ylabel('Quantized Weight')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Symmetric quantization error: {sym_error:.6f}\")\nprint(f\"Asymmetric quantization error: {asym_error:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. QLoRA: 4-bit Quantization for Fine-Tuning\n\n**QLoRA (Quantized Low-Rank Adaptation)** enables fine-tuning large language models on consumer GPUs.\n\n### Key Concepts:\n\n1. **4-bit NormalFloat (NF4)**: A special data type optimized for neural network weights (which follow a normal distribution)\n2. **Frozen base model**: The original model stays in 4-bit and doesn't get updated\n3. **Trainable adapters**: Small LoRA layers (in FP16) are added and trained\n\n### Benefits:\n- Fine-tune 70B models on a single 48GB GPU\n- 4x memory reduction vs 16-bit fine-tuning\n- Maintains 99%+ of full precision performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# NF4 quantization levels (16 values optimized for normal distribution)\n# These values are hardcoded based on quantiles of N(0,1) distribution\nNF4_VALUES = np.array([\n    -1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,\n    -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0,\n    0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224,\n    0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0\n])\n\ndef nf4_quantization(weights, block_size=64):\n    \"\"\"\n    Implement NF4 quantization similar to QLoRA\n    \n    Steps:\n    1. Split weights into blocks\n    2. For each block: normalize to [-1, 1]\n    3. Map to closest NF4 value\n    4. Store block scale factors\n    \"\"\"\n    # Reshape into blocks\n    n_blocks = len(weights) // block_size\n    weights_blocked = weights[:n_blocks * block_size].reshape(n_blocks, block_size)\n    \n    quantized_blocks = []\n    scale_factors = []\n    \n    for block in weights_blocked:\n        # Calculate absmax for this block\n        absmax = np.max(np.abs(block))\n        scale_factors.append(absmax)\n        \n        # Normalize to [-1, 1]\n        normalized = block / (absmax + 1e-8)  # Add small epsilon to avoid division by zero\n        \n        # Find closest NF4 value for each weight\n        quantized = np.zeros_like(normalized, dtype=np.int8)\n        for i, val in enumerate(normalized):\n            # Find index of closest NF4 value\n            idx = np.argmin(np.abs(NF4_VALUES - val))\n            quantized[i] = idx\n        \n        quantized_blocks.append(quantized)\n    \n    return np.array(quantized_blocks).flatten(), np.array(scale_factors)\n\ndef nf4_dequantization(quantized, scale_factors, block_size=64):\n    \"\"\"Dequantize NF4 values back to original range\"\"\"\n    n_blocks = len(scale_factors)\n    quantized_blocked = quantized.reshape(n_blocks, block_size)\n    \n    dequantized = []\n    for block_quant, scale in zip(quantized_blocked, scale_factors):\n        # Map indices back to NF4 values\n        block_nf4 = NF4_VALUES[block_quant]\n        # Scale back to original range\n        block_original = block_nf4 * scale\n        dequantized.extend(block_original)\n    \n    return np.array(dequantized)\n\n# Use the weights from the previous cell (generated in cell-3)\nweights_sample = weights[:1024]  # Use 1024 weights for clean blocks\nnf4_quant, nf4_scales = nf4_quantization(weights_sample, block_size=64)\nnf4_dequant = nf4_dequantization(nf4_quant, nf4_scales, block_size=64)\n\n# Calculate error and compression\nnf4_error = np.mean(np.abs(weights_sample - nf4_dequant))\noriginal_size = weights_sample.nbytes  # FP32 = 4 bytes per weight\nnf4_size = len(nf4_quant) * 0.5 + len(nf4_scales) * 4  # 4 bits per weight + FP32 scales\ncompression_ratio = original_size / nf4_size\n\n# Visualize NF4 quantization\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# NF4 quantization levels\naxes[0, 0].stem(range(len(NF4_VALUES)), NF4_VALUES, basefmt=' ')\naxes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[0, 0].set_title('NF4 Quantization Levels (16 values)', fontsize=14, fontweight='bold')\naxes[0, 0].set_xlabel('Index')\naxes[0, 0].set_ylabel('Value')\naxes[0, 0].grid(alpha=0.3)\n\n# Weight distribution with NF4 levels\naxes[0, 1].hist(weights_sample / np.max(np.abs(weights_sample)), bins=50, alpha=0.6, color='blue', label='Normalized weights', density=True)\nfor val in NF4_VALUES:\n    axes[0, 1].axvline(x=val, color='red', alpha=0.5, linewidth=1)\naxes[0, 1].set_title('Weight Distribution vs NF4 Levels', fontsize=14, fontweight='bold')\naxes[0, 1].set_xlabel('Normalized Weight Value')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend()\n\n# Original vs Dequantized\naxes[1, 0].scatter(weights_sample[:200], nf4_dequant[:200], alpha=0.5, s=15, color='purple')\naxes[1, 0].plot([-2, 2], [-2, 2], 'r--', linewidth=2, label='Perfect reconstruction')\naxes[1, 0].set_title(f'NF4 Quantization Quality\\nError: {nf4_error:.6f}', fontsize=14, fontweight='bold')\naxes[1, 0].set_xlabel('Original Weight')\naxes[1, 0].set_ylabel('Dequantized Weight')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\n\n# Compression comparison\nmethods = ['FP32', 'INT8', 'NF4']\nsizes = [original_size, original_size / 4, nf4_size]\ncolors_comp = ['#FF6B6B', '#4ECDC4', '#96CEB4']\n\nbars = axes[1, 1].bar(methods, sizes, color=colors_comp, edgecolor='black', linewidth=2)\naxes[1, 1].set_title('Storage Size Comparison', fontsize=14, fontweight='bold')\naxes[1, 1].set_ylabel('Size (bytes)')\naxes[1, 1].grid(axis='y', alpha=0.3)\n\nfor bar, size in zip(bars, sizes):\n    axes[1, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n                    f'{size:.0f}B\\n({original_size/size:.1f}x)', \n                    ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nNF4 Quantization Results:\")\nprint(f\"  Original size: {original_size} bytes (FP32)\")\nprint(f\"  NF4 size: {nf4_size:.0f} bytes\")\nprint(f\"  Compression ratio: {compression_ratio:.2f}x\")\nprint(f\"  Mean absolute error: {nf4_error:.6f}\")\nprint(f\"\\nðŸ’¡ NF4 achieves ~{compression_ratio:.1f}x compression with minimal accuracy loss!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. When to Use Each Precision Format\n\n| Format | Best For | Memory Savings | Quality | Fine-tuning? |\n|--------|----------|----------------|---------|--------------|\n| **FP32** | Research, maximum accuracy | Baseline | â­â­â­â­â­ | âœ… Full precision |\n| **FP16/BF16** | Standard training | 2x | â­â­â­â­â­ | âœ… Mixed precision |\n| **INT8** | Inference only | 4x | â­â­â­â­ | âŒ Not for training |\n| **INT4/NF4** | Fine-tuning on limited GPUs | 8x | â­â­â­â­ | âœ… With QLoRA |\n\n### Quick Guide:\n\nâœ… **For inference only**: Use INT8 quantization  \nâœ… **For fine-tuning with limited GPU**: Use QLoRA with NF4  \nâœ… **For maximum accuracy**: Use FP32 or FP16  \nâœ… **For production deployment**: Use INT8 or INT4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Key Takeaways\n\n### What You Learned:\n\n1. **Quantization reduces memory** by using fewer bits (FP32 â†’ FP16 â†’ INT8 â†’ INT4)\n2. **Trade-off**: Lower precision = less memory but some accuracy loss\n3. **NF4 is special**: Optimized for neural network weights (normally distributed)\n4. **QLoRA enables fine-tuning** on consumer GPUs with 4-bit quantization\n\n### Best Practices:\n\nâœ… Use **4-bit NF4 with QLoRA** for fine-tuning large models on limited hardware  \nâœ… Use **block size of 64** for good compression vs accuracy balance  \nâœ… Monitor quantization error - keep it under 1% of weight magnitude  \nâœ… Test on your specific task - impact varies by use case\n\n### Resources:\n\n- **bitsandbytes**: Python library for 4-bit/8-bit quantization\n- **PEFT (Hugging Face)**: QLoRA implementation\n- **QLoRA paper**: https://arxiv.org/abs/2305.14314\n\n---\n\n**You can now fine-tune massive LLMs on accessible hardware!** ðŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}