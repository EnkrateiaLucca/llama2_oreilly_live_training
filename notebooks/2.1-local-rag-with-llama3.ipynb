{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG with PDF and Llama 3.1\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Load the pdf\n",
    "2. Chunk that pdf (split that into pieces)\n",
    "3. Embed each piece\n",
    "4. Create the vector database, index\n",
    "5. Query (retrieving from that vector database using a llama3 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000d5295aba84c5c866b09130c4371f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Optional quantization to 4bit\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_TOKEN\")\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    model_kwargs={\n",
    "        \"token\": hf_token,\n",
    "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "        # \"quantization_config\": quantization_config\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    tokenizer_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    tokenizer_kwargs={\"token\": hf_token},\n",
    "    stopping_ids=stopping_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# bge embedding model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Llama-3-8B-Instruct model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./pdfs/lora-paper.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='2f1a7fa4-0f74-42dc-b57d-4b2b0d1ef50f', embedding=None, metadata={'page_label': '1', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='LORA: L OW-RANK ADAPTATION OF LARGE LAN-\\nGUAGE MODELS\\nEdward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\nMicrosoft Corporation\\n{edwardhu, yeshe, phwallis, zeyuana,\\nyuanzhil, swang, luw, wzchen }@microsoft.com\\nyuanzhil@andrew.cmu.edu\\n(Version 2)\\nABSTRACT\\nAn important paradigm of natural language processing consists of large-scale pre-\\ntraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\\nbecomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\\ndent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\\nexpensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\\ntrained model weights and injects trainable rank decomposition matrices into each\\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\\nrameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\\nLoRA can reduce the number of trainable parameters by 10,000 times and the\\nGPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\\nno additional inference latency . We also provide an empirical investigation into\\nrank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\\nmodels and provide our implementations and model checkpoints for RoBERTa,\\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\\n1 I NTRODUCTION\\nPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxh\\nùêµ=0\\nùê¥=ùí©(0,ùúé2)\\nùëëùëüPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxf(x)\\nùëë\\nFigure 1: Our reparametriza-\\ntion. We only train AandB.Many applications in natural language processing rely on adapt-\\ningonelarge-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\\nwhich updates all the parameters of the pre-trained model. The ma-\\njor downside of Ô¨Åne-tuning is that the new model contains as many\\nparameters as in the original model. As larger models are trained\\nevery few months, this changes from a mere ‚Äúinconvenience‚Äù for\\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\\n175 billion trainable parameters.1\\nMany sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciÔ¨Åc parameters in ad-\\ndition to the pre-trained model for each task, greatly boosting the\\noperational efÔ¨Åciency when deployed. However, existing techniques\\n‚àóEqual contribution.\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\\nmance signiÔ¨Åcantly as shown in Appendix A.\\n1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6dc77e49-102b-428b-9fda-658d1b862ba5', embedding=None, metadata={'page_label': '2', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='often introduce inference latency (Houlsby et al., 2019; RebufÔ¨Å et al., 2017) by extending model\\ndepth or reduce the model‚Äôs usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\\nmatch the Ô¨Åne-tuning baselines, posing a trade-off between efÔ¨Åciency and model quality.\\nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low ‚Äúintrinsic rank‚Äù, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers‚Äô change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\\n175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufÔ¨Åces even\\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efÔ¨Åcient.\\nLoRA possesses several key advantages.\\n‚Ä¢ A pre-trained model can be shared and used to build many small LoRA modules for dif-\\nferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\nhead signiÔ¨Åcantly.\\n‚Ä¢ LoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\nmuch smaller low-rank matrices.\\n‚Ä¢ Our simple linear design allows us to merge the trainable matrices with the frozen weights\\nwhen deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\\nconstruction.\\n‚Ä¢ LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preÔ¨Åx-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\ntrained weight matrix and ‚àÜWits accumulated gradient update during adaptation. We use rto\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\noptimization and use a Transformer MLP feedforward dimension dffn= 4√ódmodel .\\n2 P ROBLEM STATEMENT\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-\\ning use case. Below is a brief description of the language modeling problem and, in particular, the\\nmaximization of conditional probabilities given a task-speciÔ¨Åc prompt.\\nSuppose we are given a pre-trained autoregressive language model PŒ¶(y|x)parametrized by Œ¶.\\nFor instance, PŒ¶(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\\nrepresented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\\nyiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\\ncorresponding SQL command; for summarization, xiis the content of an article and yiits summary.\\n2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6a8f077-a8b3-4b0b-874e-1b5e207c12cc', embedding=None, metadata={'page_label': '3', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='During full Ô¨Åne-tuning, the model is initialized to pre-trained weights Œ¶0and updated to Œ¶0+ ‚àÜŒ¶\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nŒ¶‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(PŒ¶(yt|x,y<t)) (1)\\nOne of the main drawbacks for full Ô¨Åne-tuning is that for each downstream task, we learn a different\\nset of parameters ‚àÜŒ¶whose dimension|‚àÜŒ¶|equals|Œ¶0|. Thus, if the pre-trained model is large\\n(such as GPT-3 with |Œ¶0|‚âà175Billion), storing and deploying many independent instances of\\nÔ¨Åne-tuned models can be challenging, if at all feasible.\\nIn this paper, we adopt a more parameter-efÔ¨Åcient approach, where the task-speciÔ¨Åc parameter\\nincrement ‚àÜŒ¶ = ‚àÜŒ¶(Œò) is further encoded by a much smaller-sized set of parameters Œòwith\\n|Œò|‚â™| Œ¶0|. The task of Ô¨Ånding ‚àÜŒ¶thus becomes optimizing over Œò:\\nmax\\nŒò‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(\\npŒ¶0+‚àÜŒ¶(Œò) (yt|x,y<t))\\n(2)\\nIn the subsequent sections, we propose to use a low-rank representation to encode ‚àÜŒ¶that is both\\ncompute- and memory-efÔ¨Åcient. When the pre-trained model is GPT-3 175B, the number of train-\\nable parameters|Œò|can be as small as 0.01% of|Œ¶0|.\\n3 A REN‚ÄôTEXISTING SOLUTIONS GOOD ENOUGH ?\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\\nof works have sought to make model adaptation more parameter- and compute-efÔ¨Åcient. See Sec-\\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\\nare two prominent strategies when it comes to efÔ¨Åcient adaptations: adding adapter layers (Houlsby\\net al., 2019; RebufÔ¨Å et al., 2017; Pfeiffer et al., 2021; R ¬®uckl¬¥e et al., 2020) or optimizing some forms\\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\\nlatency-sensitive production scenario.\\nAdapter Layers Introduce Inference Latency There are many variants of adapters. We focus\\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\\ning multi-task settings (R ¬®uckl¬¥e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\\na difference in the online inference setting where the batch size is typically as small as one. In a\\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\\nvery small bottleneck dimension (Table 1).\\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\\nAllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\\nDirectly Optimizing the Prompt is Hard The other direction, as exempliÔ¨Åed by preÔ¨Åx tuning (Li\\n& Liang, 2021), faces a different challenge. We observe that preÔ¨Åx tuning is difÔ¨Åcult to optimize\\nand that its performance changes non-monotonically in trainable parameters, conÔ¨Årming similar\\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\\nadaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\\non task performance to Section 5.\\n3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is Lora?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " LoRA is a method that adapts large pre-trained language models to a specific task without fine-tuning the entire model. It does this by learning a set of additional parameters that are applied to the pre-trained model's weights, rather than updating the weights themselves. This allows LoRA to scale up to large models like GPT-3 while still achieving good performance on a variety of tasks. LoRA is particularly effective in low-data regimes and can be combined with other adaptation methods, such as prefix-embedding and prefix-layer tuning, to further improve performance. LoRA uses a measure of subspace similarity between the pre-trained model's weights and the task-specific weights to determine which weights to adapt. This allows LoRA to efficiently adapt the model to the task without overfitting or underfitting. LoRA has been shown to perform better than or at least on-par with other adaptation methods, including fine-tuning and prefix-based approaches, on a variety of tasks and datasets.  LoRA is a simple yet effective method for adapting large pre-trained language models to specific tasks, and it has the potential to be used in a wide range of applications.  LoRA has been shown to be particularly effective in low-data regimes and can be used to adapt large models like G"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='b43bd3df-a5a9-463d-822d-5cad4bbc2c9a', embedding=None, metadata={'page_label': '8', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c353776d-4b81-47f7-89db-c6e4ad9c3a7d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, hash='f8543297500daf61f96fec54cdb246a5c6172461d8c2e5c2fc3ad4957e08165a')}, text='Model&Method# Trainable WikiSQL MNLI-m SAMSum\\nParameters Acc. (%) Acc. (%) R1/R2/RL\\nGPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5\\nGPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5\\nGPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5\\nGPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5\\nGPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8\\nGPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1\\nGPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9\\nGPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\\nSAMSum. LoRA performs better than prior approaches, including full Ô¨Åne-tuning. The results\\non WikiSQL have a Ô¨Çuctuation around ¬±0.5%, MNLI-m around ¬±0.1%, and SAMSum around\\n¬±0.2/¬±0.2/¬±0.1for the three metrics.\\n5.5 S CALING UP TO GPT-3 175B\\nAs a Ô¨Ånal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\nAs shown in Table 4, LoRA matches or exceeds the Ô¨Åne-tuning baseline on all three datasets. Note\\nthat not all methods beneÔ¨Åt monotonically from having more trainable parameters, as shown in Fig-\\nure 2. We observe a signiÔ¨Åcant performance drop when we use more than 256 special tokens for\\npreÔ¨Åx-embedding tuning or more than 32 special tokens for preÔ¨Åx-layer tuning. This corroborates\\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\\nperformance of different adaptation approaches in the low-data regime in Section F.3.\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.550.600.650.700.75Validation Accuracy\\nWikiSQL\\nMethod\\nFine-Tune\\nPrefixEmbed\\nPrefixLayer\\nAdapter(H)\\nLoRA\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.840.860.880.900.92\\nMultiNLI-matched\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\\nSee Section F.2 for more details on the plotted data points.\\n6 R ELATED WORKS\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence\\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) ‚Äì both are large Transformer lan-\\n8', mimetype='text/plain', start_char_idx=0, end_char_idx=2924, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6277767358958708),\n",
       " NodeWithScore(node=TextNode(id_='cdb02423-30a5-4131-9477-e643fe3de278', embedding=None, metadata={'page_label': '21', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8969f3f7-c3be-426c-8bed-6e9e19a108a3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '21', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, hash='1c4dcf0284bbd1abc494935c75ade78c0bf40177edb3f0b7026cbc6066dc4ca5')}, text='Hyperparameters Fine-Tune PreEmbed PreLayer BitFit AdapterHLoRA\\nOptimizer AdamW\\nBatch Size 128\\n# Epoch 2\\nWarmup Tokens 250,000\\nLR Schedule Linear\\nLearning Rate 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04\\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\\nsame hyperparameters for all datasets after tuning learning rate.\\nrally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\\nembeddings and subsequent Transformer block activations are treated as trainable parameters. For\\nmore on preÔ¨Åx-layer tuning, see Section 5.1.\\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\\nFirst of all, LoRA+PE signiÔ¨Åcantly outperforms both LoRA and preÔ¨Åx-embedding tuning on\\nWikiSQL, which indicates that LoRA is somewhat orthogonal to preÔ¨Åx-embedding tuning. On\\nMultiNLI, the combination of LoRA+PE doesn‚Äôt perform better than LoRA, possibly because LoRA\\non its own already achieves performance comparable to the human baseline. Secondly, we notice\\nthat LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\\ntribute this to the fact that preÔ¨Åx-layer tuning is very sensitive to the choice of learning rate and thus\\nmakes the optimization of LoRA weights more difÔ¨Åcult in LoRA+PL.\\nF A DDITIONAL EMPIRICAL EXPERIMENTS\\nF.1 A DDITIONAL EXPERIMENTS ON GPT-2\\nWe also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017)\\nfollowing the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result\\non E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with\\npreÔ¨Åx-based approaches given the same number of trainable parameters.\\nMethod # Trainable DART\\nParameters BLEU‚ÜëMET‚ÜëTER‚Üì\\nGPT-2 Medium\\nFine-Tune 354M 46.2 0.39 0.46\\nAdapterL0.37M 42.4 0.36 0.48\\nAdapterL11M 45.2 0.38 0.46\\nFTTop224M 41.0 0.34 0.56\\nPrefLayer 0.35M 46.4 0.38 0.46\\nLoRA 0.35M 47.1¬±.2 0.39 0.46\\nGPT-2 Large\\nFine-Tune 774M 47.0 0.39 0.46\\nAdapterL0.88M 45.7¬±.1 0.38 0.46\\nAdapterL23M 47.1¬±.1 0.39 0.45\\nPrefLayer 0.77M 46.7 0.38 0.45\\nLoRA 0.77M 47.5¬±.1 0.39 0.45\\nTable 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are\\nless than 0.01for all adaption approaches.\\n21', mimetype='text/plain', start_char_idx=0, end_char_idx=2290, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.627540909989303),\n",
       " NodeWithScore(node=TextNode(id_='ad1bbe20-e071-498a-8ccd-e41ac56dda8c', embedding=None, metadata={'page_label': '22', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='dd62130d-274f-47bd-8ada-1c37784e6e47', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '22', 'file_name': 'lora-paper.pdf', 'file_path': 'pdfs/lora-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-12-02', 'last_modified_date': '2023-01-23'}, hash='0bd92be911170d2783ba15819d452169c3333c570c8ba9b95313b1074606a5bc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c5c89367-096b-4bd1-97d8-d508d3e307f5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b72446495843558db6dd426421e45037e04521199be9f3f8aed8a4f7d2dc3ee0')}, text='Method WebNLG\\nBLEU‚Üë MET‚Üë TER‚Üì\\nU S A U S A U S A\\nGPT-2 Medium\\nFine-Tune (354M) 27.7 64.2 46.5 .30 .45 .38 .76 .33 .53\\nAdapterL(0.37M) 45.1 54.5 50.2 .36 .39 .38 .46 .40 .43\\nAdapterL(11M) 48.3 60.4 54.9 .38 .43 .41 .45 .35 .39\\nFTTop2(24M) 18.9 53.6 36.0 .23 .38 .31 .99 .49 .72\\nPreÔ¨Åx (0.35M) 45.6 62.9 55.1 .38 .44 .41 .49 .35 .40\\nLoRA (0.35M) 46.7¬±.462.1¬±.255.3¬±.2.38 .44 .41 .46 .33 .39\\nGPT-2 Large\\nFine-Tune (774M) 43.1 65.3 55.5 .38 .46 .42 .53 .33 .42\\nAdapterL(0.88M) 49.8¬±.061.1¬±.056.0¬±.0.38 .43 .41 .44 .35 .39\\nAdapterL(23M) 49.2¬±.164.7¬±.257.7¬±.1.39 .46 .43 .46 .33 .39\\nPreÔ¨Åx (0.77M) 47.7 63.4 56.3 .39 .45 .42 .48 .34 .40\\nLoRA (0.77M) 48.4¬±.364.0¬±.357.0¬±.1.39 .45 .42 .45 .32 .38\\nTable 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER\\nare less than 0.01for all the experiments we ran. ‚ÄúU‚Äù indicates unseen categories, ‚ÄúS‚Äù indicates seen\\ncategories, and ‚ÄúA‚Äù indicates all categories in the test set of WebNLG.\\nF.2 A DDITIONAL EXPERIMENTS ON GPT-3\\nWe present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\\nidentifying the trade-off between performance and the number of trainable parameters.\\nF.3 L OW-DATA REGIME\\nTo evaluate the performance of different adaptation approaches in the low-data regime. we randomly\\nsample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\\nMNLI-ntasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\\nn. To our surprise, PreÔ¨ÅxEmbed and PreÔ¨ÅxLayer performs very poorly on MNLI-100 dataset, with\\nPreÔ¨ÅxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreÔ¨ÅxLayer\\nperforms better than PreÔ¨ÅxEmbed but is still signiÔ¨Åcantly worse than Fine-Tune or LoRA on MNLI-\\n100. The gap between preÔ¨Åx-based approaches and LoRA/Fine-tuning becomes smaller as we in-\\ncrease the number of training examples, which might suggest that preÔ¨Åx-based approaches are not\\nsuitable for low-data tasks in GPT-3. LoRA achieves better performance than Ô¨Åne-tuning on both\\nMNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\\n(¬±0.3) variance due to random seeds.\\nThe training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\\nble 17. We use a smaller learning rate for PreÔ¨ÅxLayer on the MNLI-100 set, as the training loss does\\nnot decrease with a larger learning rate.\\nG M EASURING SIMILARITY BETWEEN SUBSPACES\\nIn this paper we use the measure œÜ(A,B,i,j ) =œà(Ui\\nA,Uj\\nB) =‚à•Ui‚ä§\\nAUB‚à•2\\nF\\nmin{i,j}to measure the subspace\\nsimilarity between two column orthonormal matrices Ui\\nA‚ààRd√óiandUj\\nB‚ààRd√ój, obtained by\\ntaking columns of the left singular matrices of AandB.', mimetype='text/plain', start_char_idx=0, end_char_idx=2694, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6209434098084122)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "oreilly-llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
