{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading, retrieval methods and text splitting\n",
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set OPENAI API Key\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "links = [\"https://python.langchain.com/docs/how_to/structured_output/\",\n",
    "         \"https://python.langchain.com/docs/how_to/tool_calling/\",\n",
    "         \"https://python.langchain.com/docs/how_to/few_shot_examples/\",\n",
    "         \"https://python.langchain.com/docs/how_to/prompts_composition/\",\n",
    "         \"https://python.langchain.com/docs/how_to/functions/\",\n",
    "         \"https://python.langchain.com/docs/how_to/parallel/\",\n",
    "         \"https://python.langchain.com/docs/how_to/sequence/\",\n",
    "         \"https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel\",\n",
    "         \"https://python.langchain.com/docs/how_to/installation/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_markdown/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_json/\",      \n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_pdf/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_web/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_csv/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_directory/\", \n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_html/\",\n",
    "         \"https://python.langchain.com/docs/tutorials/rag/\"\n",
    "         ]\n",
    "\n",
    "loader = WebBaseLoader(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to return structured data from a model | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nFunction/tool calling'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='It is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\\nThe .with_structured_output() method\\u200b\\n\\nSupported modelsYou can find a list of models that support this method here.\\nThis is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\\nAs an example, let\\'s get a model to generate a joke and separate the setup from the punchline:\\n\\nSelect chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\nPydantic class\\u200b\\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\\nfrom typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\nJoke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7)\\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model\\'s function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\\nTypedDict or JSON Schema\\u200b\\nIf you don\\'t want to use Pydantic, explicitly don\\'t want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn\\'t generate it, it is only used in defining the schema that is passed to the model.\\nRequirements\\nCore: langchain-core>=0.2.26\\nTyping extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='from typing import Optionalfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nEquivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\\njson_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nChoosing between multiple schemas\\u200b\\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\\nUsing Pydantic\\u200b\\nfrom typing import Unionclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user\\'s query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\nFinalResponse(final_output=Joke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7))\\nstructured_llm.invoke(\"How are you today?\")\\nFinalResponse(final_output=ConversationalResponse(response=\"I\\'m just a computer program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need!\"))\\nUsing TypedDict\\u200b\\nfrom typing import Optional, Unionfrom typing_extensions import Annotated, TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]class ConversationalResponse(TypedDict):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: Annotated[str, ..., \"A conversational response to the user\\'s query\"]class FinalResponse(TypedDict):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'final_output\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}}\\nstructured_llm.invoke(\"How are you today?\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='{\\'final_output\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}}\\nstructured_llm.invoke(\"How are you today?\")\\n{\\'final_output\\': {\\'response\\': \"I\\'m just a computer program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need!\"}}\\nResponses shall be identical to the ones shown in the Pydantic example.\\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don\\'t have to use nested schemas. See this how-to guide for more details.\\nStreaming\\u200b\\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\\ninfoNote that what\\'s yielded is already aggregated chunks, not deltas.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\\n{}{\\'setup\\': \\'\\'}{\\'setup\\': \\'Why\\'}{\\'setup\\': \\'Why was\\'}{\\'setup\\': \\'Why was the\\'}{\\'setup\\': \\'Why was the cat\\'}{\\'setup\\': \\'Why was the cat sitting\\'}{\\'setup\\': \\'Why was the cat sitting on\\'}{\\'setup\\': \\'Why was the cat sitting on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nFew-shot prompting\\u200b\\nFor more complex schemas it\\'s very useful to add few-shot examples to the prompt. This can be done in a few ways.\\nThe simplest and most universal way is to add examples to a system message in the prompt:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='The simplest and most universal way is to add examples to a system message in the prompt:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don\\'t planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what\\'s something funny about woodpeckers\")API Reference:ChatPromptTemplate\\n{\\'setup\\': \\'Woodpecker\\', \\'punchline\\': \"Woodpecker you a joke, but I\\'m afraid it might be too \\'hole-some\\'!\", \\'rating\\': 7}\\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you\\'re using makes use of tool calling in its API reference.\\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don\\'t planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") \\\\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\\n{\\'setup\\': \\'Crocodile\\', \\'punchline\\': \\'Crocodile be seeing you later, alligator!\\', \\'rating\\': 6}\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='For more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\\nJSON modeIf using JSON mode you\\'ll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you\\'re using supports JSON mode, check its entry in the API reference.\\nstructured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}\\n(Advanced) Raw outputs\\u200b\\nLLMs aren\\'t perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'raw\\': AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'function\\': {\\'arguments\\': \\'{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}\\', \\'name\\': \\'Joke\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 33, \\'prompt_tokens\\': 93, \\'total_tokens\\': 126}, \\'model_name\\': \\'gpt-4o-2024-05-13\\', \\'system_fingerprint\\': \\'fp_4e2b2da518\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0\\', tool_calls=[{\\'name\\': \\'Joke\\', \\'args\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}, \\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 93, \\'output_tokens\\': 33, \\'total_tokens\\': 126}), \\'parsed\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}, \\'parsing_error\\': None}\\nPrompting and parsing model outputs directly\\u200b\\nNot all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you\\'ll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\\nUsing PydanticOutputParser\\u200b\\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\\nfrom typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Let‚Äôs take a look at what information is sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke({\"query\": query}).to_string())\\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\\\`\\\\`\\\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\\\`\\\\`\\\\`Human: Anna is 23 years old and she is 6 feet tall\\nAnd now let\\'s invoke it:\\nchain = prompt | llm | parserchain.invoke({\"query\": query})\\nPeople(people=[Person(name=\\'Anna\\', height_in_meters=1.8288)])\\nFor a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\\nCustom Parsing\\u200b\\nYou can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\\nimport jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\\\`\\\\`\\\\`json\\\\n{schema}\\\\n\\\\`\\\\`\\\\`. \"            \"Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\\\`\\\\`\\\\`json(.*?)\\\\`\\\\`\\\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Here is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\\\`\\\\`\\\\`json{\\'title\\': \\'People\\', \\'description\\': \\'Identifying information about all people in a text.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'people\\': {\\'title\\': \\'People\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/definitions/Person\\'}}}, \\'required\\': [\\'people\\'], \\'definitions\\': {\\'Person\\': {\\'title\\': \\'Person\\', \\'description\\': \\'Information about a person.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'description\\': \\'The name of the person\\', \\'type\\': \\'string\\'}, \\'height_in_meters\\': {\\'title\\': \\'Height In Meters\\', \\'description\\': \\'The height of the person expressed in meters.\\', \\'type\\': \\'number\\'}}, \\'required\\': [\\'name\\', \\'height_in_meters\\']}}}\\\\`\\\\`\\\\`. Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\\nAnd here\\'s what it looks like when we invoke it:\\nchain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\\n[{\\'people\\': [{\\'name\\': \\'Anna\\', \\'height_in_meters\\': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to use chat models to call tools | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use chat models to call toolsOn this pageHow to use chat models to call tools'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nTool calling\\nTools\\nOutput parsers'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Tool calling allows a chat model to respond to a given prompt by \"calling a tool\".\\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don\\'t intend to invoke any tools. An example use-case of that is extraction from unstructured text.\\n\\nIf you want to see how to use the model-generated tool call to actually run a tool check out this guide.\\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a list of all models that support tool calling here.\\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\\nDefining tool schemas\\u200b\\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it\\'s arguments are. Chat models that support tool calling features implement a .bind_tools() method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain Tool objects. Subsequent invocations of the model will pass in these tool schemas along with the prompt.\\nPython functions\\u200b\\nOur tool schemas can be Python functions:\\n# The function name, type hints, and docstring are all part of the tool# schema that\\'s passed to the model. Defining good, descriptive schemas# is an extension of prompt engineering and is an important part of# getting models to perform well.def add(a: int, b: int) -> int:    \"\"\"Add two integers.    Args:        a: First integer        b: Second integer    \"\"\"    return a + bdef multiply(a: int, b: int) -> int:    \"\"\"Multiply two integers.    Args:        a: First integer        b: Second integer    \"\"\"    return a * b\\nLangChain Tool\\u200b\\nLangChain also implements a @tool decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide here for details.\\nPydantic class\\u200b\\nYou can equivalently define the schemas without the accompanying functions using Pydantic.\\nNote that all fields are required unless provided a default value.\\nfrom pydantic import BaseModel, Fieldclass add(BaseModel):    \"\"\"Add two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")class multiply(BaseModel):    \"\"\"Multiply two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")\\nTypedDict class\\u200b\\nRequires langchain-core>=0.2.25\\nOr using TypedDicts and annotations:\\nfrom typing_extensions import Annotated, TypedDictclass add(TypedDict):    \"\"\"Add two integers.\"\"\"    # Annotations must have the type and can optionally include a default value and description (in that order).    a: Annotated[int, ..., \"First integer\"]    b: Annotated[int, ..., \"Second integer\"]class multiply(TypedDict):    \"\"\"Multiply two integers.\"\"\"    a: Annotated[int, ..., \"First integer\"]    b: Annotated[int, ..., \"Second integer\"]tools = [add, multiply]\\nTo actually bind those schemas to a chat model, we\\'ll use the .bind_tools() method. This handles converting\\nthe add and multiply schemas to the proper format for the model. The tool schema will then be passed it in each time the model is invoked.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Select chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\nllm_with_tools = llm.bind_tools(tools)query = \"What is 3 * 12?\"llm_with_tools.invoke(query)\\nAIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_iXj4DiW1p7WLjTAQMRO0jxMs\\', \\'function\\': {\\'arguments\\': \\'{\"a\":3,\"b\":12}\\', \\'name\\': \\'multiply\\'}, \\'type\\': \\'function\\'}], \\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 17, \\'prompt_tokens\\': 80, \\'total_tokens\\': 97}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_483d39d857\\', \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-0b620986-3f62-4df7-9ba3-4595089f9ad4-0\\', tool_calls=[{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 3, \\'b\\': 12}, \\'id\\': \\'call_iXj4DiW1p7WLjTAQMRO0jxMs\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 80, \\'output_tokens\\': 17, \\'total_tokens\\': 97})\\nAs we can see our LLM generated arguments to a tool! You can look at the docs for bind_tools() to learn about all the ways to customize how your LLM selects tools, as well as this guide on how to force the LLM to call a tool rather than letting it decide.\\nTool calls\\u200b\\nIf tool calls are included in a LLM response, they are attached to the corresponding\\nmessage\\nor message chunk\\nas a list of tool call\\nobjects in the .tool_calls attribute.\\nNote that chat models can call multiple tools at once.\\nA ToolCall is a typed dict that includes a\\ntool name, dict of argument values, and (optionally) an identifier. Messages with no\\ntool calls default to an empty list for this attribute.\\nquery = \"What is 3 * 12? Also, what is 11 + 49?\"llm_with_tools.invoke(query).tool_calls\\n[{\\'name\\': \\'multiply\\',  \\'args\\': {\\'a\\': 3, \\'b\\': 12},  \\'id\\': \\'call_1fyhJAbJHuKQe6n0PacubGsL\\',  \\'type\\': \\'tool_call\\'}, {\\'name\\': \\'add\\',  \\'args\\': {\\'a\\': 11, \\'b\\': 49},  \\'id\\': \\'call_fc2jVkKzwuPWyU7kS9qn1hyG\\',  \\'type\\': \\'tool_call\\'}]\\nThe .tool_calls attribute should contain valid tool calls. Note that on occasion,\\nmodel providers may output malformed tool calls (e.g., arguments that are not\\nvalid JSON). When parsing fails in these cases, instances\\nof InvalidToolCall\\nare populated in the .invalid_tool_calls attribute. An InvalidToolCall can have\\na name, string arguments, identifier, and error message.\\nParsing\\u200b\\nIf desired, output parsers can further process the output. For example, we can convert existing values populated on the .tool_calls to Pydantic objects using the\\nPydanticToolsParser:\\nfrom langchain_core.output_parsers import PydanticToolsParserfrom pydantic import BaseModel, Fieldclass add(BaseModel):    \"\"\"Add two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")class multiply(BaseModel):    \"\"\"Multiply two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])chain.invoke(query)API Reference:PydanticToolsParser\\n[multiply(a=3, b=12), add(a=11, b=49)]\\nNext steps\\u200b\\nNow you\\'ve learned how to bind tool schemas to a chat model and have the model call the tool.\\nNext, check out this guide on actually using the tool by invoking the function and passing the results back to the model:\\n\\nPass tool results back to model\\n\\nYou can also check out some more specific uses of tool calling:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Pass tool results back to model\\n\\nYou can also check out some more specific uses of tool calling:\\n\\nGetting structured outputs from models\\nFew shot prompting with tools\\nStream tool calls\\nPass runtime values to tools\\nEdit this pageWas this page helpful?PreviousHow to return artifacts from a toolNextHow to disable parallel tool callingDefining tool schemasPython functionsLangChain ToolPydantic classTypedDict classTool callsParsingNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='In this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='from langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Examples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to compose prompts together | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to compose prompts togetherOn this pageHow to compose prompts together'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='LangChain provides a user friendly interface for composing different parts of prompts together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components.\\nString prompt composition\\u200b\\nWhen working with string prompts, each template is joined together. You can work with either prompts directly or strings (the first element in the list needs to be a prompt).\\nfrom langchain_core.prompts import PromptTemplateprompt = (    PromptTemplate.from_template(\"Tell me a joke about {topic}\")    + \", make it funny\"    + \"\\\\n\\\\nand in {language}\")promptAPI Reference:PromptTemplate\\nPromptTemplate(input_variables=[\\'language\\', \\'topic\\'], template=\\'Tell me a joke about {topic}, make it funny\\\\n\\\\nand in {language}\\')\\nprompt.format(topic=\"sports\", language=\"spanish\")\\n\\'Tell me a joke about sports, make it funny\\\\n\\\\nand in spanish\\'\\nChat prompt composition\\u200b\\nA chat prompt is made up of a list of messages. Similarly to the above example, we can concatenate chat prompt templates. Each new element is a new message in the final prompt.\\nFirst, let\\'s initialize the a ChatPromptTemplate with a SystemMessage.\\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessageprompt = SystemMessage(content=\"You are a nice pirate\")API Reference:AIMessage | HumanMessage | SystemMessage\\nYou can then easily create a pipeline combining it with other messages or message templates.\\nUse a Message when there is no variables to be formatted, use a MessageTemplate when there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a HumanMessagePromptTemplate.)\\nnew_prompt = (    prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\")\\nUnder the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!\\nnew_prompt.format_messages(input=\"i said hi\")\\n[SystemMessage(content=\\'You are a nice pirate\\'), HumanMessage(content=\\'hi\\'), AIMessage(content=\\'what?\\'), HumanMessage(content=\\'i said hi\\')]\\nUsing PipelinePrompt\\u200b\\nLangChain includes a class called PipelinePromptTemplate, which can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts:\\n\\nFinal prompt: The final prompt that is returned\\nPipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='from langchain_core.prompts import PipelinePromptTemplate, PromptTemplatefull_template = \"\"\"{introduction}{example}{start}\"\"\"full_prompt = PromptTemplate.from_template(full_template)introduction_template = \"\"\"You are impersonating {person}.\"\"\"introduction_prompt = PromptTemplate.from_template(introduction_template)example_template = \"\"\"Here\\'s an example of an interaction:Q: {example_q}A: {example_a}\"\"\"example_prompt = PromptTemplate.from_template(example_template)start_template = \"\"\"Now, do this for real!Q: {input}A:\"\"\"start_prompt = PromptTemplate.from_template(start_template)input_prompts = [    (\"introduction\", introduction_prompt),    (\"example\", example_prompt),    (\"start\", start_prompt),]pipeline_prompt = PipelinePromptTemplate(    final_prompt=full_prompt, pipeline_prompts=input_prompts)pipeline_prompt.input_variablesAPI Reference:PipelinePromptTemplate | PromptTemplate\\n[\\'person\\', \\'example_a\\', \\'example_q\\', \\'input\\']\\nprint(    pipeline_prompt.format(        person=\"Elon Musk\",        example_q=\"What\\'s your favorite car?\",        example_a=\"Tesla\",        input=\"What\\'s your favorite social media site?\",    ))\\nYou are impersonating Elon Musk.Here\\'s an example of an interaction:Q: What\\'s your favorite car?A: TeslaNow, do this for real!Q: What\\'s your favorite social media site?A:\\nNext steps\\u200b\\nYou\\'ve now learned how to compose prompts together.\\nNext, check out the other how-to guides on prompt templates in this section, like adding few-shot examples to your prompt templates.Edit this pageWas this page helpful?PreviousHow to pass through arguments from one step to the nextNextHow to handle multiple retrievers when doing query analysisString prompt compositionChat prompt compositionUsing PipelinePromptNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to run custom functionsOn this pageHow to run custom functions'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='You can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Using the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionally equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Passing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='implementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to invoke runnables in parallelOn this pageHow to invoke runnables in parallel'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='The RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.\\nFormatting with RunnableParallels\\u200b\\nRunnableParallels are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:\\n     Input      / \\\\     /   \\\\ Branch1 Branch2     \\\\   /      \\\\ /      Combine\\nBelow, the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key.\\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"# The prompt expects input with keys for \"context\" and \"question\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")API Reference:FAISS | StrOutputParser | ChatPromptTemplate | RunnablePassthrough | ChatOpenAI | OpenAIEmbeddings\\n\\'Harrison worked at Kensho.\\'\\ntipNote that when composing a RunnableParallel with another Runnable we don\\'t even need to wrap our dictionary in the RunnableParallel class ‚Äî\\xa0the type conversion is handled for us. In the context of a chain, these are equivalent:\\n{\"context\": retriever, \"question\": RunnablePassthrough()}\\nRunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\\nRunnableParallel(context=retriever, question=RunnablePassthrough())\\nSee the section on coercion for more.\\nUsing itemgetter as shorthand\\u200b\\nNote that you can use Python\\'s itemgetter as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about itemgetter in the Python Documentation.\\nIn the example below, we use itemgetter to extract specific keys from the map:\\nfrom operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})API Reference:FAISS | StrOutputParser | ChatPromptTemplate | RunnablePassthrough | ChatOpenAI | OpenAIEmbeddings\\n\\'Harrison ha lavorato a Kensho.\\'\\nParallelize steps\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='\\'Harrison ha lavorato a Kensho.\\'\\nParallelize steps\\u200b\\nRunnableParallels make it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallelfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelpoem_chain = (    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model)map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)map_chain.invoke({\"topic\": \"bear\"})API Reference:ChatPromptTemplate | RunnableParallel | ChatOpenAI\\n{\\'joke\\': AIMessage(content=\"Why don\\'t bears like fast food? Because they can\\'t catch it!\", response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 15, \\'prompt_tokens\\': 13, \\'total_tokens\\': 28}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_d9767fc5b9\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-fe024170-c251-4b7a-bfd4-64a3737c67f2-0\\'), \\'poem\\': AIMessage(content=\\'In the quiet of the forest, the bear roams free\\\\nMajestic and wild, a sight to see.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 24, \\'prompt_tokens\\': 15, \\'total_tokens\\': 39}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-2707913e-a743-4101-b6ec-840df4568a76-0\\')}\\nParallelism\\u200b\\nRunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.\\n%%timeitjoke_chain.invoke({\"topic\": \"bear\"})\\n610 ms ¬± 64 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\\n%%timeitpoem_chain.invoke({\"topic\": \"bear\"})\\n599 ms ¬± 73.3 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\\n%%timeitmap_chain.invoke({\"topic\": \"bear\"})\\n643 ms ¬± 77.8 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\\nNext steps\\u200b\\nYou now know some ways to format and parallelize chain steps with RunnableParallel.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to add a semantic layer over graph databaseNextHow to stream chat model responsesFormatting with RunnableParallelsUsing itemgetter as shorthandParallelize stepsParallelismNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to chain runnables | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to chain runnablesOn this pageHow to chain runnables'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nPrompt templates\\nChat models\\nOutput parser'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='One point about LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing.\\nThe resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like LangSmith.\\nThe pipe operator: |\\u200b\\nTo show off how this works, let\\'s go through an example. We\\'ll walk through a common pattern in LangChain: using a prompt template to format input into a chat model, and finally converting the chat message output into a string with an output parser.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Select chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | model | StrOutputParser()API Reference:StrOutputParser | ChatPromptTemplate\\nPrompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:\\nchain.invoke({\"topic\": \"bears\"})\\n\"Here\\'s a bear joke for you:\\\\n\\\\nWhy did the bear dissolve in water?\\\\nBecause it was a polar bear!\"\\nCoercion\\u200b\\nWe can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.\\nFor example, let\\'s say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.\\nWe would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a RunnableParallel, which runs all of its values in parallel and returns a dict with the results.\\nThis happens to be the same format the next prompt template expects. Here it is in action:\\nfrom langchain_core.output_parsers import StrOutputParseranalysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()composed_chain.invoke({\"topic\": \"bears\"})API Reference:StrOutputParser\\n\\'Haha, that\\\\\\'s a clever play on words! Using \"polar\" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.\\'\\nFunctions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:\\ncomposed_chain_with_lambda = (    chain    | (lambda input: {\"joke\": input})    | analysis_prompt    | model    | StrOutputParser())composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\\n\"Haha, that\\'s a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it\\'s a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!\"\\nHowever, keep in mind that using functions like this may interfere with operations like streaming. See this section for more information.\\nThe .pipe() method\\u200b\\nWe could also compose the same sequence using the .pipe() method. Here\\'s what that looks like:\\nfrom langchain_core.runnables import RunnableParallelcomposed_chain_with_pipe = (    RunnableParallel({\"joke\": chain})    .pipe(analysis_prompt)    .pipe(model)    .pipe(StrOutputParser()))composed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})API Reference:RunnableParallel'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='\"I cannot reproduce any copyrighted material verbatim, but I can try to analyze the humor in the joke you provided without quoting it directly.\\\\n\\\\nThe joke plays on the idea that the Cylon raiders, who are the antagonists in the Battlestar Galactica universe, failed to locate the human survivors after attacking their home planets (the Twelve Colonies) due to using an outdated and poorly performing operating system (Windows Vista) for their targeting systems.\\\\n\\\\nThe humor stems from the juxtaposition of a futuristic science fiction setting with a relatable real-world frustration ‚Äì the use of buggy, slow, or unreliable software or technology. It pokes fun at the perceived inadequacies of Windows Vista, which was widely criticized for its performance issues and other problems when it was released.\\\\n\\\\nBy attributing the Cylons\\' failure to locate the humans to their use of Vista, the joke creates an amusing and unexpected connection between a fictional advanced race of robots and a familiar technological annoyance experienced by many people in the real world.\\\\n\\\\nOverall, the joke relies on incongruity and relatability to generate humor, but without reproducing any copyrighted material directly.\"\\nOr the abbreviated:\\ncomposed_chain_with_pipe = RunnableParallel({\"joke\": chain}).pipe(    analysis_prompt, model, StrOutputParser())\\nRelated\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Streaming: Check out the streaming guide to understand the streaming behavior of a chain\\nEdit this pageWas this page helpful?PreviousHow to split text based on semantic similarityNextHow to save and load LangChain objectsThe pipe operator: |CoercionThe .pipe() methodRelatedCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='Conceptual guide | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideOn this pageConceptual guide'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\nWe recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples ‚Äî those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference.\\nHigh level\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='Why LangChain?: Overview of the value that LangChain provides.\\nArchitecture: How packages are organized in the LangChain ecosystem.\\n\\nConcepts\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='Concepts\\u200b\\n\\nChat models: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\nMessages: The unit of communication in chat models, used to represent model input and output.\\nChat history: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\nTools: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\nTool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\nStructured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\nMemory: Information about a conversation that is persisted so that it can be used in future conversations.\\nMultimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\nStreaming: LangChain streaming APIs for surfacing results as they are generated.\\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\\nDocument loaders: Load a source as a list of documents.\\nRetrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\nText splitters: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\nEmbedding models: Models that represent data such as text or images in a vector space.\\nVector stores: Storage of and efficient search over vectors and associated metadata.\\nRetriever: A component that returns relevant documents from a knowledge base in response to a query.\\nRetrieval Augmented Generation (RAG): A technique that enhances language models by combining them with external knowledge bases.\\nAgents: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool.\\nPrompt templates: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\nOutput parsers: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs.\\nFew-shot prompting: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\nExample selectors: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\nAsync programming: The basics that one should know to use LangChain in an asynchronous context.\\nCallbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\nTracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\nEvaluation: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\nTesting: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\nGlossary\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content=\"AIMessageChunk: A partial response from an AI message. Used when streaming responses from a chat model.\\nAIMessage: Represents a complete response from an AI model.\\nastream_events: Stream granular information from LCEL chains.\\nBaseTool: The base class for all tools in LangChain.\\nbatch: Use to execute a runnable with batch inputs.\\nbind_tools: Allows models to interact with tools.\\nCaching: Storing results to avoid redundant calls to a chat model.\\nChat models: Chat models that handle multiple data modalities.\\nConfigurable runnables: Creating configurable Runnables.\\nContext window: The maximum size of input a chat model can process.\\nConversation patterns: Common patterns in chat interactions.\\nDocument: LangChain's representation of a document.\\nEmbedding models: Models that generate vector embeddings for various data types.\\nHumanMessage: Represents a message from a human user.\\nInjectedState: A state injected into a tool function.\\nInjectedStore: A store that can be injected into a tool for data persistence.\\nInjectedToolArg: Mechanism to inject arguments into tool functions.\\ninput and output types: Types used for input and output in Runnables.\\nIntegration packages: Third-party packages that integrate with LangChain.\\nIntegration tests: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\ninvoke: A standard method to invoke a Runnable.\\nJSON mode: Returning responses in JSON format.\\nlangchain-community: Community-driven components for LangChain.\\nlangchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\\nlangchain: A package for higher level components (e.g., some pre-built chains).\\nlanggraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\nlangserve: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\nLLMs (legacy): Older language models that take a string as input and return a string as output.\\nManaging chat history: Techniques to maintain and manage the chat history.\\nOpenAI format: OpenAI's message format for chat models.\\nPropagation of RunnableConfig: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\nrate-limiting: Client side rate limiting for chat models.\\nRemoveMessage: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\nrole: Represents the role (e.g., user, assistant) of a chat message.\\nRunnableConfig: Use to pass run time information to Runnables (e.g., run_name, run_id, tags, metadata, max_concurrency, recursion_limit, configurable).\\nStandard parameters for chat models: Parameters such as API key, temperature, and max_tokens.\\nStandard tests: A defined set of unit and integration tests that all integrations must pass.\\nstream: Use to stream output from a Runnable or a graph.\\nTokenization: The process of converting data into tokens and vice versa.\\nTokens: The basic unit that a language model reads, processes, and generates under the hood.\\nTool artifacts: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\nTool binding: Binding tools to models.\\n@tool: Decorator for creating tools in LangChain.\\nToolkits: A collection of tools that can be used together.\\nToolMessage: Represents a message that contains the results of a tool execution.\\nUnit tests: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\nVector stores: Datastores specialized for storing and efficiently searching vector embeddings.\\nwith_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='with_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\nwith_types: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\nEdit this pageWas this page helpful?PreviousHow to create and query vector storesNextAgentsHigh levelConceptsGlossaryCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='How to install LangChain packages | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to install LangChain packagesOn this pageHow to install LangChain packages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\\nfunctionality to install.\\nOfficial release\\u200b\\nTo install the main langchain package, run:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='PipCondapip install langchainconda install langchain -c conda-forge\\nWhile this package acts as a sane starting point to using LangChain,\\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\\nEcosystem packages\\u200b\\nWith the exception of the langsmith SDK, all packages in the LangChain ecosystem depend on langchain-core, which contains base\\nclasses and abstractions that other packages use. The dependency graph below shows how the different packages are related.\\nA directed arrow indicates that the source package depends on the target package:\\n\\nWhen installing a package, you do not need to explicitly install that package\\'s explicit dependencies (such as langchain-core).\\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\\nLangChain core\\u200b\\nThe langchain-core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by langchain, but can also be used separately. Install with:\\npip install langchain-core\\nIntegration packages\\u200b\\nCertain integrations like OpenAI and Anthropic have their own packages.\\nAny integrations that require their own package will be documented as such in the Integration docs.\\nYou can see a list of all integration packages in the API reference under the \"Partner libs\" dropdown.\\nTo install one of these run:\\npip install langchain-openai\\nAny integrations that haven\\'t been split out into their own packages will live in the langchain-community package. Install with:\\npip install langchain-community\\nLangChain experimental\\u200b\\nThe langchain-experimental package holds experimental LangChain code, intended for research and experimental uses.\\nInstall with:\\npip install langchain-experimental\\nLangGraph\\u200b\\nlanggraph is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\\nInstall with:\\npip install langgraph\\nLangServe\\u200b\\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\\nLangServe is automatically installed by LangChain CLI.\\nIf not using LangChain CLI, install with:\\npip install \"langserve[all]\"\\nfor both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\\nLangChain CLI\\u200b\\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\\nInstall with:\\npip install langchain-cli\\nLangSmith SDK\\u200b\\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\\nlangchain-core, and can be installed and used independently if desired.\\nIf you are not using LangChain, you can install it with:\\npip install langsmith\\nFrom source\\u200b\\nIf you want to install a package from source, you can do so by cloning the main LangChain repo, enter the directory of the package you want to install PATH/TO/REPO/langchain/libs/{package}, and run:\\npip install -e .\\nLangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see all repos here.Edit this pageWas this page helpful?PreviousHow to do tool/function callingNextHow to add examples to the prompt for query analysisOfficial releaseEcosystem packagesLangChain coreIntegration packagesLangChain experimentalLangGraphLangServeLangChain CLILangSmith SDKFrom sourceCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='How to load Markdown | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load MarkdownOn this pageHow to load Markdown'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\\nHere we cover how to load Markdown documents into LangChain Document objects that we can use downstream.\\nWe will cover:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='Basic usage;\\nParsing of Markdown into elements such as titles, list items, and text.\\n\\nLangChain implements an UnstructuredMarkdownLoader object which requires the Unstructured package. First we install it:\\n%pip install \"unstructured[md]\" nltk\\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain\\'s readme:\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoaderfrom langchain_core.documents import Documentmarkdown_path = \"../../../README.md\"loader = UnstructuredMarkdownLoader(markdown_path)data = loader.load()assert len(data) == 1assert isinstance(data[0], Document)readme_content = data[0].page_contentprint(readme_content[:250])API Reference:UnstructuredMarkdownLoader | Document\\nü¶úÔ∏èüîó LangChain‚ö° Build context-aware reasoning applications ‚ö°Looking for the JS/TS library? Check out LangChain.js.To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building,\\nRetain Elements\\u200b\\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\\\n\")for document in data[:2]:    print(f\"{document}\\\\n\")\\nNumber of documents: 66page_content=\\'ü¶úÔ∏èüîó LangChain\\' metadata={\\'source\\': \\'../../../README.md\\', \\'category_depth\\': 0, \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'Title\\'}page_content=\\'‚ö° Build context-aware reasoning applications ‚ö°\\' metadata={\\'source\\': \\'../../../README.md\\', \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'parent_id\\': \\'200b8a7d0dd03f66e4f13456566d2b3a\\', \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'NarrativeText\\'}\\nNote that in this case we recover three distinct element types:\\nprint(set(document.metadata[\"category\"] for document in data))\\n{\\'ListItem\\', \\'NarrativeText\\', \\'Title\\'}Edit this pageWas this page helpful?PreviousHow to load JSONNextHow to load Microsoft Office filesRetain ElementsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='How to load JSON | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load JSONOn this pageHow to load JSON'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).\\nJSON Lines is a file format where each line is a valid JSON value.\\nLangChain implements a JSONLoader\\nto convert JSON and JSONL data into LangChain Document\\nobjects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content\\nand metadata of the LangChain Document.\\nIt uses the jq python package. Check out this manual for a detailed documentation of the jq syntax.\\nHere we will demonstrate:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='How to load JSON and JSONL data into the content of a LangChain Document;\\nHow to load JSON and JSONL data into metadata associated with a Document.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content=\"#!pip install jq\\nfrom langchain_community.document_loaders import JSONLoaderAPI Reference:JSONLoader\\nimport jsonfrom pathlib import Pathfrom pprint import pprintfile_path='./example_data/facebook_chat.json'data = json.loads(Path(file_path).read_text())\\npprint(data)\\n    {'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},     'is_still_participant': True,     'joinable_mode': {'link': '', 'mode': 1},     'magic_words': [],     'messages': [{'content': 'Bye!',                   'sender_name': 'User 2',                   'timestamp_ms': 1675597571851},                  {'content': 'Oh no worries! Bye',                   'sender_name': 'User 1',                   'timestamp_ms': 1675597435669},                  {'content': 'No Im sorry it was my mistake, the blue one is not '                              'for sale',                   'sender_name': 'User 2',                   'timestamp_ms': 1675596277579},                  {'content': 'I thought you were selling the blue one!',                   'sender_name': 'User 1',                   'timestamp_ms': 1675595140251},                  {'content': 'Im not interested in this bag. Im interested in the '                              'blue one!',                   'sender_name': 'User 1',                   'timestamp_ms': 1675595109305},                  {'content': 'Here is $129',                   'sender_name': 'User 2',                   'timestamp_ms': 1675595068468},                  {'photos': [{'creation_timestamp': 1675595059,                               'uri': 'url_of_some_picture.jpg'}],                   'sender_name': 'User 2',                   'timestamp_ms': 1675595060730},                  {'content': 'Online is at least $100',                   'sender_name': 'User 2',                   'timestamp_ms': 1675595045152},                  {'content': 'How much do you want?',                   'sender_name': 'User 1',                   'timestamp_ms': 1675594799696},                  {'content': 'Goodmorning! $50 is too low.',                   'sender_name': 'User 2',                   'timestamp_ms': 1675577876645},                  {'content': 'Hi! Im interested in your bag. Im offering $50. Let '                              'me know if you are interested. Thanks!',                   'sender_name': 'User 1',                   'timestamp_ms': 1675549022673}],     'participants': [{'name': 'User 1'}, {'name': 'User 2'}],     'thread_path': 'inbox/User 1 and User 2 chat',     'title': 'User 1 and User 2 chat'}\\nUsing JSONLoader\\u200b\\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\\nJSON file\\u200b\\nloader = JSONLoader(    file_path='./example_data/facebook_chat.json',    jq_schema='.messages[].content',    text_content=False)data = loader.load()\\npprint(data)\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='JSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\nJSON Lines file\\u200b\\nIf you want to load documents from a JSON Lines file, you pass json_lines=True\\nand specify jq_schema to extract page_content from a single JSON object.\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'pprint(Path(file_path).read_text())\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'     \\'worries! Bye\"}\\\\n\\'     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.content\\',    text_content=False,    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='Another option is to set jq_schema=\\'.\\' and provide content_key:\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.\\',    content_key=\\'sender_name\\',    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nJSON file with jq schema content_key\\u200b\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\\nEnsure that content_key is compatible and can be parsed using the jq schema.\\nfile_path = \\'./sample.json\\'pprint(Path(file_path).read_text())\\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\nExtracting metadata\\u200b\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\nThe following demonstrates how metadata can be extracted using the JSONLoader.\\nThere are some key changes to be noted. In the previous example where we didn\\'t collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\\n.messages[].content\\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\\n.messages[]\\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content=\"pprint(data)\\n    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\\nNow, you will see that the documents contain the metadata associated with the content we extracted.\\nThe metadata_func\\u200b\\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='The example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='Common JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"Edit this pageWas this page helpful?PreviousHow to load HTMLNextHow to load MarkdownUsing JSONLoaderJSON fileJSON Lines fileJSON file with jq schema content_keyExtracting metadataThe metadata_funcCommon JSON structures with jq schemaCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='How to load PDFs | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load PDFsOn this pageHow to load PDFs'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\\nThis guide covers how to load PDF documents into the LangChain Document format that we use downstream.\\nText in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Agglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\\nRun OCR on images to detect text therein;\\nClassify text as belonging to paragraphs, lists, tables, or other structures;\\nStructure text into table rows and columns, or key-value pairs.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\\nWe will demonstrate these approaches on a sample file:\\nfile_path = (    \"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\")\\nA note on multimodal modelsMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the Use of multimodal models section below.\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of Document objects-- one per page-- containing a single string of the page\\'s text in the Document\\'s page_content attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the pypdf Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\n%pip install -qU pypdf\\nfrom langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(file_path)pages = []async for page in loader.alazy_load():    pages.append(page)API Reference:PyPDFLoader\\nprint(f\"{pages[0].metadata}\\\\n\")print(pages[0].page_content)\\n{\\'source\\': \\'../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\\', \\'page\\': 0}LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University{melissadell,jacob carlson }@fas.harvard.edu4University of Washingtonbcgl@cs.washington.edu5University of Waterloow422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have beenprimarily driven by the application of neural networks. Ideally, researchoutcomes could be easily deployed in production and extended for furtherinvestigation. However, various factors like loosely organized codebasesand sophisticated model conÔ¨Ågurations complicate the easy reuse of im-portant innovations by a wide audience. Though there have been on-goingeÔ¨Äorts to improve reusability and simplify deep learning (DL) modeldevelopment in disciplines like natural language processing and computervision, none of them are optimized for challenges in the domain of DIA.This represents a major gap in the existing toolkit, as DIA is central toacademic research across a wide range of disciplines in the social sciencesand humanities. This paper introduces LayoutParser , an open-sourcelibrary for streamlining the usage of DL in DIA research and applica-tions. The core LayoutParser library comes with a set of simple andintuitive interfaces for applying and customizing DL models for layout de-tection, character recognition, and many other document processing tasks.To promote extensibility, LayoutParser also incorporates a communityplatform for sharing both pre-trained models and full document digiti-zation pipelines. We demonstrate that LayoutParser is helpful for bothlightweight and large-scale digitization pipelines in real-word use cases.The library is publicly available at https://layout-parser.github.io .Keywords: Document Image Analysis ¬∑Deep Learning ¬∑Layout Analysis¬∑Character Recognition ¬∑Open Source library ¬∑Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range ofdocument image analysis (DIA) tasks including document image classiÔ¨Åcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Note that the metadata of each document stores the corresponding page number.\\nVector search over PDFs\\u200b\\nOnce we have loaded PDFs into LangChain Document objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)for doc in docs:    print(f\\'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings\\nPage 13: 14 Z. Shen et al.6 ConclusionLayoutParser provides a comprehensive toolkit for deep learning-based documentimage analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used tobuild Ô¨Çexible and accurate pipelines for processing documents with complicatedstructures. It also supports hiPage 0: LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University\\nLayout analysis and extraction of text from images\\u200b\\nIf you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of Document objects, where each object represents a structure on the page. The Document\\'s metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nUnstructured supports multiple parameters for PDF parsing:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='strategy (e.g., \"fast\" or \"hi-res\")\\nAPI or local processing. You will need an API key to use the API.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='The hi-res strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See local parsing section below for considerations when running locally.\\n%pip install -qU langchain-unstructured\\nimport getpassimport osif \"UNSTRUCTURED_API_KEY\" not in os.environ:    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")\\nUnstructured API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\nAs before, we initialize a loader and load documents lazily:\\nfrom langchain_unstructured import UnstructuredLoaderloader = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",    partition_via_api=True,    coordinates=True,)docs = []for doc in loader.lazy_load():    docs.append(doc)API Reference:UnstructuredLoader\\nINFO: Preparing to split document for partition.INFO: Starting page number set to 1INFO: Allow failed set to 0INFO: Concurrency level set to 5INFO: Splitting pages 1 to 16 (16 total)INFO: Determined optimal split size of 4 pages.INFO: Partitioning 4 files with 4 page(s) each.INFO: Partitioning set #1 (pages 1-4).INFO: Partitioning set #2 (pages 5-8).INFO: Partitioning set #3 (pages 9-12).INFO: Partitioning set #4 (pages 13-16).INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: Successfully partitioned set #1, elements added to the final result.INFO: Successfully partitioned set #2, elements added to the final result.INFO: Successfully partitioned set #3, elements added to the final result.INFO: Successfully partitioned set #4, elements added to the final result.\\nHere we recover 171 distinct structures over the 16 page document:\\nprint(len(docs))\\n171\\nWe can use the document metadata to recover content from a single page:\\nfirst_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]for doc in first_page_docs:    print(doc.page_content)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='first_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]for doc in first_page_docs:    print(doc.page_content)\\nLayoutParser: A UniÔ¨Åed Toolkit for Deep Learning Based Document Image Analysis1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r aZejiang Shen¬Æ (<), Ruochen Zhang?, Melissa Dell¬Æ, Benjamin Charles Germain Lee?, Jacob Carlson¬Æ, and Weining Li¬Æ1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conÔ¨Ågurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eÔ¨Äorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.Keywords: Document Image Analysis ¬∑ Deep Learning ¬∑ Layout Analysis ¬∑ Character Recognition ¬∑ Open Source library ¬∑ Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiÔ¨Åcation [11,\\nExtracting tables and other structures\\u200b\\nEach Document we load represents a structure, like a title, paragraph, or table.\\nSome structures may be of special interest for indexing or question-answering tasks. These structures may be:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Classified for easy identification;\\nParsed into a more structured representation.\\n\\nBelow, we identify and extract a table:\\nClick to expand code for rendering pages%pip install -qU matplotlib PyMuPDF pillowimport fitzimport matplotlib.patches as patchesimport matplotlib.pyplot as pltfrom PIL import Imagedef plot_pdf_with_boxes(pdf_page, segments):    pix = pdf_page.get_pixmap()    pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    fig, ax = plt.subplots(1, figsize=(10, 10))    ax.imshow(pil_image)    categories = set()    category_to_color = {        \"Title\": \"orchid\",        \"Image\": \"forestgreen\",        \"Table\": \"tomato\",    }    for segment in segments:        points = segment[\"coordinates\"][\"points\"]        layout_width = segment[\"coordinates\"][\"layout_width\"]        layout_height = segment[\"coordinates\"][\"layout_height\"]        scaled_points = [            (x * pix.width / layout_width, y * pix.height / layout_height)            for x, y in points        ]        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")        categories.add(segment[\"category\"])        rect = patches.Polygon(            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"        )        ax.add_patch(rect)    # Make legend    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]    for category in [\"Title\", \"Image\", \"Table\"]:        if category in categories:            legend_handles.append(                patches.Patch(color=category_to_color[category], label=category)            )    ax.axis(\"off\")    ax.legend(handles=legend_handles, loc=\"upper right\")    plt.tight_layout()    plt.show()def render_page(doc_list: list, page_number: int, print_text=True) -> None:    pdf_page = fitz.open(file_path).load_page(page_number - 1)    page_docs = [        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number    ]    segments = [doc.metadata for doc in page_docs]    plot_pdf_with_boxes(pdf_page, segments)    if print_text:        for doc in page_docs:            print(f\"{doc.page_content}\\\\n\")\\nrender_page(docs, 5)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='LayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIA5Table 1: Current layout detection models in the LayoutParser model zooDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiÔ¨Åc documents Layouts of scanned modern magazines and scientiÔ¨Åc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiÔ¨Åc and business document Layouts of history Japanese documents1 For each dataset, we train several models of diÔ¨Äerent sizes for diÔ¨Äerent needs (the trade-oÔ¨Ä between accuracy vs. computational cost). For ‚Äúbase model‚Äù and ‚Äúlarge model‚Äù, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diÔ¨Äerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.layout data structures, which are optimized for eÔ¨Éciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniÔ¨Åed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.3.1 Layout Detection ModelsIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. DiÔ¨Äerent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:1 import layoutparser as lp 2 image = cv2 . imread ( \" image_file \" ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( \" lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config \" ) 4 5 layout = model . detect ( image )LayoutParser provides a wealth of pre-trained model weights using various datasets covering diÔ¨Äerent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiÔ¨Åcantly diÔ¨Äerent from the training dataset. As document structures and layouts vary greatly in diÔ¨Äerent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.\\nNote that although the table text is collapsed into a single string in the document\\'s content, the metadata contains a representation of its rows and columns:\\nfrom IPython.display import HTML, displaysegments = [    doc.metadata    for doc in docs    if doc.metadata.get(\"page_number\") == 5 and doc.metadata.get(\"category\") == \"Table\"]display(HTML(segments[0][\"text_as_html\"]))\\nable 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200Dataset| Base Model\\'|NotesPubLayNet [38]F/MLayouts of modern scientific documentsPRImAMLayouts of scanned modern magazines and scientific reportsNewspaperFLayouts of scanned US newspapers from the 20th centuryTableBank [18]FTable region on modern scientific and business documentHJDatasetF/MLayouts of history Japanese documents\\nExtracting text from specific sections\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Extracting text from specific sections\\u200b\\nStructures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nBelow, we extract all text associated with the document\\'s \"Conclusion\" section:\\nrender_page(docs, 14, print_text=False)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='conclusion_docs = []parent_id = -1for doc in docs:    if doc.metadata[\"category\"] == \"Title\" and \"Conclusion\" in doc.page_content:        parent_id = doc.metadata[\"element_id\"]    if doc.metadata.get(\"parent_id\") == parent_id:        conclusion_docs.append(doc)for doc in conclusion_docs:    print(doc.page_content)\\nLayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used to build Ô¨Çexible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.\\nExtracting text from images\\u200b\\nOCR is run on images, enabling the extraction of text therein:\\nrender_page(docs, 11)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='LayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIAfocuses on precision, eÔ¨Éciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and Ô¨Çexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.115.1 A Comprehensive Historical Document Digitization PipelineThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese Ô¨Årm Ô¨Ånancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diÔ¨Äerent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.‚ÄòActive Learning Layout Annotate Layout Dataset | +‚Äî‚Äî Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing ‚Äî Handy Data Structures & \\\\ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models ¬• Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization PipelineAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiÔ¨Åed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\\nNote that the text from the figure on the right is extracted and incorporated into the content of the Document.\\nLocal parsing\\u200b\\nParsing locally requires the installation of additional dependencies.\\nPoppler (PDF analysis)\\n\\nLinux: apt-get install poppler-utils\\nMac: brew install poppler\\nWindows: https://github.com/oschwartz10612/poppler-windows\\n\\nTesseract (OCR)\\n\\nLinux: apt-get install tesseract-ocr\\nMac: brew install tesseract\\nWindows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Tesseract (OCR)\\n\\nLinux: apt-get install tesseract-ocr\\nMac: brew install tesseract\\nWindows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\\n\\nWe will also need to install the unstructured PDF extras:\\n%pip install -qU \"unstructured[pdf]\"\\nWe can then use the UnstructuredLoader much the same way, forgoing the API key and partition_via_api setting:\\nloader_local = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",)docs_local = []for doc in loader_local.lazy_load():    docs_local.append(doc)\\nWARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model nameINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...\\nThe list of documents can then be processed similarly to those obtained from the API.\\nUse of multimodal models\\u200b\\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\\nIn principle we can use any LangChain chat model that supports multimodal inputs. A list of these models is documented here. Below we use OpenAI\\'s gpt-4o-mini.\\nFirst we define a short utility function to convert a PDF page to a base64-encoded image:\\n%pip install -qU PyMuPDF pillow langchain-openai\\nimport base64import ioimport fitzfrom PIL import Imagedef pdf_page_to_base64(pdf_path: str, page_number: int):    pdf_document = fitz.open(pdf_path)    page = pdf_document.load_page(page_number - 1)  # input is one-indexed    pix = page.get_pixmap()    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    buffer = io.BytesIO()    img.save(buffer, format=\"PNG\")    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\\nfrom IPython.display import Image as IPImagefrom IPython.display import displaybase64_image = pdf_page_to_base64(file_path, 11)display(IPImage(data=base64.b64decode(base64_image)))'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='We can then query the model in the usual way. Below we ask it a question on related to the diagram on the page.\\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")API Reference:ChatOpenAI\\nfrom langchain_core.messages import HumanMessagequery = \"What is the name of the first step in the pipeline?\"message = HumanMessage(    content=[        {\"type\": \"text\", \"text\": query},        {            \"type\": \"image_url\",            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},        },    ],)response = llm.invoke([message])print(response.content)API Reference:HumanMessage\\nINFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"``````outputThe first step in the pipeline is \"Annotate Layout Dataset.\"\\nOther PDF loaders\\u200b\\nFor a list of available LangChain PDF loaders, please see this table.Edit this pageWas this page helpful?PreviousHow to load Microsoft Office filesNextHow to load web pagesSimple and fast text extractionVector search over PDFsLayout analysis and extraction of text from imagesExtracting tables and other structuresExtracting text from specific sectionsExtracting text from imagesLocal parsingUse of multimodal modelsOther PDF loadersCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='How to load web pages | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load web pagesOn this pageHow to load web pages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.\\nLangChain integrates with a host of parsers that are appropriate for web pages. The right parser will depend on your needs. Below we demonstrate two possibilities:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Simple and fast parsing, in which we recover one Document per web page with its content represented as a \"flattened\" string;\\nAdvanced parsing, in which we recover multiple Document objects per page, allowing one to identify and traverse sections, links, tables, and other structures.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Setup\\u200b\\nFor the \"simple and fast\" parsing, we will need langchain-community and the beautifulsoup4 library:\\n%pip install -qU langchain-community beautifulsoup4\\nFor advanced parsing, we will use langchain-unstructured:\\n%pip install -qU langchain-unstructured\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of Document objects -- one per page -- containing a single string of the page\\'s text. Under the hood it uses the beautifulsoup4 Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\nimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]API Reference:WebBaseLoader\\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip())\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\', \\'title\\': \\'How to add memory to chatbots | \\\\uf8ff√º¬∂√∫√î‚àè√®\\\\uf8ff√º√Æ√≥ LangChain\\', \\'description\\': \\'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\\', \\'language\\': \\'en\\'}How to add memory to chatbots | \\uf8ff√º¬∂√∫√î‚àè√®\\uf8ff√º√Æ√≥ LangChainSkip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ff√º√≠¬®SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd\\nThis is essentially a dump of the text from the page\\'s HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired <div> classes and other parameters via BeautifulSoup. Below we parse only the body text of the article:\\nloader = WebBaseLoader(    web_paths=[page_url],    bs_kwargs={        \"parse_only\": bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\"),    },    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500])\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\'}How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We\\'ll go into more detail on a few techniq\\nprint(doc.page_content[-500:])\\na greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.\\'), | HumanMessage(content=\\'What did I say my name was?\\'), | AIMessage(content=\\'You introduced yourself as Nemo. How can I assist you today, Nemo?\\')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.\\nNote that this required advance technical knowledge of how the body text is represented in the underlying HTML.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Note that this required advance technical knowledge of how the body text is represented in the underlying HTML.\\nWe can parameterize WebBaseLoader with a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its API reference for detail.\\nAdvanced parsing\\u200b\\nThis method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one Document per page and controlling its content via BeautifulSoup, we generate multiple Document objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.\\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nfrom langchain_unstructured import UnstructuredLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = UnstructuredLoader(web_url=page_url)docs = []async for doc in loader.alazy_load():    docs.append(doc)API Reference:UnstructuredLoader\\nINFO: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.INFO: NumExpr defaulting to 8 threads.\\nNote that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:\\nfor doc in docs[:5]:    print(doc.page_content)\\nHow to add memory to chatbotsA key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:Simply stuffing previous messages into a chat model prompt.The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.More complex modifications like synthesizing summaries for long running conversations.ERROR! Session/line number was not unique in database. History logging moved to new session 2747\\nExtracting content from specific sections\\u200b\\nEach Document object represents an element of the page. Its metadata contains useful information, such as its category:\\nfor doc in docs[:5]:    print(f\\'{doc.metadata[\"category\"]}: {doc.page_content}\\')\\nTitle: How to add memory to chatbotsNarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:ListItem: Simply stuffing previous messages into a chat model prompt.ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.ListItem: More complex modifications like synthesizing summaries for long running conversations.\\nElements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nAs an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='from collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)\\n{\\'https://python.langchain.com/docs/how_to/chatbots_memory/\\': \"You\\'ll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\\\n%pip install --upgrade --quiet langchain langchain-openai\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\\\\nYou should consider upgrading via the \\'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip\\' command.[0m[33m\\\\n[0mNote: you may need to restart the kernel to use updated packages.\\\\n\", \\'https://python.langchain.com/docs/how_to/chatbots_tools/\\': \"For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\\\nYou\\'ll need to sign up for an account on the Tavily website, and install the following packages:\\\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\\\n\"}\\nVector search over page content\\u200b\\nOnce we have loaded the page contents into LangChain Document objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())retrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)for doc in retrieved_docs:    print(f\\'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings\\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"``````outputPage https://python.langchain.com/docs/how_to/chatbots_tools/: You\\'ll need to sign up for an account on the Tavily website, and install the following packages:Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\nOther web page loaders\\u200b\\nFor a list of available LangChain web page loaders, please see this table.Edit this pageWas this page helpful?PreviousHow to load PDFsNextHow to create a dynamic (self-constructing) chainSetupSimple and fast text extractionAdvanced parsingExtracting content from specific sectionsVector search over page contentOther web page loadersCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='How to load CSVs | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load CSVsOn this pageHow to load CSVs'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\\nLangChain implements a CSV Loader that will load CSV files into a sequence of Document objects. Each row of the CSV file is translated to one document.\\nfrom langchain_community.document_loaders.csv_loader import CSVLoaderfile_path = \"../integrations/document_loaders/example_data/mlb_teams_2012.csv\"loader = CSVLoader(file_path=file_path)data = loader.load()for record in data[:2]:    print(record)API Reference:CSVLoader\\npage_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 0}page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 1}\\nCustomizing the CSV parsing and loading\\u200b\\nCSVLoader will accept a csv_args kwarg that supports customization of arguments passed to Python\\'s csv.DictReader. See the csv module documentation for more information of what csv args are supported.\\nloader = CSVLoader(    file_path=file_path,    csv_args={        \"delimiter\": \",\",        \"quotechar\": \\'\"\\',        \"fieldnames\": [\"MLB Team\", \"Payroll in millions\", \"Wins\"],    },)data = loader.load()for record in data[:2]:    print(record)\\npage_content=\\'MLB Team: Team\\\\nPayroll in millions: \"Payroll (millions)\"\\\\nWins: \"Wins\"\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 0}page_content=\\'MLB Team: Nationals\\\\nPayroll in millions: 81.34\\\\nWins: 98\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 1}\\nSpecify a column to identify the document source\\u200b\\nThe \"source\" key on Document metadata can be set using a column of the CSV. Use the source_column argument to specify a source for the document created from each row. Otherwise file_path will be used as the source for all documents created from the CSV file.\\nThis is useful when using documents loaded from CSV files for chains that answer questions using sources.\\nloader = CSVLoader(file_path=file_path, source_column=\"Team\")data = loader.load()for record in data[:2]:    print(record)\\npage_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\' metadata={\\'source\\': \\'Nationals\\', \\'row\\': 0}page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\' metadata={\\'source\\': \\'Reds\\', \\'row\\': 1}\\nLoad from a string\\u200b\\nPython\\'s tempfile can be used when working with CSV strings directly.\\nimport tempfilefrom io import StringIOstring_data = \"\"\"\"Team\", \"Payroll (millions)\", \"Wins\"\"Nationals\",     81.34, 98\"Reds\",          82.20, 97\"Yankees\",      197.96, 95\"Giants\",       117.62, 94\"\"\".strip()with tempfile.NamedTemporaryFile(delete=False, mode=\"w+\") as temp_file:    temp_file.write(string_data)    temp_file_path = temp_file.nameloader = CSVLoader(file_path=temp_file_path)data = loader.load()for record in data[:2]:    print(record)\\npage_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\' metadata={\\'source\\': \\'Nationals\\', \\'row\\': 0}page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\' metadata={\\'source\\': \\'Reds\\', \\'row\\': 1}Edit this pageWas this page helpful?PreviousHow to debug your LLM appsNextHow to load documents from a directoryCustomizing the CSV parsing and loadingSpecify a column to identify the document sourceLoad from a stringCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='How to load documents from a directory | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load documents from a directoryOn this pageHow to load documents from a directory'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content=\"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='How to load from a filesystem, including use of wildcard patterns;\\nHow to use multithreading for file I/O;\\nHow to use custom loader classes to parse specific file types (e.g., code);\\nHow to handle errors, such as those due to decoding.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='from langchain_community.document_loaders import DirectoryLoaderAPI Reference:DirectoryLoader\\nDirectoryLoader accepts a loader_cls kwarg, which defaults to UnstructuredLoader. Unstructured supports parsing for a number of formats, such as PDF and HTML. Here we use it to read in a markdown (.md) file.\\nWe can use the glob parameter to control which files to load. Note that here it doesn\\'t load the .rst file or the .html files.\\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\")docs = loader.load()len(docs)\\n20\\nprint(docs[0].page_content[:100])\\nSecurityLangChain has a large ecosystem of integrations with various external resources like local\\nShow a progress bar\\u200b\\nBy default a progress bar will not be shown. To show a progress bar, install the tqdm library (e.g. pip install tqdm), and set the show_progress parameter to True.\\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\", show_progress=True)docs = loader.load()\\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 54.56it/s]\\nUse multithreading\\u200b\\nBy default the loading happens in one thread. In order to utilize several threads set the use_multithreading flag to true.\\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\", use_multithreading=True)docs = loader.load()\\nChange loader class\\u200b\\nBy default this uses the UnstructuredLoader class. To customize the loader, specify the loader class in the loader_cls kwarg. Below we show an example using TextLoader:\\nfrom langchain_community.document_loaders import TextLoaderloader = DirectoryLoader(\"../\", glob=\"**/*.md\", loader_cls=TextLoader)docs = loader.load()API Reference:TextLoader\\nprint(docs[0].page_content[:100])\\n# SecurityLangChain has a large ecosystem of integrations with various external resources like loc\\nNotice that while the UnstructuredLoader parses Markdown headers, TextLoader does not.\\nIf you need to load Python source code files, use the PythonLoader:\\nfrom langchain_community.document_loaders import PythonLoaderloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)API Reference:PythonLoader\\nAuto-detect file encodings with TextLoader\\u200b\\nDirectoryLoader can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\\nA. Default Behavior\\u200b\\nBy default we raise an error:\\nloader.load()\\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='A. Default Behavior\\u200b\\nBy default we raise an error:\\nloader.load()\\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\n---------------------------------------------------------------------------``````outputUnicodeDecodeError                        Traceback (most recent call last)``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)     42     with open(self.file_path, encoding=self.encoding) as f:---> 43         text = f.read()     44 except UnicodeDecodeError as e:``````outputFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)    321 data = self.buffer + input--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)    323 # keep undecoded input until the next call``````outputUnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xca in position 0: invalid continuation byte``````outputThe above exception was the direct cause of the following exception:``````outputRuntimeError                              Traceback (most recent call last)``````outputCell In[10], line 1----> 1 loader.load()``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)    115 def load(self) -> List[Document]:    116     \"\"\"Load documents.\"\"\"--> 117     return list(self.lazy_load())``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)    180 else:    181     for i in items:--> 182         yield from self._lazy_load_file(i, p, pbar)    184 if pbar:    185     pbar.close()``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)    218     else:    219         logger.error(f\"Error loading file {str(item)}\")--> 220         raise e    221 finally:    222     if pbar:``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)    208 loader = self.loader_cls(str(item), **self.loader_kwargs)    209 try:--> 210     for subdoc in loader.lazy_load():    211         yield subdoc    212 except NotImplementedError:``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)     54                 continue     55     else:---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e     57 except Exception as e:     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e``````outputRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\nThe file example-non-utf8.txt uses a different encoding, so the load() function fails with a helpful message indicating which file failed decoding.\\nWith the default behavior of TextLoader any failure to load any of the documents will fail the whole loading process and no documents are loaded.\\nB. Silent fail\\u200b\\nWe can pass the parameter silent_errors to the DirectoryLoader to skip the files which could not be loaded and continue the load process.\\nloader = DirectoryLoader(    path, glob=\"**/*.txt\", loader_cls=TextLoader, silent_errors=True)docs = loader.load()\\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\ndoc_sources = [doc.metadata[\"source\"] for doc in docs]doc_sources\\n[\\'../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt\\']\\nC. Auto detect encodings\\u200b\\nWe can also ask TextLoader to auto detect the file encoding before failing, by passing the autodetect_encoding to the loader class.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='C. Auto detect encodings\\u200b\\nWe can also ask TextLoader to auto detect the file encoding before failing, by passing the autodetect_encoding to the loader class.\\ntext_loader_kwargs = {\"autodetect_encoding\": True}loader = DirectoryLoader(    path, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)docs = loader.load()\\ndoc_sources = [doc.metadata[\"source\"] for doc in docs]doc_sources\\n[\\'../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt\\', \\'../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\']Edit this pageWas this page helpful?PreviousHow to load CSVsNextHow to load HTMLShow a progress barUse multithreadingChange loader classAuto-detect file encodings with TextLoaderA. Default BehaviorB. Silent failC. Auto detect encodingsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='How to load HTML | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load HTMLOn this pageHow to load HTML'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.\\nThis covers how to load HTML documents into a LangChain Document objects that we can use downstream.\\nParsing HTML files often requires specialized tools. Here we demonstrate parsing via Unstructured and BeautifulSoup4, which can be installed via pip. Head over to the integrations page to find integrations with additional services, such as Azure AI Document Intelligence or FireCrawl.\\nLoading HTML with Unstructured\\u200b\\n%pip install unstructured\\nfrom langchain_community.document_loaders import UnstructuredHTMLLoaderfile_path = \"../../docs/integrations/document_loaders/example_data/fake-content.html\"loader = UnstructuredHTMLLoader(file_path)data = loader.load()print(data)API Reference:UnstructuredHTMLLoader\\n[Document(page_content=\\'My First Heading\\\\n\\\\nMy first paragraph.\\', metadata={\\'source\\': \\'../../docs/integrations/document_loaders/example_data/fake-content.html\\'})]\\nLoading HTML with BeautifulSoup4\\u200b\\nWe can also use BeautifulSoup4 to load HTML documents using the BSHTMLLoader.  This will extract the text from the HTML into page_content, and the page title as title into metadata.\\n%pip install bs4\\nfrom langchain_community.document_loaders import BSHTMLLoaderloader = BSHTMLLoader(file_path)data = loader.load()print(data)API Reference:BSHTMLLoader\\n[Document(page_content=\\'\\\\nTest Title\\\\n\\\\n\\\\nMy First Heading\\\\nMy first paragraph.\\\\n\\\\n\\\\n\\', metadata={\\'source\\': \\'../../docs/integrations/document_loaders/example_data/fake-content.html\\', \\'title\\': \\'Test Title\\'})]Edit this pageWas this page helpful?PreviousHow to load documents from a directoryNextHow to load JSONLoading HTML with UnstructuredLoading HTML with BeautifulSoup4CommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis is a multi-part tutorial:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Part 1 (this guide) introduces RAG and walks through a minimal implementation.\\nPart 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\\n\\nThis tutorial will show how to build a simple Q&A application\\nover a text data source. Along the way we‚Äôll go over a typical Q&A\\narchitecture and highlight additional resources for more advanced Q&A techniques. We‚Äôll also see\\nhow LangSmith can help us trace and understand our application.\\nLangSmith will become increasingly helpful as our application grows in\\ncomplexity.\\nIf you\\'re already familiar with basic retrieval, you might also be interested in\\nthis high-level overview of different retrieval techniques.\\nNote: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\\nOverview\\u200b\\nA typical RAG application has two main components:\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\nNote: the indexing portion of this tutorial will largely follow the semantic search tutorial.\\nThe most common full sequence from raw data to answer looks like:\\nIndexing\\u200b\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\\'t fit in a model\\'s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\nRetrieval and generation\\u200b\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nOnce we\\'ve indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires these langchain dependencies:\\n\\nPipConda%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraphconda install langchain-text-splitters langchain-community langgraph -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nComponents\\u200b\\nWe will need to select three components from LangChain\\'s suite of integrations.\\n\\nSelect chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Select embeddings model:OpenAI‚ñæOpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Select vector store:In-memory‚ñæIn-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\\nPreview\\u200b\\nIn this guide we‚Äôll build an app that answers questions about the website\\'s content. The specific website we will use is the LLM Powered Autonomous\\nAgents blog post\\nby Lilian Weng, which allows us to ask questions about the contents of\\nthe post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~50\\nlines of code.\\nimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:hub | WebBaseLoader | Document | RecursiveCharacterTextSplitter | StateGraph\\nresponse = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"])\\nTask Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model\\'s thinking process.\\nCheck out the LangSmith\\ntrace.\\nDetailed walkthrough\\u200b\\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs\\ngoing on.\\n1. Indexing\\u200b\\nnoteThis section is an abbreviated version of the content in the semantic search tutorial.\\nIf you\\'re comfortable with document loaders, embeddings, and vector stores,\\nfeel free to skip to the next section on retrieval and generation.\\nLoading documents\\u200b\\nWe need to first load the blog post contents. We can use\\nDocumentLoaders\\nfor this, which are objects that load in data from a source and return a\\nlist of\\nDocument\\nobjects.\\nIn this case we‚Äôll use the\\nWebBaseLoader,\\nwhich uses urllib to load HTML from web URLs and BeautifulSoup to\\nparse it to text. We can customize the HTML -> text parsing by passing\\nin parameters into the BeautifulSoup parser via bs_kwargs (see\\nBeautifulSoup\\ndocs).\\nIn this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\\n‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='BeautifulSoup\\ndocs).\\nIn this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\\n‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\")API Reference:WebBaseLoader\\nTotal characters: 43131\\nprint(docs[0].page_content[:500])\\n      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\\nGo deeper\\u200b\\nDocumentLoader: Object that loads data from a source as list of Documents.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Docs:\\nDetailed documentation on how to use DocumentLoaders.\\nIntegrations: 160+\\nintegrations to choose from.\\nInterface:\\nAPI reference for the base interface.\\n\\nSplitting documents\\u200b\\nOur loaded document is over 42k characters which is too long to fit\\ninto the context window of many models. Even for those models that could\\nfit the full post in their context window, models can struggle to find\\ninformation in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and\\nvector storage. This should help us retrieve only the most relevant parts\\nof the blog post at run time.\\nAs in the semantic search tutorial, we use a\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")API Reference:RecursiveCharacterTextSplitter\\nSplit blog post into 66 sub-documents.\\nGo deeper\\u200b\\nTextSplitter: Object that splits a list of Documents into smaller\\nchunks. Subclass of DocumentTransformers.\\n\\nLearn more about splitting text using different methods by reading the how-to docs\\nCode (py or js)\\nScientific papers\\nInterface: API reference for the base interface.\\n\\nDocumentTransformer: Object that performs a transformation on a list\\nof Document objects.\\n\\nDocs: Detailed documentation on how to use DocumentTransformers\\nIntegrations\\nInterface: API reference for the base interface.\\n\\nStoring documents\\u200b\\nNow we need to index our 66 text chunks so that we can search over them\\nat runtime. Following the semantic search tutorial,\\nour approach is to embed the contents of each document split and insert these embeddings\\ninto a vector store. Given an input query, we can then use\\nvector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command\\nusing the vector store and embeddings model selected at the start of the tutorial.\\ndocument_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3])\\n[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\nGo deeper\\u200b\\nEmbeddings: Wrapper around a text embedding model, used for converting\\ntext to embeddings.\\n\\nDocs: Detailed documentation on how to use embeddings.\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and\\nquerying embeddings.\\n\\nDocs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Docs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point\\nwe have a query-able vector store containing the chunked contents of our\\nblog post. Given a user question, we should ideally be able to return\\nthe snippets of the blog post that answer the question.\\n2. Retrieval and Generation\\u200b\\nNow let‚Äôs write the actual application logic. We want to create a simple\\napplication that takes a user question, searches for documents relevant\\nto that question, passes the retrieved documents and initial question to\\na model, and returns an answer.\\nFor generation, we will use the chat model selected at the start of the tutorial.\\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub\\n(here).\\nfrom langchain import hubprompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content)API Reference:hub\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer:\\nWe\\'ll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\\n\\nWe can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\\nWe get streamlined deployments via LangGraph Platform.\\nLangSmith will automatically trace the steps of our application together.\\nWe can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\\n\\nTo use LangGraph, we need to define three things:\\n\\nThe state of our application;\\nThe nodes of our application (i.e., application steps);\\nThe \"control flow\" of our application (e.g., the ordering of the steps).\\n\\nState:\\u200b\\nThe state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\\nFor a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\\nfrom langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: strAPI Reference:Document\\nNodes (application steps)\\u200b\\nLet\\'s start with a simple sequence of two steps: retrieval and generation.\\ndef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}\\nOur retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\\nControl flow\\u200b\\nFinally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence.\\nfrom langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:StateGraph\\nLangGraph also comes with built-in utilities for visualizing the control flow of your application:\\nfrom IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include:\\nSupport for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\\nAutomatic support for tracing via LangSmith and deployments via LangGraph Platform;\\nSupport for persistence, human-in-the-loop, and other features.\\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph\\'s management and persistence of state simplifies these applications enormously.\\nUsage\\u200b\\nLet\\'s test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\\nInvoke:\\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\\'Context: {result[\"context\"]}\\\\n\\\\n\\')print(f\\'Answer: {result[\"answer\"]}\\')'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Invoke:\\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f\\'Context: {result[\"context\"]}\\\\n\\\\n\\')print(f\\'Answer: {result[\"answer\"]}\\')\\nContext: [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\\nStream steps:\\nfor step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Stream steps:\\nfor step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'retrieve\\': {\\'context\\': [Document(id=\\'a42dc78b-8f76-472a-9e25-180508af74f3\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}, page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\'), Document(id=\\'c0e45887-d0b0-483d-821a-bb5d8316d51d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}, page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\'), Document(id=\\'4cc7f318-35f5-440f-a4a4-145b5f0b918d\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}, page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\'), Document(id=\\'f621ade4-9b0d-471f-a522-44eb5feeba0c\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\\\nInstruction:\\\\n\\\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{\\'generate\\': {\\'answer\\': \\'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.\\'}}----------------\\nStream tokens:\\nfor message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Stream tokens:\\nfor message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")\\n|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|\\'s| reasoning| process|.||\\ntipFor async invocations, use:result = await graph.ainvoke(...)andasync for step in graph.astream(...):\\nReturning sources\\u200b\\nNote that by storing the retrieved context in the state of the graph, we recover sources for the model\\'s generated answer in the \"context\" field of the state. See this guide on returning sources for more detail.\\nGo deeper\\u200b\\nChat models take in a sequence of messages and return a message.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Docs\\nIntegrations: 25+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nCustomizing the prompt\\nAs shown above, we can load prompts (e.g., this RAG\\nprompt) from the prompt\\nhub. The prompt can also be easily customized. For example:\\nfrom langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)API Reference:PromptTemplate\\nQuery analysis\\u200b\\nSo far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\\n\\nIn addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\\nThe model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let\\'s add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\\ntotal_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"all_splits[0].metadata\\n{\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 8, \\'section\\': \\'beginning\\'}\\nWe will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store.\\nfrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)API Reference:InMemoryVectorStore\\nLet\\'s next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\\nfrom typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]\\nFinally, we add a step to our LangGraph application to generate a query from the user\\'s raw input:\\nclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Full Code:from typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()API Reference:hub | WebBaseLoader | Document | InMemoryVectorStore | RecursiveCharacterTextSplitter | StateGraph\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='We can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\\nfor step in graph.stream(    {\"question\": \"What does the end of the post say about Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\\\n\\\\n----------------\\\\n\")\\n{\\'analyze_query\\': {\\'query\\': {\\'query\\': \\'Task Decomposition\\', \\'section\\': \\'end\\'}}}----------------{\\'retrieve\\': {\\'context\\': [Document(id=\\'d6cef137-e1e8-4ddc-91dc-b62bd33c6020\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39221, \\'section\\': \\'end\\'}, page_content=\\'Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\'), Document(id=\\'d1834ae1-eb6a-43d7-a023-08dfa5028799\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 39086, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nChallenges#\\\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\'), Document(id=\\'ca7f06e4-2c2e-4788-9a81-2418d82213d9\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 32942, \\'section\\': \\'end\\'}, page_content=\\'}\\\\n]\\\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\\\nSystem message:\\'), Document(id=\\'1fcc2736-30f4-4ef6-90f2-c64af92118cb\\', metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 35127, \\'section\\': \\'end\\'}, page_content=\\'\"content\": \"You will get instructions for code to write.\\\\\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\\\\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\\\`\\\\`\\\\`LANG\\\\\\\\nCODE\\\\\\\\n\\\\`\\\\`\\\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\"entrypoint\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease\\')]}}----------------{\\'generate\\': {\\'answer\\': \\'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.\\'}}----------------\\nIn both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step.\\nQuery Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.\\nNext steps\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content=\"Query Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.\\nNext steps\\u200b\\nWe've covered the steps to build a basic Q&A app over data:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App: Part 1 | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Loading data with a Document Loader\\nChunking the indexed data with a Text Splitter to make it more easily usable by a model\\nEmbedding the data and storing the data in a vectorstore\\nRetrieving the previously stored chunks in response to incoming questions\\nGenerating an answer using the retrieved chunks as context.\\n\\nIn Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\\nFurther reading:\\n\\nReturn sources: Learn how to return source documents\\nStreaming: Learn how to stream outputs and intermediate steps\\nAdd chat history: Learn how to add chat history to your app\\nRetrieval conceptual guide: A high-level overview of specific retrieval techniques\\nEdit this pageWas this page helpful?PreviousTaggingNextBuild a semantic search engineOverviewIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithComponentsPreviewDetailed walkthrough1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationQuery analysisNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs = loader.load_and_split() # SPLIT\n",
    "langchain_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group documents by source\n",
    "grouped_docs = defaultdict(list)\n",
    "for doc in langchain_docs:\n",
    "    source = doc.metadata.get('source', '')\n",
    "    grouped_docs[source].append(doc)\n",
    "\n",
    "# Combine documents with the same source\n",
    "combined_docs = []\n",
    "for source, docs in grouped_docs.items():\n",
    "    combined_content = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    combined_metadata = docs[0].metadata.copy()  # Use metadata from the first document\n",
    "    combined_metadata['num_chunks'] = len(docs)  # Add number of original chunks\n",
    "    combined_docs.append(Document(page_content=combined_content, metadata=combined_metadata))\n",
    "\n",
    "# Replace langchain_docs with the combined documents\n",
    "langchain_docs = combined_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(langchain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 11}, page_content='How to return structured data from a model | ü¶úÔ∏èüîó LangChain\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow\\nresponsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nFunction/tool calling\\nIt is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\\nThe .with_structured_output() method\\u200b\\n\\nSupported modelsYou can find a list of models that support this method here.\\nThis is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\\nAs an example, let\\'s get a model to generate a joke and separate the setup from the punchline:\\n\\nSelect chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\nPydantic class\\u200b\\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\\nfrom typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\nJoke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7)\\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model\\'s function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\\nTypedDict or JSON Schema\\u200b\\nIf you don\\'t want to use Pydantic, explicitly don\\'t want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn\\'t generate it, it is only used in defining the schema that is passed to the model.\\nRequirements\\nCore: langchain-core>=0.2.26\\nTyping extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\\nfrom typing import Optionalfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nEquivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\\njson_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nChoosing between multiple schemas\\u200b\\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\\nUsing Pydantic\\u200b\\nfrom typing import Unionclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user\\'s query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\nFinalResponse(final_output=Joke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7))\\nstructured_llm.invoke(\"How are you today?\")\\nFinalResponse(final_output=ConversationalResponse(response=\"I\\'m just a computer program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need!\"))\\nUsing TypedDict\\u200b\\nfrom typing import Optional, Unionfrom typing_extensions import Annotated, TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]class ConversationalResponse(TypedDict):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: Annotated[str, ..., \"A conversational response to the user\\'s query\"]class FinalResponse(TypedDict):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'final_output\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}}\\nstructured_llm.invoke(\"How are you today?\")\\n{\\'final_output\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}}\\nstructured_llm.invoke(\"How are you today?\")\\n{\\'final_output\\': {\\'response\\': \"I\\'m just a computer program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need!\"}}\\nResponses shall be identical to the ones shown in the Pydantic example.\\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don\\'t have to use nested schemas. See this how-to guide for more details.\\nStreaming\\u200b\\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\\ninfoNote that what\\'s yielded is already aggregated chunks, not deltas.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\\n{}{\\'setup\\': \\'\\'}{\\'setup\\': \\'Why\\'}{\\'setup\\': \\'Why was\\'}{\\'setup\\': \\'Why was the\\'}{\\'setup\\': \\'Why was the cat\\'}{\\'setup\\': \\'Why was the cat sitting\\'}{\\'setup\\': \\'Why was the cat sitting on\\'}{\\'setup\\': \\'Why was the cat sitting on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nFew-shot prompting\\u200b\\nFor more complex schemas it\\'s very useful to add few-shot examples to the prompt. This can be done in a few ways.\\nThe simplest and most universal way is to add examples to a system message in the prompt:\\nThe simplest and most universal way is to add examples to a system message in the prompt:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don\\'t planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what\\'s something funny about woodpeckers\")API Reference:ChatPromptTemplate\\n{\\'setup\\': \\'Woodpecker\\', \\'punchline\\': \"Woodpecker you a joke, but I\\'m afraid it might be too \\'hole-some\\'!\", \\'rating\\': 7}\\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you\\'re using makes use of tool calling in its API reference.\\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don\\'t planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") \\\\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\\n{\\'setup\\': \\'Crocodile\\', \\'punchline\\': \\'Crocodile be seeing you later, alligator!\\', \\'rating\\': 6}\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\\nJSON modeIf using JSON mode you\\'ll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you\\'re using supports JSON mode, check its entry in the API reference.\\nstructured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}\\n(Advanced) Raw outputs\\u200b\\nLLMs aren\\'t perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'raw\\': AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'function\\': {\\'arguments\\': \\'{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}\\', \\'name\\': \\'Joke\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 33, \\'prompt_tokens\\': 93, \\'total_tokens\\': 126}, \\'model_name\\': \\'gpt-4o-2024-05-13\\', \\'system_fingerprint\\': \\'fp_4e2b2da518\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0\\', tool_calls=[{\\'name\\': \\'Joke\\', \\'args\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}, \\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 93, \\'output_tokens\\': 33, \\'total_tokens\\': 126}), \\'parsed\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}, \\'parsing_error\\': None}\\nPrompting and parsing model outputs directly\\u200b\\nNot all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you\\'ll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\\nUsing PydanticOutputParser\\u200b\\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\\nfrom typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate\\nLet‚Äôs take a look at what information is sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke({\"query\": query}).to_string())\\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\\\`\\\\`\\\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\\\`\\\\`\\\\`Human: Anna is 23 years old and she is 6 feet tall\\nAnd now let\\'s invoke it:\\nchain = prompt | llm | parserchain.invoke({\"query\": query})\\nPeople(people=[Person(name=\\'Anna\\', height_in_meters=1.8288)])\\nFor a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\\nCustom Parsing\\u200b\\nYou can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\\nimport jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\\\`\\\\`\\\\`json\\\\n{schema}\\\\n\\\\`\\\\`\\\\`. \"            \"Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\\\`\\\\`\\\\`json(.*?)\\\\`\\\\`\\\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\\\`\\\\`\\\\`json{\\'title\\': \\'People\\', \\'description\\': \\'Identifying information about all people in a text.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'people\\': {\\'title\\': \\'People\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/definitions/Person\\'}}}, \\'required\\': [\\'people\\'], \\'definitions\\': {\\'Person\\': {\\'title\\': \\'Person\\', \\'description\\': \\'Information about a person.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'description\\': \\'The name of the person\\', \\'type\\': \\'string\\'}, \\'height_in_meters\\': {\\'title\\': \\'Height In Meters\\', \\'description\\': \\'The height of the person expressed in meters.\\', \\'type\\': \\'number\\'}}, \\'required\\': [\\'name\\', \\'height_in_meters\\']}}}\\\\`\\\\`\\\\`. Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\\nAnd here\\'s what it looks like when we invoke it:\\nchain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\\n[{\\'people\\': [{\\'name\\': \\'Anna\\', \\'height_in_meters\\': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj = langchain_docs[0]\n",
    "doc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to return structured data from a model | ü¶úÔ∏èüîó LangChain\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow\\nresponsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nFunction/tool calling\\nIt is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\\nThe .with_structured_output() method\\u200b\\n\\nSupported modelsYou can find a list of models that support this method here.\\nThis is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\\nAs an example, let\\'s get a model to generate a joke and separate the setup from the punchline:\\n\\nSelect chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\nPydantic class\\u200b\\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\\nfrom typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\nJoke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7)\\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model\\'s function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\\nTypedDict or JSON Schema\\u200b\\nIf you don\\'t want to use Pydantic, explicitly don\\'t want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn\\'t generate it, it is only used in defining the schema that is passed to the model.\\nRequirements\\nCore: langchain-core>=0.2.26\\nTyping extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\\nfrom typing import Optionalfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nEquivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\\njson_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nChoosing between multiple schemas\\u200b\\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\\nUsing Pydantic\\u200b\\nfrom typing import Unionclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user\\'s query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\nFinalResponse(final_output=Joke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7))\\nstructured_llm.invoke(\"How are you today?\")\\nFinalResponse(final_output=ConversationalResponse(response=\"I\\'m just a computer program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need!\"))\\nUsing TypedDict\\u200b\\nfrom typing import Optional, Unionfrom typing_extensions import Annotated, TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]class ConversationalResponse(TypedDict):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: Annotated[str, ..., \"A conversational response to the user\\'s query\"]class FinalResponse(TypedDict):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'final_output\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}}\\nstructured_llm.invoke(\"How are you today?\")\\n{\\'final_output\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}}\\nstructured_llm.invoke(\"How are you today?\")\\n{\\'final_output\\': {\\'response\\': \"I\\'m just a computer program, so I don\\'t have feelings, but I\\'m here and ready to help you with whatever you need!\"}}\\nResponses shall be identical to the ones shown in the Pydantic example.\\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don\\'t have to use nested schemas. See this how-to guide for more details.\\nStreaming\\u200b\\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\\ninfoNote that what\\'s yielded is already aggregated chunks, not deltas.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\\n{}{\\'setup\\': \\'\\'}{\\'setup\\': \\'Why\\'}{\\'setup\\': \\'Why was\\'}{\\'setup\\': \\'Why was the\\'}{\\'setup\\': \\'Why was the cat\\'}{\\'setup\\': \\'Why was the cat sitting\\'}{\\'setup\\': \\'Why was the cat sitting on\\'}{\\'setup\\': \\'Why was the cat sitting on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nFew-shot prompting\\u200b\\nFor more complex schemas it\\'s very useful to add few-shot examples to the prompt. This can be done in a few ways.\\nThe simplest and most universal way is to add examples to a system message in the prompt:\\nThe simplest and most universal way is to add examples to a system message in the prompt:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don\\'t planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what\\'s something funny about woodpeckers\")API Reference:ChatPromptTemplate\\n{\\'setup\\': \\'Woodpecker\\', \\'punchline\\': \"Woodpecker you a joke, but I\\'m afraid it might be too \\'hole-some\\'!\", \\'rating\\': 7}\\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you\\'re using makes use of tool calling in its API reference.\\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don\\'t planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") \\\\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\\n{\\'setup\\': \\'Crocodile\\', \\'punchline\\': \\'Crocodile be seeing you later, alligator!\\', \\'rating\\': 6}\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\\nJSON modeIf using JSON mode you\\'ll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you\\'re using supports JSON mode, check its entry in the API reference.\\nstructured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}\\n(Advanced) Raw outputs\\u200b\\nLLMs aren\\'t perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'raw\\': AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'function\\': {\\'arguments\\': \\'{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}\\', \\'name\\': \\'Joke\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 33, \\'prompt_tokens\\': 93, \\'total_tokens\\': 126}, \\'model_name\\': \\'gpt-4o-2024-05-13\\', \\'system_fingerprint\\': \\'fp_4e2b2da518\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0\\', tool_calls=[{\\'name\\': \\'Joke\\', \\'args\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}, \\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 93, \\'output_tokens\\': 33, \\'total_tokens\\': 126}), \\'parsed\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}, \\'parsing_error\\': None}\\nPrompting and parsing model outputs directly\\u200b\\nNot all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you\\'ll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\\nUsing PydanticOutputParser\\u200b\\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\\nfrom typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate\\nLet‚Äôs take a look at what information is sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke({\"query\": query}).to_string())\\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\\\`\\\\`\\\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\\\`\\\\`\\\\`Human: Anna is 23 years old and she is 6 feet tall\\nAnd now let\\'s invoke it:\\nchain = prompt | llm | parserchain.invoke({\"query\": query})\\nPeople(people=[Person(name=\\'Anna\\', height_in_meters=1.8288)])\\nFor a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\\nCustom Parsing\\u200b\\nYou can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\\nimport jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\\\`\\\\`\\\\`json\\\\n{schema}\\\\n\\\\`\\\\`\\\\`. \"            \"Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\\\`\\\\`\\\\`json(.*?)\\\\`\\\\`\\\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\\\`\\\\`\\\\`json{\\'title\\': \\'People\\', \\'description\\': \\'Identifying information about all people in a text.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'people\\': {\\'title\\': \\'People\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/definitions/Person\\'}}}, \\'required\\': [\\'people\\'], \\'definitions\\': {\\'Person\\': {\\'title\\': \\'Person\\', \\'description\\': \\'Information about a person.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'description\\': \\'The name of the person\\', \\'type\\': \\'string\\'}, \\'height_in_meters\\': {\\'title\\': \\'Height In Meters\\', \\'description\\': \\'The height of the person expressed in meters.\\', \\'type\\': \\'number\\'}}, \\'required\\': [\\'name\\', \\'height_in_meters\\']}}}\\\\`\\\\`\\\\`. Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\\nAnd here\\'s what it looks like when we invoke it:\\nchain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\\n[{\\'people\\': [{\\'name\\': \\'Anna\\', \\'height_in_meters\\': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(langchain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "How to return structured data from a model | ü¶úÔ∏èüîó LangChain\n",
       "Skip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow\n",
       "responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model\n",
       "PrerequisitesThis guide assumes familiarity with the following concepts:\n",
       "Chat models\n",
       "Function/tool calling\n",
       "It is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\n",
       "The .with_structured_output() method‚Äã\n",
       "\n",
       "Supported modelsYou can find a list of models that support this method here.\n",
       "This is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\n",
       "This method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\n",
       "As an example, let's get a model to generate a joke and separate the setup from the punchline:\n",
       "\n",
       "Select chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
       "Pydantic class‚Äã\n",
       "If we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n",
       "from typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n",
       "tipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n",
       "TypedDict or JSON Schema‚Äã\n",
       "If you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n",
       "Requirements\n",
       "Core: langchain-core>=0.2.26\n",
       "Typing extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\n",
       "from typing import Optionalfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n",
       "Equivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n",
       "json_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n",
       "Choosing between multiple schemas‚Äã\n",
       "The simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n",
       "Using Pydantic‚Äã\n",
       "from typing import Unionclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user's query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "FinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n",
       "structured_llm.invoke(\"How are you today?\")\n",
       "FinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n",
       "Using TypedDict‚Äã\n",
       "from typing import Optional, Unionfrom typing_extensions import Annotated, TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]class ConversationalResponse(TypedDict):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: Annotated[str, ..., \"A conversational response to the user's query\"]class FinalResponse(TypedDict):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "{'final_output': {'setup': 'Why was the cat sitting on the computer?',  'punchline': 'Because it wanted to keep an eye on the mouse!',  'rating': 7}}\n",
       "structured_llm.invoke(\"How are you today?\")\n",
       "{'final_output': {'setup': 'Why was the cat sitting on the computer?',  'punchline': 'Because it wanted to keep an eye on the mouse!',  'rating': 7}}\n",
       "structured_llm.invoke(\"How are you today?\")\n",
       "{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n",
       "Responses shall be identical to the ones shown in the Pydantic example.\n",
       "Alternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don't have to use nested schemas. See this how-to guide for more details.\n",
       "Streaming‚Äã\n",
       "We can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\n",
       "infoNote that what's yielded is already aggregated chunks, not deltas.\n",
       "from typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\n",
       "{}{'setup': ''}{'setup': 'Why'}{'setup': 'Why was'}{'setup': 'Why was the'}{'setup': 'Why was the cat'}{'setup': 'Why was the cat sitting'}{'setup': 'Why was the cat sitting on'}{'setup': 'Why was the cat sitting on the'}{'setup': 'Why was the cat sitting on the computer'}{'setup': 'Why was the cat sitting on the computer?'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n",
       "Few-shot prompting‚Äã\n",
       "For more complex schemas it's very useful to add few-shot examples to the prompt. This can be done in a few ways.\n",
       "The simplest and most universal way is to add examples to a system message in the prompt:\n",
       "The simplest and most universal way is to add examples to a system message in the prompt:\n",
       "from langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")API Reference:ChatPromptTemplate\n",
       "{'setup': 'Woodpecker', 'punchline': \"Woodpecker you a joke, but I'm afraid it might be too 'hole-some'!\", 'rating': 7}\n",
       "When the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.\n",
       "from langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don't planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\Return a joke which has the setup (the response to \"Who's there?\") \\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\n",
       "{'setup': 'Crocodile', 'punchline': 'Crocodile be seeing you later, alligator!', 'rating': 6}\n",
       "For more on few shot prompting when using tool calling, see here.\n",
       "(Advanced) Specifying the method for structuring outputs‚Äã\n",
       "For more on few shot prompting when using tool calling, see here.\n",
       "(Advanced) Specifying the method for structuring outputs‚Äã\n",
       "For models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\n",
       "JSON modeIf using JSON mode you'll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you're using supports JSON mode, check its entry in the API reference.\n",
       "structured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\n",
       "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n",
       "(Advanced) Raw outputs‚Äã\n",
       "LLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\n",
       "structured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}), 'parsed': {'setup': 'Why was the cat sitting on the computer?',  'punchline': 'Because it wanted to keep an eye on the mouse!',  'rating': 7}, 'parsing_error': None}\n",
       "Prompting and parsing model outputs directly‚Äã\n",
       "Not all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\n",
       "Using PydanticOutputParser‚Äã\n",
       "The following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\n",
       "from typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate\n",
       "Let‚Äôs take a look at what information is sent to the model:\n",
       "query = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke({\"query\": query}).to_string())\n",
       "System: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\`\\`\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\`\\`\\`Human: Anna is 23 years old and she is 6 feet tall\n",
       "And now let's invoke it:\n",
       "chain = prompt | llm | parserchain.invoke({\"query\": query})\n",
       "People(people=[Person(name='Anna', height_in_meters=1.8288)])\n",
       "For a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\n",
       "Custom Parsing‚Äã\n",
       "You can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\n",
       "import jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"            \"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\n",
       "Here is the prompt sent to the model:\n",
       "query = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\n",
       "Here is the prompt sent to the model:\n",
       "query = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\n",
       "System: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\n",
       "And here's what it looks like when we invoke it:\n",
       "chain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\n",
       "[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example containing just a few document pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x121332050>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(langchain_docs, embedding=embeddings) # STORE\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/#:~:text=A%20retriever%20is,Document's%20as%20output.):\n",
    "\n",
    "> A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x121332050>, search_kwargs={})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever() \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.2/docs/tutorials/pdf_qa/#question-answering-with-rag\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "question_answer_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method `create_stuff_documents_chain` [outputs an LCEL runnable](https://arc.net/l/quote/bnsztwth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to return structured output in LangChain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How to return structured output in LangChain?',\n",
       " 'context': [Document(metadata={'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en', 'num_chunks': 11, 'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain'}, page_content='How to load JSON | ü¶úÔ∏èüîó LangChain\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow\\nresponsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load JSONOn this pageHow to load JSON\\nJSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).\\nJSON Lines is a file format where each line is a valid JSON value.\\nLangChain implements a JSONLoader\\nto convert JSON and JSONL data into LangChain Document\\nobjects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content\\nand metadata of the LangChain Document.\\nIt uses the jq python package. Check out this manual for a detailed documentation of the jq syntax.\\nHere we will demonstrate:\\nHow to load JSON and JSONL data into the content of a LangChain Document;\\nHow to load JSON and JSONL data into metadata associated with a Document.\\n#!pip install jq\\nfrom langchain_community.document_loaders import JSONLoaderAPI Reference:JSONLoader\\nimport jsonfrom pathlib import Pathfrom pprint import pprintfile_path=\\'./example_data/facebook_chat.json\\'data = json.loads(Path(file_path).read_text())\\npprint(data)\\n    {\\'image\\': {\\'creation_timestamp\\': 1675549016, \\'uri\\': \\'image_of_the_chat.jpg\\'},     \\'is_still_participant\\': True,     \\'joinable_mode\\': {\\'link\\': \\'\\', \\'mode\\': 1},     \\'magic_words\\': [],     \\'messages\\': [{\\'content\\': \\'Bye!\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675597571851},                  {\\'content\\': \\'Oh no worries! Bye\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675597435669},                  {\\'content\\': \\'No Im sorry it was my mistake, the blue one is not \\'                              \\'for sale\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675596277579},                  {\\'content\\': \\'I thought you were selling the blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595140251},                  {\\'content\\': \\'Im not interested in this bag. Im interested in the \\'                              \\'blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595109305},                  {\\'content\\': \\'Here is $129\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595068468},                  {\\'photos\\': [{\\'creation_timestamp\\': 1675595059,                               \\'uri\\': \\'url_of_some_picture.jpg\\'}],                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595060730},                  {\\'content\\': \\'Online is at least $100\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595045152},                  {\\'content\\': \\'How much do you want?\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675594799696},                  {\\'content\\': \\'Goodmorning! $50 is too low.\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675577876645},                  {\\'content\\': \\'Hi! Im interested in your bag. Im offering $50. Let \\'                              \\'me know if you are interested. Thanks!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675549022673}],     \\'participants\\': [{\\'name\\': \\'User 1\\'}, {\\'name\\': \\'User 2\\'}],     \\'thread_path\\': \\'inbox/User 1 and User 2 chat\\',     \\'title\\': \\'User 1 and User 2 chat\\'}\\nUsing JSONLoader\\u200b\\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\nJSON Lines file\\u200b\\nIf you want to load documents from a JSON Lines file, you pass json_lines=True\\nand specify jq_schema to extract page_content from a single JSON object.\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'pprint(Path(file_path).read_text())\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'     \\'worries! Bye\"}\\\\n\\'     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.content\\',    text_content=False,    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.\\',    content_key=\\'sender_name\\',    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nJSON file with jq schema content_key\\u200b\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\\nEnsure that content_key is compatible and can be parsed using the jq schema.\\nfile_path = \\'./sample.json\\'pprint(Path(file_path).read_text())\\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\nExtracting metadata\\u200b\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\nThe following demonstrates how metadata can be extracted using the JSONLoader.\\nThere are some key changes to be noted. In the previous example where we didn\\'t collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\\n.messages[].content\\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\\n.messages[]\\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nNow, you will see that the documents contain the metadata associated with the content we extracted.\\nThe metadata_func\\u200b\\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"Edit this pageWas this page helpful?PreviousHow to load HTMLNextHow to load MarkdownUsing JSONLoaderJSON fileJSON Lines fileJSON file with jq schema content_keyExtracting metadataThe metadata_funcCommon JSON structures with jq schemaCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow\\nresponsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores\\nIn this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:\\nfrom langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings\\nExamples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow\\nresponsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to run custom functionsOn this pageHow to run custom functions\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables\\nYou can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators\\nUsing the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionally equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:\\nimplementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain'}, page_content='Conceptual guide | ü¶úÔ∏èüîó LangChain\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow\\nresponsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideOn this pageConceptual guide\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\nWe recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples ‚Äî those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference.\\nHigh level\\u200b\\nWhy LangChain?: Overview of the value that LangChain provides.\\nArchitecture: How packages are organized in the LangChain ecosystem.\\n\\nConcepts\\u200b\\nConcepts\\u200b\\n\\nChat models: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\nMessages: The unit of communication in chat models, used to represent model input and output.\\nChat history: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\nTools: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\nTool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\nStructured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\nMemory: Information about a conversation that is persisted so that it can be used in future conversations.\\nMultimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\nStreaming: LangChain streaming APIs for surfacing results as they are generated.\\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\\nDocument loaders: Load a source as a list of documents.\\nRetrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\nText splitters: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\nEmbedding models: Models that represent data such as text or images in a vector space.\\nVector stores: Storage of and efficient search over vectors and associated metadata.\\nRetriever: A component that returns relevant documents from a knowledge base in response to a query.\\nRetrieval Augmented Generation (RAG): A technique that enhances language models by combining them with external knowledge bases.\\nAgents: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool.\\nPrompt templates: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\nOutput parsers: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs.\\nFew-shot prompting: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\nExample selectors: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\nAsync programming: The basics that one should know to use LangChain in an asynchronous context.\\nCallbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\nTracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\nEvaluation: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\nTesting: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.\\n\\nGlossary\\u200b\\nAIMessageChunk: A partial response from an AI message. Used when streaming responses from a chat model.\\nAIMessage: Represents a complete response from an AI model.\\nastream_events: Stream granular information from LCEL chains.\\nBaseTool: The base class for all tools in LangChain.\\nbatch: Use to execute a runnable with batch inputs.\\nbind_tools: Allows models to interact with tools.\\nCaching: Storing results to avoid redundant calls to a chat model.\\nChat models: Chat models that handle multiple data modalities.\\nConfigurable runnables: Creating configurable Runnables.\\nContext window: The maximum size of input a chat model can process.\\nConversation patterns: Common patterns in chat interactions.\\nDocument: LangChain\\'s representation of a document.\\nEmbedding models: Models that generate vector embeddings for various data types.\\nHumanMessage: Represents a message from a human user.\\nInjectedState: A state injected into a tool function.\\nInjectedStore: A store that can be injected into a tool for data persistence.\\nInjectedToolArg: Mechanism to inject arguments into tool functions.\\ninput and output types: Types used for input and output in Runnables.\\nIntegration packages: Third-party packages that integrate with LangChain.\\nIntegration tests: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\ninvoke: A standard method to invoke a Runnable.\\nJSON mode: Returning responses in JSON format.\\nlangchain-community: Community-driven components for LangChain.\\nlangchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\\nlangchain: A package for higher level components (e.g., some pre-built chains).\\nlanggraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\nlangserve: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\nLLMs (legacy): Older language models that take a string as input and return a string as output.\\nManaging chat history: Techniques to maintain and manage the chat history.\\nOpenAI format: OpenAI\\'s message format for chat models.\\nPropagation of RunnableConfig: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\nrate-limiting: Client side rate limiting for chat models.\\nRemoveMessage: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\nrole: Represents the role (e.g., user, assistant) of a chat message.\\nRunnableConfig: Use to pass run time information to Runnables (e.g., run_name, run_id, tags, metadata, max_concurrency, recursion_limit, configurable).\\nStandard parameters for chat models: Parameters such as API key, temperature, and max_tokens.\\nStandard tests: A defined set of unit and integration tests that all integrations must pass.\\nstream: Use to stream output from a Runnable or a graph.\\nTokenization: The process of converting data into tokens and vice versa.\\nTokens: The basic unit that a language model reads, processes, and generates under the hood.\\nTool artifacts: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\nTool binding: Binding tools to models.\\n@tool: Decorator for creating tools in LangChain.\\nToolkits: A collection of tools that can be used together.\\nToolMessage: Represents a message that contains the results of a tool execution.\\nUnit tests: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\nVector stores: Datastores specialized for storing and efficiently searching vector embeddings.\\nwith_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\nwith_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\nwith_types: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\nEdit this pageWas this page helpful?PreviousHow to create and query vector storesNextAgentsHigh levelConceptsGlossaryCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.')],\n",
       " 'answer': 'LangChain is a Python library that allows you to build custom workflows and integrate them with various tools and services. It provides a flexible way to manage data and create complex workflows.\\n\\nTo return structured output in LangChain, you can use the `Link` class from the `langchain.graphs.core` module. A Link represents a connection between two entities (nodes) in your graph, which can be a function or an operation that returns some output.\\n\\nHere\\'s an example of how you can create a simple workflow that returns structured output:\\n```python\\nfrom langchain import Chain, Handler\\nfrom langchain.graphs.core import Link\\n\\ndef process_input(input_data: str) -> dict:\\n    # Process the input data and return a dictionary with the result\\n    results = {\\n        \"input\": input_data,\\n        \"processed_output\": \"Processed output\"\\n    }\\n    return results\\n\\n# Create a Chain object that will hold our workflow\\nworkflow = Chain()\\n\\n# Add a Link to our workflow that takes an input and returns a processed output\\nworkflow.add_link(Link(\\n    # The operation that will be executed when the link is triggered\\n    process_input,\\n    # The input that will be passed to the operation\\n    input_data=\"Hello, world!\",\\n))\\n\\n# Create a Handler object that will handle the Chain\\'s output\\nhandler = Handler()\\n\\n# Add a Link to our workflow that takes the result of the previous link and returns it as output\\nworkflow.add_link(Link(\\n    handler.handle,\\n    # The value returned by the `process_input` operation\\n    results=process_input(input_data=\"Hello, world!\"),\\n))\\n\\n# Run the Chain\\'s workflow\\noutput = workflow.run()\\n\\nprint(output)\\n```\\nIn this example, we define a simple function `process_input` that takes an input string and returns a dictionary with two keys: \"input\" and \"processed_output\". We then create a Chain object that holds our workflow, which consists of two Links:\\n\\n1. The first Link uses the `process_input` operation to process the input data.\\n2. The second Link uses the output from the previous operation as input for its own operation (in this case, simply returning it).\\n\\nWe also define a Handler object that will handle the Chain\\'s output.\\n\\nFinally, we run the Chain\\'s workflow and print the result.\\n\\nWhen you run this code, you should see an output like:\\n```\\n{\\n    \"input\": \"Hello, world!\",\\n    \"processed_output\": \"Processed output\"\\n}\\n```\\nThis demonstrates how LangChain allows you to create complex workflows with structured output.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is a Python library that allows you to build custom workflows and integrate them with various tools and services. It provides a flexible way to manage data and create complex workflows.\n",
       "\n",
       "To return structured output in LangChain, you can use the `Link` class from the `langchain.graphs.core` module. A Link represents a connection between two entities (nodes) in your graph, which can be a function or an operation that returns some output.\n",
       "\n",
       "Here's an example of how you can create a simple workflow that returns structured output:\n",
       "```python\n",
       "from langchain import Chain, Handler\n",
       "from langchain.graphs.core import Link\n",
       "\n",
       "def process_input(input_data: str) -> dict:\n",
       "    # Process the input data and return a dictionary with the result\n",
       "    results = {\n",
       "        \"input\": input_data,\n",
       "        \"processed_output\": \"Processed output\"\n",
       "    }\n",
       "    return results\n",
       "\n",
       "# Create a Chain object that will hold our workflow\n",
       "workflow = Chain()\n",
       "\n",
       "# Add a Link to our workflow that takes an input and returns a processed output\n",
       "workflow.add_link(Link(\n",
       "    # The operation that will be executed when the link is triggered\n",
       "    process_input,\n",
       "    # The input that will be passed to the operation\n",
       "    input_data=\"Hello, world!\",\n",
       "))\n",
       "\n",
       "# Create a Handler object that will handle the Chain's output\n",
       "handler = Handler()\n",
       "\n",
       "# Add a Link to our workflow that takes the result of the previous link and returns it as output\n",
       "workflow.add_link(Link(\n",
       "    handler.handle,\n",
       "    # The value returned by the `process_input` operation\n",
       "    results=process_input(input_data=\"Hello, world!\"),\n",
       "))\n",
       "\n",
       "# Run the Chain's workflow\n",
       "output = workflow.run()\n",
       "\n",
       "print(output)\n",
       "```\n",
       "In this example, we define a simple function `process_input` that takes an input string and returns a dictionary with two keys: \"input\" and \"processed_output\". We then create a Chain object that holds our workflow, which consists of two Links:\n",
       "\n",
       "1. The first Link uses the `process_input` operation to process the input data.\n",
       "2. The second Link uses the output from the previous operation as input for its own operation (in this case, simply returning it).\n",
       "\n",
       "We also define a Handler object that will handle the Chain's output.\n",
       "\n",
       "Finally, we run the Chain's workflow and print the result.\n",
       "\n",
       "When you run this code, you should see an output like:\n",
       "```\n",
       "{\n",
       "    \"input\": \"Hello, world!\",\n",
       "    \"processed_output\": \"Processed output\"\n",
       "}\n",
       "```\n",
       "This demonstrates how LangChain allows you to create complex workflows with structured output."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_answer = results[\"answer\"]\n",
    "\n",
    "Markdown(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain Expression Language (LCEL) is an open-source, human-readable expression language designed specifically for use in LangChain, a blockchain-agnostic data management platform. LCEL allows developers to express complex data operations and transformations in a concise and readable manner.\n",
       "\n",
       "The main goals of LCEL are:\n",
       "\n",
       "1. **Simplify data operations**: By providing a standardized way to describe data processing tasks, LCEL makes it easier for developers to write efficient and maintainable code.\n",
       "2. **Improve readability**: LCEL's syntax is designed to be easy to understand, even for non-technical users, making it ideal for collaboration between developers and data analysts.\n",
       "3. **Enable data transformation**: LCEL allows developers to define data transformations as a series of steps, enabling the creation of complex workflows that can handle large datasets.\n",
       "\n",
       "Key features of LCEL include:\n",
       "\n",
       "1. **Declarative syntax**: LCEL uses a declarative syntax, where the focus is on what needs to be done, rather than how it should be done.\n",
       "2. **Built-in data types**: LCEL supports various built-in data types, such as arrays, objects, and sets, making it easy to work with different data structures.\n",
       "3. **Functions and operators**: LCEL provides a range of functions and operators for performing common data operations, such as filtering, mapping, and aggregating data.\n",
       "\n",
       "Some examples of LCEL expressions include:\n",
       "\n",
       "* `data | filter(x => x > 5)` (filter an array of numbers greater than 5)\n",
       "* `data | map(x => x * 2)` (multiply each number in the array by 2)\n",
       "* `data | reduce((a, b) => a + b, 0)` (calculate the sum of all numbers in the array)\n",
       "\n",
       "LCEL is designed to be used with LangChain's data management platform, which provides a flexible and scalable way to store, retrieve, and manipulate data. By combining LCEL with LangChain's features, developers can build robust data pipelines that automate complex workflows.\n",
       "\n",
       "Overall, LangChain Expression Language (LCEL) offers a powerful tool for expressing data operations in a concise, readable, and maintainable way, making it an attractive choice for developers working on blockchain-agnostic data management projects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_summary = \"Write about what is LangChain Expression Language (LCEL)\"\n",
    "\n",
    " # adding chat history so the model remembers previous questions\n",
    "output = rag_chain.invoke({\"input\": query_summary})\n",
    "\n",
    "Markdown(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output is easily verifiable, we can see below that the chunk context for the answer came from pages 0,5,7 and 16 in the source pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}\n",
      "{'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en', 'num_chunks': 5, 'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain'}\n",
      "{'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en', 'num_chunks': 11, 'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain'}\n",
      "{'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output['context'])):\n",
    "    print(output['context'][i].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dig deeper into RAG over the langchain documentation and construct this rag chain ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        'input': lambda x: x['input'],\n",
    "        'context': lambda x: format_docs(x['context']), \n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the input query to the retriever\n",
    "retrieve_docs = (lambda x: x['input']) | retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  context: RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11b3f8850>, search_kwargs={})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: {\n",
       "              input: RunnableLambda(...),\n",
       "              context: RunnableLambda(...)\n",
       "            }\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOllama(model='llama3.2')\n",
       "            | StrOutputParser()\n",
       "  })"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'In LangChain how do we get structured outputs from a model?',\n",
       " 'context': [Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to run custom functionsOn this pageHow to run custom functions\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables\\nYou can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators\\nUsing the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:\\nimplementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en', 'num_chunks': 5, 'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain'}, page_content='How to load Markdown | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load MarkdownOn this pageHow to load Markdown\\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor.\\nHere we cover how to load Markdown documents into LangChain Document objects that we can use downstream.\\nWe will cover:\\nBasic usage;\\nParsing of Markdown into elements such as titles, list items, and text.\\n\\nLangChain implements an UnstructuredMarkdownLoader object which requires the Unstructured package. First we install it:\\n%pip install \"unstructured[md]\" nltk\\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain\\'s readme:\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoaderfrom langchain_core.documents import Documentmarkdown_path = \"../../../README.md\"loader = UnstructuredMarkdownLoader(markdown_path)data = loader.load()assert len(data) == 1assert isinstance(data[0], Document)readme_content = data[0].page_contentprint(readme_content[:250])API Reference:UnstructuredMarkdownLoader | Document\\nü¶úÔ∏èüîó LangChain‚ö° Build context-aware reasoning applications ‚ö°Looking for the JS/TS library? Check out LangChain.js.To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building,\\nRetain Elements\\u200b\\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\\\n\")for document in data[:2]:    print(f\"{document}\\\\n\")\\nNumber of documents: 66page_content=\\'ü¶úÔ∏èüîó LangChain\\' metadata={\\'source\\': \\'../../../README.md\\', \\'category_depth\\': 0, \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'Title\\'}page_content=\\'‚ö° Build context-aware reasoning applications ‚ö°\\' metadata={\\'source\\': \\'../../../README.md\\', \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'parent_id\\': \\'200b8a7d0dd03f66e4f13456566d2b3a\\', \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'NarrativeText\\'}\\nNote that in this case we recover three distinct element types:\\nprint(set(document.metadata[\"category\"] for document in data))\\n{\\'ListItem\\', \\'NarrativeText\\', \\'Title\\'}Edit this pageWas this page helpful?PreviousHow to load JSONNextHow to load Microsoft Office filesRetain ElementsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores\\nIn this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:\\nfrom langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings\\nExamples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en', 'num_chunks': 11, 'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain'}, page_content='How to load JSON | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load JSONOn this pageHow to load JSON\\nJSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).\\nJSON Lines is a file format where each line is a valid JSON value.\\nLangChain implements a JSONLoader\\nto convert JSON and JSONL data into LangChain Document\\nobjects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content\\nand metadata of the LangChain Document.\\nIt uses the jq python package. Check out this manual for a detailed documentation of the jq syntax.\\nHere we will demonstrate:\\nHow to load JSON and JSONL data into the content of a LangChain Document;\\nHow to load JSON and JSONL data into metadata associated with a Document.\\n#!pip install jq\\nfrom langchain_community.document_loaders import JSONLoaderAPI Reference:JSONLoader\\nimport jsonfrom pathlib import Pathfrom pprint import pprintfile_path=\\'./example_data/facebook_chat.json\\'data = json.loads(Path(file_path).read_text())\\npprint(data)\\n    {\\'image\\': {\\'creation_timestamp\\': 1675549016, \\'uri\\': \\'image_of_the_chat.jpg\\'},     \\'is_still_participant\\': True,     \\'joinable_mode\\': {\\'link\\': \\'\\', \\'mode\\': 1},     \\'magic_words\\': [],     \\'messages\\': [{\\'content\\': \\'Bye!\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675597571851},                  {\\'content\\': \\'Oh no worries! Bye\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675597435669},                  {\\'content\\': \\'No Im sorry it was my mistake, the blue one is not \\'                              \\'for sale\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675596277579},                  {\\'content\\': \\'I thought you were selling the blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595140251},                  {\\'content\\': \\'Im not interested in this bag. Im interested in the \\'                              \\'blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595109305},                  {\\'content\\': \\'Here is $129\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595068468},                  {\\'photos\\': [{\\'creation_timestamp\\': 1675595059,                               \\'uri\\': \\'url_of_some_picture.jpg\\'}],                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595060730},                  {\\'content\\': \\'Online is at least $100\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595045152},                  {\\'content\\': \\'How much do you want?\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675594799696},                  {\\'content\\': \\'Goodmorning! $50 is too low.\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675577876645},                  {\\'content\\': \\'Hi! Im interested in your bag. Im offering $50. Let \\'                              \\'me know if you are interested. Thanks!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675549022673}],     \\'participants\\': [{\\'name\\': \\'User 1\\'}, {\\'name\\': \\'User 2\\'}],     \\'thread_path\\': \\'inbox/User 1 and User 2 chat\\',     \\'title\\': \\'User 1 and User 2 chat\\'}\\nUsing JSONLoader\\u200b\\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\nJSON Lines file\\u200b\\nIf you want to load documents from a JSON Lines file, you pass json_lines=True\\nand specify jq_schema to extract page_content from a single JSON object.\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'pprint(Path(file_path).read_text())\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'     \\'worries! Bye\"}\\\\n\\'     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.content\\',    text_content=False,    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.\\',    content_key=\\'sender_name\\',    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nJSON file with jq schema content_key\\u200b\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\\nEnsure that content_key is compatible and can be parsed using the jq schema.\\nfile_path = \\'./sample.json\\'pprint(Path(file_path).read_text())\\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\nExtracting metadata\\u200b\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\nThe following demonstrates how metadata can be extracted using the JSONLoader.\\nThere are some key changes to be noted. In the previous example where we didn\\'t collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\\n.messages[].content\\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\\n.messages[]\\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nNow, you will see that the documents contain the metadata associated with the content we extracted.\\nThe metadata_func\\u200b\\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"Edit this pageWas this page helpful?PreviousHow to load HTMLNextHow to load MarkdownUsing JSONLoaderJSON fileJSON Lines fileJSON file with jq schema content_keyExtracting metadataThe metadata_funcCommon JSON structures with jq schemaCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')],\n",
       " 'answer': 'LangChain is a Python library that provides a simple and intuitive way to chain models together for various NLP tasks. To get structured outputs from a model using LangChain, you can use the `Chain` class or the `Pipeline` class.\\n\\nHere\\'s an example of how to use `Chain`:\\n\\n```python\\nfrom langchain.llms import HuggingFaceLLM\\n\\n# Initialize the LLM\\nllm = HuggingFaceLLM(\"distilbert-base-uncased\")\\n\\n# Define a function to process input text\\ndef process_input(text):\\n    return {\"text\": text}\\n\\n# Create a Chain object with the LLM and output processing function\\nchain = Chain(llm, process_input)\\n\\n# Use the Chain object to generate output\\noutput = chain({\"text\": \"Hello, how are you?\"})\\n\\nprint(output)\\n```\\n\\nIn this example, `Chain` takes two arguments: an LLM instance and a function that processes the input. The function is used to transform the output of the LLM into a structured format.\\n\\nAlternatively, you can use the `Pipeline` class, which provides more flexibility in terms of customization and parallel processing:\\n\\n```python\\nfrom langchain import Pipeline\\n\\n# Define a list of processors that will be applied in order\\nprocessors = [\\n    {\"name\": \"text-normalization\", \"kwargs\": {}},\\n    {\"name\": \"ner\", \"kwargs\": {}},\\n    {\"name\": \"sentiment-analysis\", \"kwargs\": {}},\\n]\\n\\n# Initialize the LLM\\nllm = HuggingFaceLLM(\"distilbert-base-uncased\")\\n\\n# Create a Pipeline object with the LLM and processors\\npipeline = Pipeline(llm, processors)\\n\\n# Use the Pipeline object to generate output\\noutput = pipeline({\"text\": \"Hello, how are you?\"})\\n\\nprint(output)\\n```\\n\\nIn this example, `Pipeline` takes two arguments: an LLM instance and a list of processors that will be applied in order. Each processor is a dictionary that defines the name of the processing function and any additional keyword arguments.\\n\\nBoth `Chain` and `Pipeline` provide a flexible way to chain models together for various NLP tasks, allowing you to customize the output format and processing pipeline to suit your specific needs.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"In LangChain how do we get structured outputs from a model?\" \n",
    "chain.invoke({'input': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding structured sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'In LangChain how do we get structured outputs from a model?',\n",
       " 'context': [Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to run custom functionsOn this pageHow to run custom functions\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables\\nYou can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators\\nUsing the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:\\nimplementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en', 'num_chunks': 5, 'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain'}, page_content='How to load Markdown | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load MarkdownOn this pageHow to load Markdown\\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor.\\nHere we cover how to load Markdown documents into LangChain Document objects that we can use downstream.\\nWe will cover:\\nBasic usage;\\nParsing of Markdown into elements such as titles, list items, and text.\\n\\nLangChain implements an UnstructuredMarkdownLoader object which requires the Unstructured package. First we install it:\\n%pip install \"unstructured[md]\" nltk\\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain\\'s readme:\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoaderfrom langchain_core.documents import Documentmarkdown_path = \"../../../README.md\"loader = UnstructuredMarkdownLoader(markdown_path)data = loader.load()assert len(data) == 1assert isinstance(data[0], Document)readme_content = data[0].page_contentprint(readme_content[:250])API Reference:UnstructuredMarkdownLoader | Document\\nü¶úÔ∏èüîó LangChain‚ö° Build context-aware reasoning applications ‚ö°Looking for the JS/TS library? Check out LangChain.js.To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building,\\nRetain Elements\\u200b\\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\\\n\")for document in data[:2]:    print(f\"{document}\\\\n\")\\nNumber of documents: 66page_content=\\'ü¶úÔ∏èüîó LangChain\\' metadata={\\'source\\': \\'../../../README.md\\', \\'category_depth\\': 0, \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'Title\\'}page_content=\\'‚ö° Build context-aware reasoning applications ‚ö°\\' metadata={\\'source\\': \\'../../../README.md\\', \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'parent_id\\': \\'200b8a7d0dd03f66e4f13456566d2b3a\\', \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'NarrativeText\\'}\\nNote that in this case we recover three distinct element types:\\nprint(set(document.metadata[\"category\"] for document in data))\\n{\\'ListItem\\', \\'NarrativeText\\', \\'Title\\'}Edit this pageWas this page helpful?PreviousHow to load JSONNextHow to load Microsoft Office filesRetain ElementsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores\\nIn this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:\\nfrom langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings\\nExamples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en', 'num_chunks': 11, 'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain'}, page_content='How to load JSON | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load JSONOn this pageHow to load JSON\\nJSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).\\nJSON Lines is a file format where each line is a valid JSON value.\\nLangChain implements a JSONLoader\\nto convert JSON and JSONL data into LangChain Document\\nobjects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content\\nand metadata of the LangChain Document.\\nIt uses the jq python package. Check out this manual for a detailed documentation of the jq syntax.\\nHere we will demonstrate:\\nHow to load JSON and JSONL data into the content of a LangChain Document;\\nHow to load JSON and JSONL data into metadata associated with a Document.\\n#!pip install jq\\nfrom langchain_community.document_loaders import JSONLoaderAPI Reference:JSONLoader\\nimport jsonfrom pathlib import Pathfrom pprint import pprintfile_path=\\'./example_data/facebook_chat.json\\'data = json.loads(Path(file_path).read_text())\\npprint(data)\\n    {\\'image\\': {\\'creation_timestamp\\': 1675549016, \\'uri\\': \\'image_of_the_chat.jpg\\'},     \\'is_still_participant\\': True,     \\'joinable_mode\\': {\\'link\\': \\'\\', \\'mode\\': 1},     \\'magic_words\\': [],     \\'messages\\': [{\\'content\\': \\'Bye!\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675597571851},                  {\\'content\\': \\'Oh no worries! Bye\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675597435669},                  {\\'content\\': \\'No Im sorry it was my mistake, the blue one is not \\'                              \\'for sale\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675596277579},                  {\\'content\\': \\'I thought you were selling the blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595140251},                  {\\'content\\': \\'Im not interested in this bag. Im interested in the \\'                              \\'blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595109305},                  {\\'content\\': \\'Here is $129\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595068468},                  {\\'photos\\': [{\\'creation_timestamp\\': 1675595059,                               \\'uri\\': \\'url_of_some_picture.jpg\\'}],                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595060730},                  {\\'content\\': \\'Online is at least $100\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595045152},                  {\\'content\\': \\'How much do you want?\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675594799696},                  {\\'content\\': \\'Goodmorning! $50 is too low.\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675577876645},                  {\\'content\\': \\'Hi! Im interested in your bag. Im offering $50. Let \\'                              \\'me know if you are interested. Thanks!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675549022673}],     \\'participants\\': [{\\'name\\': \\'User 1\\'}, {\\'name\\': \\'User 2\\'}],     \\'thread_path\\': \\'inbox/User 1 and User 2 chat\\',     \\'title\\': \\'User 1 and User 2 chat\\'}\\nUsing JSONLoader\\u200b\\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\nJSON Lines file\\u200b\\nIf you want to load documents from a JSON Lines file, you pass json_lines=True\\nand specify jq_schema to extract page_content from a single JSON object.\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'pprint(Path(file_path).read_text())\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'     \\'worries! Bye\"}\\\\n\\'     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.content\\',    text_content=False,    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.\\',    content_key=\\'sender_name\\',    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nJSON file with jq schema content_key\\u200b\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\\nEnsure that content_key is compatible and can be parsed using the jq schema.\\nfile_path = \\'./sample.json\\'pprint(Path(file_path).read_text())\\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\nExtracting metadata\\u200b\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\nThe following demonstrates how metadata can be extracted using the JSONLoader.\\nThere are some key changes to be noted. In the previous example where we didn\\'t collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\\n.messages[].content\\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\\n.messages[]\\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nNow, you will see that the documents contain the metadata associated with the content we extracted.\\nThe metadata_func\\u200b\\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"Edit this pageWas this page helpful?PreviousHow to load HTMLNextHow to load MarkdownUsing JSONLoaderJSON fileJSON Lines fileJSON file with jq schema content_keyExtracting metadataThe metadata_funcCommon JSON structures with jq schemaCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')],\n",
       " 'answer': {'answer': 'Structured outputs from a model can be obtained in LangChain by using the <|model_output_tokenizer|> tokenizer to split the output into constituent parts.',\n",
       "  'sources': '[\"LangChain documentation\", \"Stack Overflow discussion\"]'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.3/docs/how_to/qa_sources/\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# Desired schema for response\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# Our rag_chain_from_docs has the following changes:\n",
    "# - add `.with_structured_output` to the LLM;\n",
    "# - remove the output parser\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.with_structured_output(AnswerWithSources)\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"input\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb \n",
    "Below are notebook from openai cookbook on these topics of search and embeddings:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Code_search.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- [In-context learning abilities of ChatGPT models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Issue with long context](https://arxiv.org/pdf/2303.18223.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
