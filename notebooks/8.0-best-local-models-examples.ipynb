{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Local LLMs in Practice (2025 Edition)\n",
    "\n",
    "This notebook demonstrates how to use the top-performing open-source language models that can run locally with <64GB RAM. We'll explore models beyond Llama including Qwen, DeepSeek, Mixtral, and others.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Install Ollama: https://ollama.ai/\n",
    "- At least 16GB RAM (32GB+ recommended)\n",
    "- GPU with 8GB+ VRAM (optional but recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Qwen2.5 - The Multilingual Powerhouse\n",
    "\n",
    "Qwen2.5 by Alibaba is currently one of the best open-source models, excelling in:\n",
    "- Multilingual capabilities (25+ languages)\n",
    "- Code generation\n",
    "- Mathematical reasoning\n",
    "- Long context understanding (128K tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install ollama requests transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# First, pull the Qwen2.5 model (this may take a while for first download)\n",
    "# ollama pull qwen2.5:32b  # Run this in terminal first\n",
    "\n",
    "def chat_with_qwen(prompt, model=\"qwen2.5:14b\"):\n",
    "    \"\"\"Chat with Qwen2.5 model via Ollama\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}. Make sure to run 'ollama pull {model}' first.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Qwen2.5's multilingual capabilities\n",
    "multilingual_prompt = \"\"\"\n",
    "Translate the following text to Chinese, Spanish, and French:\n",
    "\"Machine learning is revolutionizing how we process and understand data.\"\n",
    "\n",
    "Then, write a brief explanation in each language about why machine learning is important.\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_qwen(multilingual_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Qwen2.5's coding capabilities\n",
    "coding_prompt = \"\"\"\n",
    "Write a Python class that implements a LRU (Least Recently Used) cache with the following requirements:\n",
    "1. Set maximum capacity\n",
    "2. get(key) - returns value if exists, -1 otherwise\n",
    "3. put(key, value) - adds or updates key-value pair\n",
    "4. When capacity exceeded, remove least recently used item\n",
    "\n",
    "Include comprehensive docstrings and example usage.\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_qwen(coding_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Qwen2.5's mathematical reasoning\n",
    "math_prompt = \"\"\"\n",
    "Solve this step-by-step:\n",
    "\n",
    "A water tank has a capacity of 1000 liters. It's being filled by two pipes:\n",
    "- Pipe A fills at 25 liters per minute\n",
    "- Pipe B fills at 15 liters per minute\n",
    "\n",
    "At the same time, there's a drain that empties at 8 liters per minute.\n",
    "\n",
    "If the tank starts empty and all pipes/drain operate simultaneously:\n",
    "1. What's the net filling rate?\n",
    "2. How long will it take to fill the tank completely?\n",
    "3. If Pipe A stops working after 20 minutes, how much longer will it take to fill the tank?\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_qwen(math_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeepSeek-Coder - The Programming Specialist\n",
    "\n",
    "DeepSeek-Coder is specialized for programming tasks with exceptional performance in:\n",
    "- Code generation and completion\n",
    "- Code explanation and debugging\n",
    "- Algorithm implementation\n",
    "- Multiple programming languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_deepseek_coder(prompt, model=\"deepseek-coder-v2:16b\"):\n",
    "    \"\"\"Chat with DeepSeek-Coder model via Ollama\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}. Make sure to run 'ollama pull {model}' first.\"\n",
    "\n",
    "# Advanced algorithm implementation\n",
    "algorithm_prompt = \"\"\"\n",
    "Implement a solution for the \"Sliding Window Maximum\" problem:\n",
    "\n",
    "Given an array of integers and a window size k, find the maximum element in each sliding window.\n",
    "\n",
    "Example:\n",
    "Input: nums = [1,3,-1,-3,5,3,6,7], k = 3\n",
    "Output: [3,3,5,5,6,7]\n",
    "\n",
    "Requirements:\n",
    "1. Optimal time complexity (O(n))\n",
    "2. Use a deque data structure\n",
    "3. Include detailed comments explaining the algorithm\n",
    "4. Add test cases\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_deepseek_coder(algorithm_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code debugging and optimization\n",
    "debug_prompt = \"\"\"\n",
    "Debug and optimize this Python code that's supposed to find all prime numbers up to n using the Sieve of Eratosthenes:\n",
    "\n",
    "```python\n",
    "def sieve_of_eratosthenes(n):\n",
    "    primes = []\n",
    "    for num in range(2, n+1):\n",
    "        is_prime = True\n",
    "        for i in range(2, int(num**0.5)+1):\n",
    "            if num % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            primes.append(num)\n",
    "    return primes\n",
    "```\n",
    "\n",
    "Issues:\n",
    "1. This isn't actually implementing the Sieve of Eratosthenes algorithm\n",
    "2. It's inefficient for large values of n\n",
    "3. Performance could be much better\n",
    "\n",
    "Please provide:\n",
    "1. Corrected implementation of true Sieve of Eratosthenes\n",
    "2. Explanation of what was wrong\n",
    "3. Performance comparison\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_deepseek_coder(debug_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-language code generation\n",
    "multilang_prompt = \"\"\"\n",
    "Implement a binary search tree (BST) with insert, search, and delete operations in three languages:\n",
    "1. Python (with type hints)\n",
    "2. JavaScript (ES6+)\n",
    "3. Rust\n",
    "\n",
    "For each implementation, include:\n",
    "- Proper error handling\n",
    "- Unit tests\n",
    "- Performance comments\n",
    "\n",
    "Keep the implementations clean and production-ready.\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_deepseek_coder(multilang_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mixtral 8x22B - The Efficient Specialist\n",
    "\n",
    "Mixtral uses Mixture of Experts (MoE) architecture for efficient scaling:\n",
    "- Only 2 out of 8 experts active per token\n",
    "- Excellent multilingual capabilities\n",
    "- Strong instruction following\n",
    "- Efficient inference despite large parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mixtral(prompt, model=\"mixtral:8x22b\"):\n",
    "    \"\"\"Chat with Mixtral model via Ollama\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}. Make sure to run 'ollama pull {model}' first.\"\n",
    "\n",
    "# Complex reasoning task\n",
    "reasoning_prompt = \"\"\"\n",
    "You're a senior consultant helping a tech startup make a critical decision. Here's the scenario:\n",
    "\n",
    "**Company**: AI-powered healthcare analytics startup\n",
    "**Team**: 25 employees, $5M in funding\n",
    "**Challenge**: Choose between two strategic paths:\n",
    "\n",
    "**Option A**: Focus on hospital systems\n",
    "- Larger contracts ($500K-2M each)\n",
    "- Longer sales cycles (12-18 months)\n",
    "- Regulatory compliance requirements\n",
    "- 3-4 major competitors\n",
    "\n",
    "**Option B**: Focus on individual clinics  \n",
    "- Smaller contracts ($10K-50K each)\n",
    "- Shorter sales cycles (2-3 months)\n",
    "- Less regulatory burden\n",
    "- Many smaller competitors\n",
    "\n",
    "**Constraints**:\n",
    "- Current runway: 18 months\n",
    "- Technical team can only focus on one path initially\n",
    "- Market penetration needed within 12 months\n",
    "\n",
    "Provide a detailed strategic analysis with:\n",
    "1. Risk assessment for each option\n",
    "2. Financial projections\n",
    "3. Resource allocation recommendations\n",
    "4. Final recommendation with reasoning\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_mixtral(reasoning_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative and technical writing\n",
    "writing_prompt = \"\"\"\n",
    "Write a technical blog post (800-1000 words) titled:\n",
    "\"Why Mixture of Experts (MoE) Models Are Revolutionizing AI Efficiency\"\n",
    "\n",
    "Target audience: Software engineers and AI researchers\n",
    "\n",
    "Include:\n",
    "1. Clear explanation of MoE architecture\n",
    "2. Comparison with traditional dense models\n",
    "3. Real-world examples and benefits\n",
    "4. Implementation considerations\n",
    "5. Future implications\n",
    "\n",
    "Style: Technical but accessible, with concrete examples\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_mixtral(writing_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison - Same Task, Different Strengths\n",
    "\n",
    "Let's give the same complex task to different models to see their unique approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_prompt = \"\"\"\n",
    "Design a distributed system for real-time fraud detection that can handle:\n",
    "- 100,000 transactions per second\n",
    "- Sub-100ms latency requirement\n",
    "- 99.99% availability\n",
    "- Global deployment\n",
    "\n",
    "Provide:\n",
    "1. High-level architecture diagram (in text/ASCII)\n",
    "2. Technology stack recommendations\n",
    "3. Scaling strategy\n",
    "4. Key challenges and solutions\n",
    "\n",
    "Keep it practical and implementable.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== QWEN2.5 RESPONSE ===\")\n",
    "qwen_response = chat_with_qwen(comparison_prompt)\n",
    "display(Markdown(qwen_response))\n",
    "\n",
    "print(\"\\n\\n=== DEEPSEEK-CODER RESPONSE ===\")\n",
    "deepseek_response = chat_with_deepseek_coder(comparison_prompt)\n",
    "display(Markdown(deepseek_response))\n",
    "\n",
    "print(\"\\n\\n=== MIXTRAL RESPONSE ===\")\n",
    "mixtral_response = chat_with_mixtral(comparison_prompt)\n",
    "display(Markdown(mixtral_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Monitoring and Resource Usage\n",
    "\n",
    "Let's monitor the performance characteristics of different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def benchmark_model(model_name, prompt, chat_function):\n",
    "    \"\"\"Benchmark model performance\"\"\"\n",
    "    # Get initial memory usage\n",
    "    initial_memory = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "    \n",
    "    # Time the response\n",
    "    start_time = time.time()\n",
    "    response = chat_function(prompt)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Get final memory usage\n",
    "    final_memory = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "    \n",
    "    # Calculate metrics\n",
    "    response_time = end_time - start_time\n",
    "    memory_used = final_memory - initial_memory\n",
    "    tokens_estimated = len(response.split())  # Rough estimate\n",
    "    tokens_per_second = tokens_estimated / response_time if response_time > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'response_time': response_time,\n",
    "        'memory_used': memory_used,\n",
    "        'tokens_estimated': tokens_estimated,\n",
    "        'tokens_per_second': tokens_per_second,\n",
    "        'response_length': len(response)\n",
    "    }\n",
    "\n",
    "# Simple benchmark prompt\n",
    "benchmark_prompt = \"Explain the concept of machine learning in exactly 100 words.\"\n",
    "\n",
    "# Benchmark different models (uncomment as needed)\n",
    "results = []\n",
    "\n",
    "# results.append(benchmark_model(\"Qwen2.5-14B\", benchmark_prompt, chat_with_qwen))\n",
    "# results.append(benchmark_model(\"DeepSeek-Coder-16B\", benchmark_prompt, chat_with_deepseek_coder))\n",
    "# results.append(benchmark_model(\"Mixtral-8x22B\", benchmark_prompt, chat_with_mixtral))\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    for result in results:\n",
    "        print(f\"\\n{result['model']} Performance:\")\n",
    "        print(f\"  Response Time: {result['response_time']:.2f}s\")\n",
    "        print(f\"  Estimated Tokens/sec: {result['tokens_per_second']:.1f}\")\n",
    "        print(f\"  Response Length: {result['response_length']} chars\")\n",
    "else:\n",
    "    print(\"Uncomment the benchmark lines above to run performance tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Use Cases\n",
    "\n",
    "### Tool Calling and Function Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using models for tool calling/function generation\n",
    "tool_calling_prompt = \"\"\"\n",
    "Create a Python class that acts as a \"Smart Assistant\" with the following capabilities:\n",
    "\n",
    "1. Weather lookup (mock API call)\n",
    "2. Calendar management (add/remove events)\n",
    "3. Email composition\n",
    "4. Web search (mock implementation)\n",
    "5. File operations (create, read, update files)\n",
    "\n",
    "The class should:\n",
    "- Have a unified interface for natural language commands\n",
    "- Parse user intent and route to appropriate functions\n",
    "- Handle errors gracefully\n",
    "- Return structured responses\n",
    "\n",
    "Include example usage showing how to:\n",
    "- \"What's the weather in New York?\"\n",
    "- \"Schedule a meeting for tomorrow at 2 PM\"\n",
    "- \"Write an email to John about the project update\"\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_qwen(tool_calling_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG (Retrieval-Augmented Generation) Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"\n",
    "Design and implement a RAG (Retrieval-Augmented Generation) system that:\n",
    "\n",
    "1. Ingests documents from multiple formats (PDF, Word, HTML, Markdown)\n",
    "2. Creates embeddings using sentence-transformers\n",
    "3. Stores embeddings in a vector database (Chroma or FAISS)\n",
    "4. Implements semantic search with relevance scoring\n",
    "5. Combines retrieved context with LLM generation\n",
    "\n",
    "Requirements:\n",
    "- Handle documents up to 10MB\n",
    "- Support real-time updates\n",
    "- Implement caching for performance\n",
    "- Include source attribution\n",
    "\n",
    "Provide complete Python implementation with:\n",
    "- Document processing pipeline\n",
    "- Vector storage and retrieval\n",
    "- Integration with local LLM\n",
    "- Example usage\n",
    "\"\"\"\n",
    "\n",
    "response = chat_with_deepseek_coder(rag_prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Selection Guide\n",
    "\n",
    "Based on the examples above, here's when to use each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_guide = \"\"\"\n",
    "## Model Selection Guide\n",
    "\n",
    "### Qwen2.5 Series - Best For:\n",
    "- **Multilingual applications** (25+ languages)\n",
    "- **General-purpose reasoning** and analysis\n",
    "- **Mathematical problem solving**\n",
    "- **Long-form content generation**\n",
    "- **Research and academic work**\n",
    "\n",
    "**RAM Requirements:**\n",
    "- 7B: 8GB RAM\n",
    "- 14B: 16GB RAM  \n",
    "- 32B: 32GB RAM\n",
    "- 72B: 64GB RAM\n",
    "\n",
    "### DeepSeek-Coder Series - Best For:\n",
    "- **Software development** and programming\n",
    "- **Code review** and debugging\n",
    "- **Algorithm implementation**\n",
    "- **Technical documentation**\n",
    "- **DevOps and infrastructure**\n",
    "\n",
    "**RAM Requirements:**\n",
    "- 1.3B: 4GB RAM\n",
    "- 6.7B: 8GB RAM\n",
    "- 16B: 20GB RAM\n",
    "- 33B: 40GB RAM\n",
    "\n",
    "### Mixtral 8x22B - Best For:\n",
    "- **Business applications** requiring reliability\n",
    "- **Complex reasoning** tasks\n",
    "- **Content creation** and writing\n",
    "- **Strategic planning** and analysis\n",
    "- **Efficient high-performance** inference\n",
    "\n",
    "**RAM Requirements:**\n",
    "- 8x7B: 32GB RAM\n",
    "- 8x22B: 48GB RAM\n",
    "\n",
    "### Quick Decision Matrix:\n",
    "\n",
    "| Use Case | 16GB RAM | 32GB RAM | 64GB RAM |\n",
    "|----------|----------|----------|----------|\n",
    "| **Coding** | DeepSeek-Coder-16B | DeepSeek-Coder-33B | Qwen2.5-72B |\n",
    "| **Multilingual** | Qwen2.5-14B | Qwen2.5-32B | Qwen2.5-72B |\n",
    "| **Business** | Qwen2.5-14B | Mixtral-8x7B | Mixtral-8x22B |\n",
    "| **Research** | Qwen2.5-14B | Qwen2.5-32B | Qwen2.5-72B |\n",
    "| **General** | Qwen2.5-14B | Mixtral-8x7B | Qwen2.5-72B |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(selection_guide))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Installation and Setup Commands\n",
    "\n",
    "Here are the commands to get started with these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_commands = \"\"\"\n",
    "## Quick Setup Commands\n",
    "\n",
    "### 1. Install Ollama\n",
    "```bash\n",
    "# Linux/Mac\n",
    "curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Windows: Download from https://ollama.ai/\n",
    "```\n",
    "\n",
    "### 2. Pull Models (choose based on your RAM)\n",
    "\n",
    "**For 16GB RAM:**\n",
    "```bash\n",
    "ollama pull qwen2.5:14b\n",
    "ollama pull deepseek-coder-v2:16b  \n",
    "ollama pull gemma2:9b\n",
    "```\n",
    "\n",
    "**For 32GB RAM:**\n",
    "```bash\n",
    "ollama pull qwen2.5:32b\n",
    "ollama pull deepseek-coder-v2:33b\n",
    "ollama pull mixtral:8x7b\n",
    "```\n",
    "\n",
    "**For 64GB RAM:**\n",
    "```bash\n",
    "ollama pull qwen2.5:72b\n",
    "ollama pull mixtral:8x22b\n",
    "ollama pull yi:34b\n",
    "```\n",
    "\n",
    "### 3. Test Installation\n",
    "```bash\n",
    "# Quick test\n",
    "ollama run qwen2.5:14b \"Hello, tell me about yourself\"\n",
    "\n",
    "# Start server mode\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "### 4. Python Integration\n",
    "```bash\n",
    "pip install ollama openai transformers torch\n",
    "```\n",
    "\n",
    "### 5. Alternative: LM Studio (GUI)\n",
    "- Download: https://lmstudio.ai/\n",
    "- Search and download GGUF models\n",
    "- One-click local server setup\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(setup_commands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the practical capabilities of the best open-source models beyond Llama:\n",
    "\n",
    "- **Qwen2.5**: Exceptional multilingual and reasoning capabilities\n",
    "- **DeepSeek-Coder**: Specialized programming and development tasks\n",
    "- **Mixtral**: Efficient high-performance general-purpose model\n",
    "\n",
    "Each model has unique strengths, and the choice depends on your specific use case, available hardware, and performance requirements.\n",
    "\n",
    "For more detailed information, see the accompanying `best-local-models-2025.md` guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}