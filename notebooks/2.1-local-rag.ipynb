{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG with LlamaIndex + Ollama\n",
    "\n",
    "This notebook demonstrates how to build a **fully local** RAG (Retrieval-Augmented Generation) pipeline using:\n",
    "- **Ollama** for the LLM (runs locally, no API keys needed)\n",
    "- **HuggingFace Embeddings** for document embedding (sentence-transformers)\n",
    "- **LlamaIndex** for orchestrating the RAG pipeline\n",
    "\n",
    "## RAG Pipeline Steps\n",
    "\n",
    "1. **Load** - Read the PDF document\n",
    "2. **Chunk** - Split into manageable pieces\n",
    "3. **Embed** - Convert chunks to vector representations\n",
    "4. **Index** - Store vectors for efficient retrieval\n",
    "5. **Query** - Retrieve relevant chunks and generate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. **Install Ollama**: Download from [ollama.ai](https://ollama.ai)\n",
    "2. **Pull a model**: Run `ollama pull llama3.2` in your terminal\n",
    "3. **Install Python packages**: Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: These are installed as separate packages since LlamaIndex v0.10+\n",
    "!pip install -q llama-index-core llama-index-llms-ollama llama-index-embeddings-huggingface\n",
    "!pip install -q llama-index-readers-file pypdf sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure the LLM and Embedding Model\n",
    "\n",
    "We'll use:\n",
    "- **Ollama** with `llama3.2` for text generation (runs on localhost:11434)\n",
    "- **BGE-small** from HuggingFace for embeddings (384-dim, fast & accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embedding model configured!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Configure the LLM - Ollama runs locally on port 11434\n",
    "Settings.llm = Ollama(\n",
    "    model=\"llama3.2\",           # Use llama3.2 (3B params, good balance)\n",
    "    request_timeout=120.0,       # Timeout for generation\n",
    "    temperature=0.1,             # Low temperature for factual responses\n",
    ")\n",
    "\n",
    "# Configure the embedding model - runs locally via sentence-transformers\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # 384-dim, ~130MB\n",
    ")\n",
    "\n",
    "print(\"LLM and Embedding model configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the PDF Document\n",
    "\n",
    "LlamaIndex's `SimpleDirectoryReader` handles PDF parsing automatically.\n",
    "Each page becomes a separate Document object with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages from the PDF\n",
      "\n",
      "First page preview (first 500 chars):\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load the PDF - using the attention paper as our example\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./assets-resources/attention_paper.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from the PDF\")\n",
    "print(f\"\\nFirst page preview (first 500 chars):\\n{documents[0].text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Vector Index\n",
    "\n",
    "This step:\n",
    "1. Chunks the documents (default: 1024 tokens with 20 overlap)\n",
    "2. Generates embeddings for each chunk\n",
    "3. Stores them in an in-memory vector store\n",
    "\n",
    "For production, you'd use a persistent vector store like ChromaDB or FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0975bbe4dbb04bbea37c06c78a65f62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a7700ae9cc46e09d7847061a0aece6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create the index - this embeds all chunks\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,  # Show embedding progress\n",
    ")\n",
    "\n",
    "print(\"\\nIndex created successfully!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create the Query Engine\n",
    "\n",
    "The query engine combines:\n",
    "- **Retriever**: Finds relevant chunks using vector similarity\n",
    "- **Response Synthesizer**: Generates answers using the LLM\n",
    "\n",
    "`similarity_top_k=3` means we retrieve the 3 most relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 17:14:14,833 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query engine ready!\n"
     ]
    }
   ],
   "source": [
    "# Create query engine with top-3 retrieval\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,  # Number of chunks to retrieve\n",
    ")\n",
    "\n",
    "print(\"Query engine ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query the Documents\n",
    "\n",
    "Now we can ask questions about the paper! The RAG pipeline will:\n",
    "1. Embed your question\n",
    "2. Find similar chunks in the index\n",
    "3. Send chunks + question to the LLM\n",
    "4. Return the generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 17:14:22,892 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The main contribution of this paper appears to be the presentation of the Transformer, a sequence transduction model based entirely on attention mechanisms. This model significantly improves training speed for translation tasks compared to architectures using recurrent or convolutional layers. The paper also explores various modifications and extensions to the original Transformer architecture, including multi-head attention, positional encoding, and dropout regularization, which enhance its performance and efficiency.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question about the paper\n",
    "response = query_engine.query(\"What is the main contribution of this paper?\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 17:21:15,998 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Here are five bullet points explaining what self-attention and how it works:\n",
      "\n",
      "• Self-attention is an attention mechanism that allows a model to attend to different positions within the same input sequence to compute a representation of the entire sequence.\n",
      "• In traditional recurrent neural networks (RNNs), information flows sequentially, meaning each position in the sequence only attends to the previous position. Self-attention breaks this constraint by allowing each position to attend to all other positions.\n",
      "• The self-attention mechanism typically involves three components: queries, keys, and values. These are usually learned linear projections of the input data, which are then used to compute attention weights.\n",
      "• The attention weights are computed by taking the dot product of the query and key vectors, divided by the square root of the dimensionality of the vectors. This is often referred to as \"scaled dot-product attention\".\n",
      "• The final representation of each position is a weighted sum of the values, where the weights are determined by the attention weights. This allows the model to selectively focus on different parts of the input sequence when computing its output.\n"
     ]
    }
   ],
   "source": [
    "# Let's try another question\n",
    "response = query_engine.query(\"What is self-attention and how does it work?Answer in 5 bullet short points\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Retrieved Sources\n",
    "\n",
    "One advantage of RAG is transparency - we can see which chunks were used to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source chunks: 3\n",
      "\n",
      "--- Source 1 (score: 0.687) ---\n",
      "MultiHead(Q,K,V ) = Concat(head 1,..., headh)W O\n",
      "where headi = Attention(QW Q\n",
      "i ,KW K\n",
      "i ,VW V\n",
      "i )\n",
      "Where the projections are parameter matricesW Q\n",
      "i ∈ Rdmodel×dk,W K\n",
      "i ∈ Rdmodel×dk,W V\n",
      "i ∈ Rdmodel×dv\n",
      "andW O∈ Rhdv×dmodel.\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of th...\n",
      "\n",
      "--- Source 2 (score: 0.672) ---\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practi...\n",
      "\n",
      "--- Source 3 (score: 0.652) ---\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "statesht, as a function of the previous hidden stateht−1 and the input for positiont. This inherently\n",
      "se...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the source nodes (retrieved chunks)\n",
    "print(f\"Number of source chunks: {len(response.source_nodes)}\\n\")\n",
    "\n",
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(f\"--- Source {i+1} (score: {node.score:.3f}) ---\")\n",
    "    print(f\"{node.text[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Persist the Index\n",
    "\n",
    "Save the index to disk so you don't need to re-embed documents each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index to disk\n",
    "index.storage_context.persist(persist_dir=\"./storage/attention_paper\")\n",
    "print(\"Index saved to ./storage/attention_paper/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the index later:\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# Uncomment to load:\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=\"./storage/attention_paper\")\n",
    "# loaded_index = load_index_from_storage(storage_context)\n",
    "# query_engine = loaded_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've built a complete local RAG pipeline! Key components:\n",
    "\n",
    "| Component | Tool | Why |\n",
    "|-----------|------|-----|\n",
    "| LLM | Ollama (llama3.2) | Local, no API keys, easy model management |\n",
    "| Embeddings | HuggingFace (bge-small) | Fast, accurate, runs locally |\n",
    "| Orchestration | LlamaIndex | Handles chunking, indexing, retrieval |\n",
    "\n",
    "### Next Steps\n",
    "- Try different models: `ollama pull mistral` or `ollama pull phi3`\n",
    "- Use persistent storage: ChromaDB, FAISS\n",
    "- Experiment with chunk sizes via `Settings.chunk_size`\n",
    "- Add hybrid search (keyword + semantic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
