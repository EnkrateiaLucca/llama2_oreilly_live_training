{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-header",
      "metadata": {},
      "source": [
        "# Complete Guide to Fine-Tuning Llama 3 Models (2025 Edition)\n",
        "\n",
        "This notebook provides a comprehensive walkthrough of fine-tuning Llama 3 models using the latest best practices and top frameworks.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. **When to fine-tune** vs. when to use RAG or prompting\n",
        "2. **Top 5 fine-tuning frameworks** compared and demonstrated\n",
        "3. **Best practices** for efficient training (LoRA, QLoRA, memory optimization)\n",
        "4. **Hardware requirements** for different model sizes\n",
        "5. **Synthetic data generation** techniques\n",
        "\n",
        "## Frameworks Covered\n",
        "\n",
        "| Framework | Best For | Key Feature |\n",
        "|-----------|----------|-------------|\n",
        "| **Unsloth** | Speed & Memory Efficiency | 2-5x faster, 70% less VRAM |\n",
        "| **torchtune** | PyTorch Native | Official Meta support |\n",
        "| **TRL** | Hugging Face Ecosystem | SFTTrainer, RLHF support |\n",
        "| **Axolotl** | Flexibility | YAML config, many techniques |\n",
        "| **LLaMA-Factory** | All-in-one | WebUI, 100+ models |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decision-tree",
      "metadata": {},
      "source": [
        "## When to Fine-Tune vs. Alternatives\n",
        "\n",
        "### Use Fine-Tuning When:\n",
        "- You need consistent formatting or style\n",
        "- Domain-specific knowledge is required\n",
        "- You want to reduce inference costs (smaller fine-tuned model can match larger base model)\n",
        "- Privacy: You can't send data to external APIs\n",
        "\n",
        "### Use RAG Instead When:\n",
        "- Knowledge changes frequently\n",
        "- You need source attribution\n",
        "- You have a large, evolving knowledge base\n",
        "\n",
        "### Use Prompt Engineering When:\n",
        "- Quick iteration is needed\n",
        "- Limited training data available\n",
        "- Task is well-defined with few examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hardware-requirements",
      "metadata": {},
      "source": [
        "## Hardware Requirements\n",
        "\n",
        "| Model Size | Full Fine-Tune | LoRA | QLoRA (4-bit) |\n",
        "|------------|----------------|------|---------------|\n",
        "| 1B | 8GB VRAM | 6GB | 4GB |\n",
        "| 3B | 16GB VRAM | 10GB | 6GB |\n",
        "| 8B | 40GB VRAM | 16GB | 8GB |\n",
        "| 70B | 320GB VRAM | 80GB | 24GB |\n",
        "\n",
        "**Recommendation**: Start with QLoRA on consumer GPUs (RTX 3090/4090 with 24GB), then scale up if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-section",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: Environment Setup\n",
        "\n",
        "Let's install the required packages for all frameworks we'll explore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-packages",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core packages for all frameworks\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
        "\n",
        "# TRL for Hugging Face training\n",
        "!pip install -q trl\n",
        "\n",
        "# Unsloth for fast training (install separately as it has specific requirements)\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# torchtune (PyTorch native)\n",
        "!pip install -q torchtune\n",
        "\n",
        "# For evaluation and visualization\n",
        "!pip install -q wandb matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import os\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "framework1-header",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: Framework 1 - Unsloth (Recommended for Speed)\n",
        "\n",
        "[Unsloth](https://github.com/unslothai/unsloth) is the fastest way to fine-tune LLMs, achieving **2-5x speedup** with **70% less memory**.\n",
        "\n",
        "**Key Features:**\n",
        "- No accuracy degradation\n",
        "- Automatic Ollama export\n",
        "- Supports Llama 3.x, Gemma, Mistral, Qwen, and more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unsloth-install",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth (run this cell if you want to use Unsloth)\n",
        "# For Colab:\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# For local installation:\n",
        "# !pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unsloth-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unsloth Fine-Tuning Example\n",
        "# Uncomment and run if Unsloth is installed\n",
        "\n",
        "'''\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Configuration\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto-detect\n",
        "load_in_4bit = True  # Use 4-bit quantization\n",
        "\n",
        "# Load model with Unsloth optimizations\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank - higher = more parameters\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth optimization\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "framework2-header",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: Framework 2 - TRL + Hugging Face (Most Popular)\n",
        "\n",
        "[TRL](https://huggingface.co/docs/trl) is Hugging Face's library for training language models with reinforcement learning and supervised fine-tuning.\n",
        "\n",
        "**Key Features:**\n",
        "- SFTTrainer for supervised fine-tuning\n",
        "- PPO, DPO, ORPO for alignment\n",
        "- Seamless integration with transformers ecosystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trl-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRL Fine-Tuning with QLoRA\n",
        "\n",
        "# Model configuration\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"  # Or use a smaller model for testing\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # LoRA rank\n",
        "    lora_alpha=32,  # Alpha scaling\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trl-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample dataset\n",
        "# Using a small subset for demonstration\n",
        "dataset = load_dataset(\"HuggingFaceH4/no_robots\", split=\"train[:1000]\")\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"\\nExample:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trl-format",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format dataset for chat template\n",
        "def format_chat_template(example):\n",
        "    \"\"\"Format the dataset for Llama 3 chat template.\"\"\"\n",
        "    messages = example.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        # Handle different dataset formats\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": example.get(\"prompt\", \"\")},\n",
        "            {\"role\": \"assistant\", \"content\": example.get(\"completion\", \"\")}\n",
        "        ]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Apply formatting\n",
        "formatted_dataset = dataset.map(format_chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trl-training",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRL Training Configuration\n",
        "# Note: This is configured for demonstration. Uncomment to run actual training.\n",
        "\n",
        "'''\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training arguments\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./llama3-finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    max_seq_length=2048,\n",
        "    packing=True,  # Pack multiple samples into one sequence\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"./llama3-finetuned-final\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "framework3-header",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: Framework 3 - torchtune (PyTorch Native)\n",
        "\n",
        "[torchtune](https://github.com/pytorch/torchtune) is PyTorch's official library for LLM fine-tuning, with deep ecosystem integration.\n",
        "\n",
        "**Key Features:**\n",
        "- Official Meta support for Llama models\n",
        "- YAML-based configuration\n",
        "- Multi-GPU and multi-node training\n",
        "- Supports NVIDIA, AMD, Intel, and Apple Silicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "torchtune-commands",
      "metadata": {},
      "outputs": [],
      "source": [
        "# torchtune uses CLI commands for training\n",
        "# Here are the key commands:\n",
        "\n",
        "# 1. Download a model\n",
        "# !tune download meta-llama/Llama-3.2-3B-Instruct --output-dir ./models/llama-3.2-3b\n",
        "\n",
        "# 2. Single-GPU LoRA fine-tuning\n",
        "# !tune run lora_finetune_single_device --config llama3_2/3B_lora_single_device\n",
        "\n",
        "# 3. Multi-GPU full fine-tuning\n",
        "# !tune run --nproc_per_node 2 full_finetune_distributed --config llama3_2/3B_full\n",
        "\n",
        "# 4. QLoRA for memory-constrained environments\n",
        "# !tune run lora_finetune_single_device --config llama3_2/3B_qlora_single_device\n",
        "\n",
        "print(\"torchtune commands shown above. Uncomment to run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "torchtune-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example torchtune YAML configuration\n",
        "# Save this as 'custom_config.yaml' to use with torchtune\n",
        "\n",
        "torchtune_config = '''\n",
        "# Custom torchtune configuration for Llama 3.2 3B\n",
        "model:\n",
        "  _component_: torchtune.models.llama3_2.lora_llama3_2_3b\n",
        "  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
        "  lora_rank: 16\n",
        "  lora_alpha: 32\n",
        "\n",
        "tokenizer:\n",
        "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
        "  path: ./models/llama-3.2-3b/tokenizer.model\n",
        "\n",
        "checkpointer:\n",
        "  _component_: torchtune.training.FullModelHFCheckpointer\n",
        "  checkpoint_dir: ./models/llama-3.2-3b\n",
        "  output_dir: ./outputs/llama-3.2-3b-finetuned\n",
        "\n",
        "dataset:\n",
        "  _component_: torchtune.datasets.alpaca_dataset\n",
        "  source: tatsu-lab/alpaca\n",
        "\n",
        "optimizer:\n",
        "  _component_: torch.optim.AdamW\n",
        "  lr: 2e-5\n",
        "  weight_decay: 0.01\n",
        "\n",
        "lr_scheduler:\n",
        "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
        "  num_warmup_steps: 100\n",
        "\n",
        "training:\n",
        "  epochs: 3\n",
        "  batch_size: 4\n",
        "  gradient_accumulation_steps: 4\n",
        "  compile: false  # Set to true for faster training with torch.compile\n",
        "'''\n",
        "\n",
        "print(torchtune_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "framework4-header",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 5: Framework 4 - Axolotl (Maximum Flexibility)\n",
        "\n",
        "[Axolotl](https://github.com/axolotl-ai-cloud/axolotl) is known for its flexibility and community support.\n",
        "\n",
        "**Key Features:**\n",
        "- YAML configuration for reproducibility\n",
        "- Supports many training techniques (LoRA, QLoRA, full fine-tune)\n",
        "- Rapid adoption of new models and methods\n",
        "- Great for beginners"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "axolotl-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Axolotl YAML configuration example\n",
        "# Save as 'axolotl_config.yaml'\n",
        "\n",
        "axolotl_config = '''\n",
        "base_model: meta-llama/Llama-3.2-3B-Instruct\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "\n",
        "# LoRA configuration\n",
        "adapter: lora\n",
        "lora_r: 16\n",
        "lora_alpha: 32\n",
        "lora_dropout: 0.05\n",
        "lora_target_modules:\n",
        "  - q_proj\n",
        "  - k_proj\n",
        "  - v_proj\n",
        "  - o_proj\n",
        "  - gate_proj\n",
        "  - up_proj\n",
        "  - down_proj\n",
        "\n",
        "# Dataset\n",
        "datasets:\n",
        "  - path: HuggingFaceH4/no_robots\n",
        "    type: chat_template\n",
        "    chat_template: llama3\n",
        "\n",
        "# Training parameters\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "gradient_accumulation_steps: 4\n",
        "micro_batch_size: 4\n",
        "num_epochs: 3\n",
        "learning_rate: 2e-4\n",
        "lr_scheduler: cosine\n",
        "warmup_ratio: 0.1\n",
        "\n",
        "optimizer: adamw_bnb_8bit\n",
        "bf16: true\n",
        "tf32: true\n",
        "\n",
        "gradient_checkpointing: true\n",
        "flash_attention: true\n",
        "\n",
        "output_dir: ./outputs/llama-3.2-3b-axolotl\n",
        "'''\n",
        "\n",
        "print(axolotl_config)\n",
        "\n",
        "# To run Axolotl:\n",
        "# !pip install axolotl\n",
        "# !accelerate launch -m axolotl.cli.train axolotl_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "framework5-header",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 6: Framework 5 - LLaMA-Factory (All-in-One)\n",
        "\n",
        "[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) provides a unified interface for training 100+ LLMs with web UI support.\n",
        "\n",
        "**Key Features:**\n",
        "- WebUI for no-code fine-tuning\n",
        "- Supports 100+ models\n",
        "- Integrated methods: SFT, RLHF, DPO, PPO\n",
        "- Unsloth integration for speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "llamafactory-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLaMA-Factory installation and usage\n",
        "\n",
        "# Install\n",
        "# !git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "# !cd LLaMA-Factory && pip install -e \".[torch,metrics]\"\n",
        "\n",
        "# Launch WebUI (easiest way to start)\n",
        "# !cd LLaMA-Factory && llamafactory-cli webui\n",
        "\n",
        "# Or use CLI for training\n",
        "# !llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\n",
        "\n",
        "print(\"LLaMA-Factory commands shown above. Uncomment to run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "llamafactory-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLaMA-Factory configuration example\n",
        "llamafactory_config = '''\n",
        "### Model\n",
        "model_name_or_path: meta-llama/Llama-3.2-3B-Instruct\n",
        "\n",
        "### Method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 16\n",
        "lora_alpha: 32\n",
        "lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\n",
        "\n",
        "### Dataset\n",
        "dataset: alpaca_en\n",
        "template: llama3\n",
        "cutoff_len: 2048\n",
        "\n",
        "### Training\n",
        "output_dir: outputs/llama3-lora\n",
        "per_device_train_batch_size: 4\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 2.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "\n",
        "### Optimization\n",
        "quantization_bit: 4\n",
        "gradient_checkpointing: true\n",
        "'''\n",
        "\n",
        "print(llamafactory_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "best-practices",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 7: Best Practices for Fine-Tuning\n",
        "\n",
        "## Data Quality Matters Most\n",
        "\n",
        "1. **Quality over Quantity**: 1,000 high-quality examples often beats 100,000 noisy ones\n",
        "2. **Diversity**: Cover edge cases and variations in your dataset\n",
        "3. **Formatting**: Use consistent formatting that matches inference\n",
        "\n",
        "## LoRA Hyperparameter Guidelines\n",
        "\n",
        "| Parameter | Recommended | Notes |\n",
        "|-----------|-------------|-------|\n",
        "| `lora_r` | 8-32 | Higher = more capacity, more memory |\n",
        "| `lora_alpha` | 16-64 | Usually 2x lora_r |\n",
        "| `lora_dropout` | 0.0-0.1 | 0 for small datasets |\n",
        "| `target_modules` | All attention + MLP | q,k,v,o,gate,up,down |\n",
        "\n",
        "## Training Hyperparameters\n",
        "\n",
        "| Parameter | Recommended | Notes |\n",
        "|-----------|-------------|-------|\n",
        "| Learning rate | 1e-4 to 2e-4 | Lower for larger models |\n",
        "| Batch size | 4-8 per GPU | Use gradient accumulation |\n",
        "| Epochs | 1-3 | Watch for overfitting |\n",
        "| Warmup | 10% of steps | Prevents early instability |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "synthetic-data",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 8: Synthetic Data Generation\n",
        "\n",
        "When you don't have enough training data, you can generate synthetic examples using a larger model.\n",
        "\n",
        "## Techniques:\n",
        "\n",
        "1. **Self-Instruct**: Use the model to generate instruction-response pairs\n",
        "2. **Evol-Instruct**: Evolve simple instructions into complex ones\n",
        "3. **Distillation**: Use a larger model to generate training data for a smaller one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "synthetic-data-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "def generate_synthetic_examples(topic: str, num_examples: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Generate synthetic training examples using a local LLM.\n",
        "    \n",
        "    Args:\n",
        "        topic: The topic or domain for generating examples\n",
        "        num_examples: Number of examples to generate\n",
        "    \n",
        "    Returns:\n",
        "        List of instruction-response pairs\n",
        "    \"\"\"\n",
        "    prompt = f'''Generate {num_examples} diverse instruction-response pairs for training a helpful AI assistant on the topic of \"{topic}\".\n",
        "\n",
        "Format each example as:\n",
        "INSTRUCTION: [user question or task]\n",
        "RESPONSE: [helpful, detailed response]\n",
        "\n",
        "Make the examples varied in complexity and style. Include edge cases.'''\n",
        "\n",
        "    try:\n",
        "        response = ollama.chat(\n",
        "            model='llama3.2',\n",
        "            messages=[{'role': 'user', 'content': prompt}]\n",
        "        )\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating synthetic data: {e}\")\n",
        "        return []\n",
        "\n",
        "# Example usage (uncomment to run)\n",
        "# synthetic_examples = generate_synthetic_examples(\"Python debugging\", num_examples=3)\n",
        "# print(synthetic_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "evaluation-section",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 9: Evaluating Your Fine-Tuned Model\n",
        "\n",
        "Always evaluate your model after fine-tuning to ensure it has learned the desired behavior without forgetting general capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluation-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_prompts: list):\n",
        "    \"\"\"\n",
        "    Simple evaluation function for fine-tuned models.\n",
        "    \n",
        "    Args:\n",
        "        model: The fine-tuned model\n",
        "        tokenizer: The tokenizer\n",
        "        test_prompts: List of test prompts to evaluate\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "    \n",
        "    for prompt in test_prompts:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        \n",
        "        # Format for Llama 3 chat template\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            messages, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        \n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "            )\n",
        "        \n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        results.append({\"prompt\": prompt, \"response\": response})\n",
        "        \n",
        "    return results\n",
        "\n",
        "# Example test prompts\n",
        "test_prompts = [\n",
        "    \"Explain what machine learning is in simple terms.\",\n",
        "    \"Write a Python function to calculate the Fibonacci sequence.\",\n",
        "    \"What are the benefits of using renewable energy?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "export-section",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 10: Exporting to Ollama for Local Use\n",
        "\n",
        "After fine-tuning, you can export your model to Ollama for easy local deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ollama-export",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export fine-tuned model to GGUF format for Ollama\n",
        "\n",
        "def export_to_gguf(model_path: str, output_path: str, quantization: str = \"q4_k_m\"):\n",
        "    \"\"\"\n",
        "    Convert a fine-tuned model to GGUF format for Ollama.\n",
        "    \n",
        "    Requires llama.cpp to be installed.\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    \n",
        "    # Convert to GGUF\n",
        "    cmd = f\"python llama.cpp/convert_hf_to_gguf.py {model_path} --outfile {output_path} --outtype {quantization}\"\n",
        "    print(f\"Running: {cmd}\")\n",
        "    # subprocess.run(cmd, shell=True)\n",
        "    \n",
        "# Create Ollama Modelfile\n",
        "modelfile_content = '''\n",
        "FROM ./llama-3.2-3b-finetuned.gguf\n",
        "\n",
        "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{ .Response }}<|eot_id|>\"\"\"\n",
        "\n",
        "PARAMETER stop <|start_header_id|>\n",
        "PARAMETER stop <|end_header_id|>\n",
        "PARAMETER stop <|eot_id|>\n",
        "PARAMETER temperature 0.7\n",
        "'''\n",
        "\n",
        "print(\"Modelfile for Ollama:\")\n",
        "print(modelfile_content)\n",
        "\n",
        "# To create the Ollama model:\n",
        "# !ollama create my-finetuned-model -f Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison-section",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 11: Framework Comparison Summary\n",
        "\n",
        "## Decision Guide\n",
        "\n",
        "| Your Situation | Recommended Framework |\n",
        "|----------------|----------------------|\n",
        "| Limited GPU memory | **Unsloth** |\n",
        "| Need fastest training | **Unsloth** |\n",
        "| PyTorch ecosystem | **torchtune** |\n",
        "| Hugging Face ecosystem | **TRL** |\n",
        "| Maximum flexibility | **Axolotl** |\n",
        "| Prefer web UI | **LLaMA-Factory** |\n",
        "| New to fine-tuning | **Axolotl** or **LLaMA-Factory** |\n",
        "| Production deployment | **torchtune** or **TRL** |\n",
        "\n",
        "## Performance Comparison (8B model on RTX 4090)\n",
        "\n",
        "| Framework | Training Speed | Memory Usage |\n",
        "|-----------|---------------|---------------|\n",
        "| Unsloth | ~2.1x faster | ~8GB |\n",
        "| TRL + QLoRA | Baseline | ~12GB |\n",
        "| torchtune | ~1.1x faster | ~11GB |\n",
        "| Axolotl | Baseline | ~12GB |\n",
        "| LLaMA-Factory | ~1.5x faster (w/ Unsloth) | ~8GB |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "resources-section",
      "metadata": {},
      "source": [
        "---\n",
        "# Additional Resources\n",
        "\n",
        "## Official Documentation\n",
        "- [Meta Llama Fine-tuning Guide](https://www.llama.com/docs/how-to-guides/fine-tuning/)\n",
        "- [Hugging Face PEFT](https://huggingface.co/docs/peft)\n",
        "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
        "\n",
        "## Framework Repositories\n",
        "- [Unsloth](https://github.com/unslothai/unsloth) - Fast training with low memory\n",
        "- [torchtune](https://github.com/pytorch/torchtune) - PyTorch native\n",
        "- [Axolotl](https://github.com/axolotl-ai-cloud/axolotl) - Flexible YAML config\n",
        "- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - All-in-one solution\n",
        "\n",
        "## Tutorials & Guides\n",
        "- [Unsloth Llama 3 Tutorial](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide)\n",
        "- [Fine-tune Llama 3.1 with TRL](https://huggingface.co/blog/mlabonne/sft-llama3)\n",
        "- [llama-cookbook Fine-tuning](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/finetuning)\n",
        "\n",
        "## Related Notebooks in This Course\n",
        "- `6.0-fine-tuning-llama3-what-you-need-to-know.md` - Conceptual overview\n",
        "- `6.2-quantization-precision-format-code-explanation.ipynb` - Deep dive into quantization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
