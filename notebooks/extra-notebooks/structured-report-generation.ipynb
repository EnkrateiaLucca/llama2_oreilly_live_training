{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Report Generation with Ollama and Python\n",
    "\n",
    "This notebook demonstrates how to build a structured report generation system using basic Python code and the Ollama API. Unlike complex frameworks like LangGraph, we'll implement all the necessary components from scratch, making it easier to understand and customize.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "We'll create an AI-powered report generator that:\n",
    "- Uses local language models via Ollama\n",
    "- Performs web searches to gather information\n",
    "- Generates structured reports with consistent formatting\n",
    "- Implements tool calling and routing without complex frameworks\n",
    "\n",
    "## Why Local Models?\n",
    "\n",
    "- **Privacy**: Your data stays on your machine\n",
    "- **Cost**: No API fees or usage limits\n",
    "- **Control**: Full control over the model and generation process\n",
    "- **Learning**: Better understanding of how these systems work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ollama pydantic duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have Ollama installed and running on your system. If not, download it from https://ollama.com/download\n",
    "\n",
    "Pull the model we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the model (this will take a few minutes the first time)\n",
    "!ollama pull gemma3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import ollama\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from duckduckgo_search import DDGS\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Structured Output with Pydantic\n",
    "\n",
    "One of the key features we need is the ability to get structured, consistent outputs from our language model. We'll use Pydantic to define schemas and ensure the model returns data in the format we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure for a report section\n",
    "class ReportSection(BaseModel):\n",
    "    title: str = Field(description=\"The title of this section\")\n",
    "    content: str = Field(description=\"The main content of this section\")\n",
    "    key_points: List[str] = Field(description=\"Key takeaways from this section\")\n",
    "    sources: List[str] = Field(description=\"Sources used for this section\", default=[])\n",
    "\n",
    "# Define the overall report structure\n",
    "class StructuredReport(BaseModel):\n",
    "    title: str = Field(description=\"The main title of the report\")\n",
    "    executive_summary: str = Field(description=\"A brief executive summary\")\n",
    "    sections: List[ReportSection] = Field(description=\"The main sections of the report\")\n",
    "    conclusion: str = Field(description=\"The conclusion of the report\")\n",
    "    generated_at: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "# Example of how to use structured output with Ollama\n",
    "def generate_structured_output(prompt: str, schema: BaseModel):\n",
    "    \"\"\"\n",
    "    Generate structured output using Ollama with a Pydantic schema\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='gemma3n',\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': prompt\n",
    "        }],\n",
    "        format=schema.model_json_schema()\n",
    "    )\n",
    "    \n",
    "    # Parse the response\n",
    "    return schema.model_validate_json(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Benefits of Renewable Energy: A Comprehensive Overview\n",
      "Content: ## Benefits of Renewable Energy: A Comprehensive Overview\n",
      "\n",
      "Transitioning to renewable energy sources offers a multitude of benefits, spanning environmental, economic, and social dimensions. This secti...\n",
      "Key Points: ['Reduced greenhouse gas emissions and improved air quality.', 'Job creation, energy independence, and price stability.', 'Improved public health, energy access, and community empowerment.']\n"
     ]
    }
   ],
   "source": [
    "# Test structured output generation\n",
    "test_section = generate_structured_output(\n",
    "    \"Create a report section about the benefits of renewable energy\",\n",
    "    ReportSection\n",
    ")\n",
    "print(f\"Title: {test_section.title}\")\n",
    "print(f\"Content: {test_section.content[:200]}...\")\n",
    "print(f\"Key Points: {test_section.key_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Benefits of Renewable Energy: A Comprehensive Overview\n",
       "\n",
       "## Benefits of Renewable Energy: A Comprehensive Overview\n",
       "\n",
       "Transitioning to renewable energy sources offers a multitude of benefits, spanning environmental, economic, and social dimensions. This section details these advantages, highlighting the compelling reasons for prioritizing renewable energy development and deployment.\n",
       "\n",
       "**1. Environmental Advantages:**\n",
       "\n",
       "*   **Reduced Greenhouse Gas Emissions:**  Renewable energy sources like solar, wind, hydro, and geothermal produce little to no greenhouse gas emissions during operation. This is crucial in mitigating climate change by reducing the concentration of heat-trapping gases in the atmosphere.  Compared to fossil fuels, renewables significantly lower the carbon footprint of electricity generation.\n",
       "*   **Improved Air Quality:** Unlike fossil fuel power plants, renewable energy facilities do not release harmful air pollutants such as sulfur dioxide, nitrogen oxides, and particulate matter. This leads to cleaner air, reducing respiratory illnesses and improving public health.\n",
       "*   **Water Conservation:** Many renewable energy technologies, particularly solar and wind, require significantly less water than traditional power plants that rely on water for cooling. This is especially important in water-stressed regions.\n",
       "*   **Reduced Environmental Impact:** Renewable energy development can minimize habitat destruction and ecosystem disruption compared to fossil fuel extraction and transportation.  Careful planning and siting are essential to further minimize any potential impacts.\n",
       "\n",
       "**2. Economic Advantages:**\n",
       "\n",
       "*   **Job Creation:** The renewable energy sector is a rapidly growing industry, creating numerous jobs in manufacturing, installation, maintenance, and research & development.  This provides economic opportunities and stimulates local economies.\n",
       "*   **Energy Independence & Security:**  Renewable energy sources are domestically available in most countries, reducing reliance on volatile global fossil fuel markets and enhancing energy security.  This shields economies from price fluctuations and geopolitical instability.\n",
       "*   **Price Stability:**  Once a renewable energy facility is built, the fuel (sun, wind, water) is free. This leads to more stable and predictable electricity prices compared to fossil fuels, which are subject to market fluctuations.\n",
       "*   **Economic Development:**  Renewable energy projects can attract investment, stimulate local economic activity, and create new business opportunities in rural areas.\n",
       "*   **Reduced Healthcare Costs:** Improved air quality resulting from renewable energy adoption translates to lower healthcare costs associated with respiratory and cardiovascular diseases.\n",
       "\n",
       "**3. Social Advantages:**\n",
       "\n",
       "*   **Improved Public Health:** Cleaner air and water contribute to improved public health outcomes and a higher quality of life.\n",
       "*   **Energy Access:** Renewable energy technologies, particularly off-grid solar systems, can provide electricity to remote communities that lack access to the traditional power grid, improving education, healthcare, and economic opportunities.\n",
       "*   **Community Empowerment:**  Community-owned renewable energy projects can empower local communities, fostering economic development and promoting energy democracy.\n",
       "*   **Resilience:**  Decentralized renewable energy systems, such as rooftop solar with battery storage, can enhance grid resilience and provide power during outages caused by natural disasters or other disruptions.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The benefits of renewable energy are undeniable and far-reaching.  By embracing renewable energy, we can create a cleaner, more sustainable, and more equitable future for all.  Continued investment in research, development, and deployment of renewable energy technologies is essential to realizing these benefits and addressing the urgent challenges of climate change and energy security.\n",
       "\n",
       "**Key Points:**\n",
       "- Reduced greenhouse gas emissions and improved air quality.\n",
       "- Job creation, energy independence, and price stability.\n",
       "- Improved public health, energy access, and community empowerment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"### {test_section.title}\\n\\n{test_section.content}\\n\\n**Key Points:**\\n\" + \"\\n\".join(f\"- {kp}\" for kp in test_section.key_points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Web Search Integration\n",
    "\n",
    "To create comprehensive reports, we need to gather information from the web. We'll implement a simple web search function that our AI can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ArtificialAiming\n",
      "   www.ArtificialAiming.net - The best website for quality cheats for games like GTA, BattleField, Call...\n",
      "   URL: https://www.artificialaiming.net/forum/index.php\n",
      "\n",
      "2. HWID spoofer - ArtificialAiming\n",
      "   Mar 13, 2015 · Hallo wollte mal frageb ob es HWID spoofer schon gibt bevor ich den Cheat Kaufe geht ...\n",
      "   URL: https://www.artificialaiming.net/forum/german-speaking-forum/98858-hwid-spoofer.html\n",
      "\n",
      "3. ArtificialAiming\n",
      "   Anthem Feb 21, 2019 - 2:37 AM - by HelioS A first build of our new Anthem cheat is now available to ...\n",
      "   URL: https://www.artificialaiming.net/forum/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/y8s3fc655417629rqwgxkhx80000gn/T/ipykernel_60944/1207023605.py:13: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  results = DDGS().text(query, max_results=max_results)\n"
     ]
    }
   ],
   "source": [
    "def web_search(query: str, max_results: int = 5) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Perform a web search using DuckDuckGo\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        max_results: Maximum number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with title, body, and href\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = DDGS().text(query, max_results=max_results)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": r.get(\"title\", \"\"),\n",
    "                \"snippet\": r.get(\"body\", \"\"),\n",
    "                \"url\": r.get(\"href\", \"\")\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the web search function\n",
    "search_results = web_search(\"artificial intelligence applications 2024\")\n",
    "for i, result in enumerate(search_results[:3]):\n",
    "    print(f\"\\n{i+1}. {result['title']}\")\n",
    "    print(f\"   {result['snippet'][:100]}...\")\n",
    "    print(f\"   URL: {result['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a Simple Tool Calling System\n",
    "\n",
    "Now let's implement a basic tool calling system that allows our LLM to use functions like web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schemas for tool calling\n",
    "class ToolCall(BaseModel):\n",
    "    tool_name: str = Field(description=\"The name of the tool to call\")\n",
    "    arguments: Dict[str, Any] = Field(description=\"Arguments to pass to the tool\")\n",
    "\n",
    "class ToolResponse(BaseModel):\n",
    "    tool_name: str\n",
    "    result: Any\n",
    "    success: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "# Available tools\n",
    "AVAILABLE_TOOLS = {\n",
    "    \"web_search\": {\n",
    "        \"function\": web_search,\n",
    "        \"description\": \"Search the web for information\",\n",
    "        \"parameters\": {\n",
    "            \"query\": \"The search query string\",\n",
    "            \"max_results\": \"Maximum number of results (default: 5)\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def execute_tool(tool_call: ToolCall) -> ToolResponse:\n",
    "    \"\"\"\n",
    "    Execute a tool call and return the result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if tool_call.tool_name not in AVAILABLE_TOOLS:\n",
    "            return ToolResponse(\n",
    "                tool_name=tool_call.tool_name,\n",
    "                result=None,\n",
    "                success=False,\n",
    "                error=f\"Unknown tool: {tool_call.tool_name}\"\n",
    "            )\n",
    "        \n",
    "        tool_func = AVAILABLE_TOOLS[tool_call.tool_name][\"function\"]\n",
    "        result = tool_func(**tool_call.arguments)\n",
    "        \n",
    "        return ToolResponse(\n",
    "            tool_name=tool_call.tool_name,\n",
    "            result=result,\n",
    "            success=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ToolResponse(\n",
    "            tool_name=tool_call.tool_name,\n",
    "            result=None,\n",
    "            success=False,\n",
    "            error=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_with_tools(prompt: str, include_tool_results: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Process a prompt that may require tool usage\n",
    "    \"\"\"\n",
    "    # First, ask the LLM if it needs to use any tools\n",
    "    tool_check_prompt = f\"\"\"\n",
    "    You have access to the following tools:\n",
    "    {json.dumps(AVAILABLE_TOOLS, indent=2)}\n",
    "    \n",
    "    User request: {prompt}\n",
    "    \n",
    "    Do you need to use any tools to answer this request? \n",
    "    If yes, specify which tool and with what arguments.\n",
    "    Respond with a ToolCall object or say \"NO_TOOLS_NEEDED\" if you can answer directly.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='gemma3n',\n",
    "        messages=[{'role': 'user', 'content': tool_check_prompt}]\n",
    "    )\n",
    "    \n",
    "    response_text = response['message']['content']\n",
    "    \n",
    "    # Check if tools are needed\n",
    "    if \"NO_TOOLS_NEEDED\" in response_text:\n",
    "        # Answer directly without tools\n",
    "        direct_response = ollama.chat(\n",
    "            model='gemma3n',\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return direct_response['message']['content']\n",
    "    \n",
    "    # Try to parse tool call\n",
    "    try:\n",
    "        # Extract JSON from response\n",
    "        import re\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            tool_call_data = json.loads(json_match.group())\n",
    "            tool_call = ToolCall(**tool_call_data)\n",
    "            \n",
    "            # Execute the tool\n",
    "            tool_result = execute_tool(tool_call)\n",
    "            \n",
    "            if include_tool_results:\n",
    "                # Generate final response with tool results\n",
    "                final_prompt = f\"\"\"\n",
    "                Original request: {prompt}\n",
    "                \n",
    "                Tool used: {tool_result.tool_name}\n",
    "                Tool result: {json.dumps(tool_result.result, indent=2)}\n",
    "                \n",
    "                Please provide a comprehensive answer to the original request using the tool results.\n",
    "                \"\"\"\n",
    "                \n",
    "                final_response = ollama.chat(\n",
    "                    model='gemma3n',\n",
    "                    messages=[{'role': 'user', 'content': final_prompt}]\n",
    "                )\n",
    "                return final_response['message']['content']\n",
    "            else:\n",
    "                return json.dumps(tool_result.dict(), indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing tool call: {e}\")\n",
    "        # Fall back to direct response\n",
    "        direct_response = ollama.chat(\n",
    "            model='gemma3n',\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return direct_response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type function is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test the tool calling system\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m result \u001b[39m=\u001b[39m llm_with_tools(\u001b[39m\"\u001b[39;49m\u001b[39mWhat are the latest developments in quantum computing in 2024?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(result[:\u001b[39m500\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb Cell 16\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mProcess a prompt that may require tool usage\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# First, ask the LLM if it needs to use any tools\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tool_check_prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mYou have access to the following tools:\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m{\u001b[39;00mjson\u001b[39m.\u001b[39;49mdumps(AVAILABLE_TOOLS,\u001b[39m \u001b[39;49mindent\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\u001b[39m}\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mUser request: \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mDo you need to use any tools to answer this request? \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mIf yes, specify which tool and with what arguments.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mRespond with a ToolCall object or say \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNO_TOOLS_NEEDED\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m if you can answer directly.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m response \u001b[39m=\u001b[39m ollama\u001b[39m.\u001b[39mchat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgemma3n\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     messages\u001b[39m=\u001b[39m[{\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: tool_check_prompt}]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m response_text \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/json/encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterencode(o, _one_shot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(chunks)\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[1;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type function is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Test the tool calling system\n",
    "result = llm_with_tools(\"What are the latest developments in quantum computing in 2024?\")\n",
    "print(result[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Report Generation Pipeline\n",
    "\n",
    "Now let's build the main report generation pipeline that combines all our components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    def __init__(self, model_name: str = 'gemma3n'):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def research_topic(self, topic: str, num_searches: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Research a topic by performing multiple web searches\n",
    "        \"\"\"\n",
    "        # Generate search queries\n",
    "        query_prompt = f\"\"\"\n",
    "        Generate {num_searches} different search queries to research the topic: \"{topic}\"\n",
    "        Make the queries specific and diverse to cover different aspects.\n",
    "        Return as a JSON list of strings.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.model_name,\n",
    "            messages=[{'role': 'user', 'content': query_prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            queries = json.loads(response['message']['content'])\n",
    "        except:\n",
    "            # Fallback to simple query generation\n",
    "            queries = [\n",
    "                topic,\n",
    "                f\"{topic} latest developments\",\n",
    "                f\"{topic} challenges and opportunities\"\n",
    "            ]\n",
    "        \n",
    "        # Perform searches\n",
    "        all_results = []\n",
    "        for query in queries[:num_searches]:\n",
    "            print(f\"Searching for: {query}\")\n",
    "            results = web_search(query, max_results=3)\n",
    "            all_results.extend(results)\n",
    "            time.sleep(1)  # Be nice to the search API\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def generate_section(self, topic: str, section_title: str, research_data: List[Dict]) -> ReportSection:\n",
    "        \"\"\"\n",
    "        Generate a specific section of the report\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Create a report section about: {section_title}\n",
    "        Topic: {topic}\n",
    "        \n",
    "        Use the following research data:\n",
    "        {json.dumps(research_data[:5], indent=2)}\n",
    "        \n",
    "        Make the content informative and well-structured.\n",
    "        Include specific examples and data points from the research.\n",
    "        \"\"\"\n",
    "        \n",
    "        return generate_structured_output(prompt, ReportSection)\n",
    "    \n",
    "    def generate_report(self, topic: str, sections: List[str] = None) -> StructuredReport:\n",
    "        \"\"\"\n",
    "        Generate a complete structured report on a topic\n",
    "        \"\"\"\n",
    "        print(f\"Generating report on: {topic}\")\n",
    "        \n",
    "        # Default sections if not provided\n",
    "        if sections is None:\n",
    "            sections = [\n",
    "                \"Overview and Current State\",\n",
    "                \"Key Developments and Trends\",\n",
    "                \"Challenges and Opportunities\",\n",
    "                \"Future Outlook\"\n",
    "            ]\n",
    "        \n",
    "        # Research the topic\n",
    "        print(\"\\nResearching topic...\")\n",
    "        research_data = self.research_topic(topic)\n",
    "        \n",
    "        # Generate sections\n",
    "        report_sections = []\n",
    "        for section_title in sections:\n",
    "            print(f\"\\nGenerating section: {section_title}\")\n",
    "            section = self.generate_section(topic, section_title, research_data)\n",
    "            report_sections.append(section)\n",
    "        \n",
    "        # Generate executive summary and conclusion\n",
    "        summary_prompt = f\"\"\"\n",
    "        Based on the following report sections about \"{topic}\", \n",
    "        create an executive summary that highlights the key findings.\n",
    "        \n",
    "        Sections:\n",
    "        {json.dumps([s.dict() for s in report_sections], indent=2)}\n",
    "        \n",
    "        Make it concise but comprehensive.\n",
    "        \"\"\"\n",
    "        \n",
    "        summary_response = ollama.chat(\n",
    "            model=self.model_name,\n",
    "            messages=[{'role': 'user', 'content': summary_prompt}]\n",
    "        )\n",
    "        \n",
    "        conclusion_prompt = f\"\"\"\n",
    "        Based on the report about \"{topic}\", write a strong conclusion that:\n",
    "        1. Summarizes the main findings\n",
    "        2. Provides actionable insights\n",
    "        3. Suggests areas for future research or attention\n",
    "        \"\"\"\n",
    "        \n",
    "        conclusion_response = ollama.chat(\n",
    "            model=self.model_name,\n",
    "            messages=[{'role': 'user', 'content': conclusion_prompt}]\n",
    "        )\n",
    "        \n",
    "        # Create the final report\n",
    "        report = StructuredReport(\n",
    "            title=f\"Comprehensive Report: {topic}\",\n",
    "            executive_summary=summary_response['message']['content'],\n",
    "            sections=report_sections,\n",
    "            conclusion=conclusion_response['message']['content']\n",
    "        )\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display report nicely\n",
    "def display_report(report: StructuredReport):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\\n{report.title}\")\n",
    "    print(f\"\\nGenerated: {report.generated_at}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    \n",
    "    print(f\"\\n\\nEXECUTIVE SUMMARY\")\n",
    "    print(f\"\\n{report.executive_summary}\")\n",
    "    \n",
    "    for section in report.sections:\n",
    "        print(f\"\\n\\n{'-'*60}\")\n",
    "        print(f\"\\n{section.title.upper()}\")\n",
    "        print(f\"\\n{section.content}\")\n",
    "        \n",
    "        if section.key_points:\n",
    "            print(f\"\\n**Key Points:**\")\n",
    "            for point in section.key_points:\n",
    "                print(f\"  • {point}\")\n",
    "    \n",
    "    print(f\"\\n\\n{'-'*60}\")\n",
    "    print(f\"\\nCONCLUSION\")\n",
    "    print(f\"\\n{report.conclusion}\")\n",
    "    print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Example Applications\n",
    "\n",
    "Let's use our report generator to create some example reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report on: Artificial Intelligence in Healthcare 2024\n",
      "\n",
      "Researching topic...\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model \"llama3.1\" not found, try pulling it first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Example 1: Technology Report\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m generator \u001b[39m=\u001b[39m ReportGenerator()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tech_report \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mgenerate_report(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mArtificial Intelligence in Healthcare 2024\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     sections\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mCurrent Applications in Healthcare\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mRecent Breakthroughs and Innovations\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mRegulatory and Ethical Considerations\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mMarket Analysis and Future Trends\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m display_report(tech_report)\n",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Research the topic\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mResearching topic...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m research_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresearch_topic(topic)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# Generate sections\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m report_sections \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Generate search queries\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m query_prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mGenerate \u001b[39m\u001b[39m{\u001b[39;00mnum_searches\u001b[39m}\u001b[39;00m\u001b[39m different search queries to research the topic: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtopic\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mMake the queries specific and diverse to cover different aspects.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mReturn as a JSON list of strings.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m response \u001b[39m=\u001b[39m ollama\u001b[39m.\u001b[39;49mchat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[{\u001b[39m'\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m: query_prompt}]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/llama2_oreilly_live_training/notebooks/extra-notebooks/structured-report-generation.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     queries \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/site-packages/ollama/_client.py:235\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[39mif\u001b[39;00m images \u001b[39m:=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    233\u001b[0m     message[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [_encode_image(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[0;32m--> 235\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_stream(\n\u001b[1;32m    236\u001b[0m   \u001b[39m'\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    237\u001b[0m   \u001b[39m'\u001b[39;49m\u001b[39m/api/chat\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    238\u001b[0m   json\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    239\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m: model,\n\u001b[1;32m    240\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m'\u001b[39;49m: messages,\n\u001b[1;32m    241\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m'\u001b[39;49m: tools \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m    242\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m'\u001b[39;49m: stream,\n\u001b[1;32m    243\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mformat\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mformat\u001b[39;49m,\n\u001b[1;32m    244\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39moptions\u001b[39;49m\u001b[39m'\u001b[39;49m: options \u001b[39mor\u001b[39;49;00m {},\n\u001b[1;32m    245\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mkeep_alive\u001b[39;49m\u001b[39m'\u001b[39;49m: keep_alive,\n\u001b[1;32m    246\u001b[0m   },\n\u001b[1;32m    247\u001b[0m   stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    248\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/site-packages/ollama/_client.py:98\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_request_stream\u001b[39m(\n\u001b[1;32m     93\u001b[0m   \u001b[39mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m   \u001b[39m*\u001b[39margs,\n\u001b[1;32m     95\u001b[0m   stream: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m   \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     97\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Mapping[\u001b[39mstr\u001b[39m, Any], Iterator[Mapping[\u001b[39mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 98\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-llama3/lib/python3.10/site-packages/ollama/_client.py:74\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m   response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 74\u001b[0m   \u001b[39mraise\u001b[39;00m ResponseError(e\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mtext, e\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseError\u001b[0m: model \"llama3.1\" not found, try pulling it first"
     ]
    }
   ],
   "source": [
    "# Example 1: Technology Report\n",
    "generator = ReportGenerator()\n",
    "tech_report = generator.generate_report(\n",
    "    \"Artificial Intelligence in Healthcare 2024\",\n",
    "    sections=[\n",
    "        \"Current Applications in Healthcare\",\n",
    "        \"Recent Breakthroughs and Innovations\",\n",
    "        \"Regulatory and Ethical Considerations\",\n",
    "        \"Market Analysis and Future Trends\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "display_report(tech_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Market Research Report\n",
    "market_report = generator.generate_report(\n",
    "    \"Electric Vehicle Market Analysis\",\n",
    "    sections=[\n",
    "        \"Market Size and Growth Projections\",\n",
    "        \"Key Players and Competition\",\n",
    "        \"Technology Trends and Innovations\",\n",
    "        \"Investment Opportunities and Risks\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "with open('ev_market_report.json', 'w') as f:\n",
    "    json.dump(market_report.dict(), f, indent=2)\n",
    "print(\"Report saved to ev_market_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Features\n",
    "\n",
    "Let's add some advanced features to make our report generator more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry logic for failed generations\n",
    "def generate_with_retry(func, max_retries=3, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Retry a function call if it fails\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "# Section validation\n",
    "def validate_section(section: ReportSection) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that a section meets quality standards\n",
    "    \"\"\"\n",
    "    if len(section.content) < 100:\n",
    "        print(\"Warning: Section content too short\")\n",
    "        return False\n",
    "    \n",
    "    if len(section.key_points) < 2:\n",
    "        print(\"Warning: Too few key points\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Enhanced report generator with validation\n",
    "class EnhancedReportGenerator(ReportGenerator):\n",
    "    def generate_section(self, topic: str, section_title: str, research_data: List[Dict]) -> ReportSection:\n",
    "        \"\"\"\n",
    "        Generate a section with validation and retry logic\n",
    "        \"\"\"\n",
    "        section = generate_with_retry(\n",
    "            super().generate_section,\n",
    "            max_retries=3,\n",
    "            topic=topic,\n",
    "            section_title=section_title,\n",
    "            research_data=research_data\n",
    "        )\n",
    "        \n",
    "        # Validate and regenerate if needed\n",
    "        if not validate_section(section):\n",
    "            print(f\"Regenerating section: {section_title}\")\n",
    "            enhanced_prompt = f\"\"\"\n",
    "            Create a DETAILED report section about: {section_title}\n",
    "            Topic: {topic}\n",
    "            \n",
    "            Requirements:\n",
    "            - Content must be at least 200 words\n",
    "            - Include at least 3 key points\n",
    "            - Use specific examples from the research data\n",
    "            \n",
    "            Research data:\n",
    "            {json.dumps(research_data[:5], indent=2)}\n",
    "            \"\"\"\n",
    "            section = generate_structured_output(enhanced_prompt, ReportSection)\n",
    "        \n",
    "        return section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom report templates\n",
    "REPORT_TEMPLATES = {\n",
    "    \"technical\": {\n",
    "        \"sections\": [\n",
    "            \"Technical Overview\",\n",
    "            \"Architecture and Implementation\",\n",
    "            \"Performance Analysis\",\n",
    "            \"Security Considerations\",\n",
    "            \"Integration Guidelines\"\n",
    "        ]\n",
    "    },\n",
    "    \"business\": {\n",
    "        \"sections\": [\n",
    "            \"Market Overview\",\n",
    "            \"Competitive Analysis\",\n",
    "            \"Revenue Models\",\n",
    "            \"Risk Assessment\",\n",
    "            \"Strategic Recommendations\"\n",
    "        ]\n",
    "    },\n",
    "    \"research\": {\n",
    "        \"sections\": [\n",
    "            \"Literature Review\",\n",
    "            \"Methodology\",\n",
    "            \"Key Findings\",\n",
    "            \"Discussion\",\n",
    "            \"Future Research Directions\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def generate_templated_report(topic: str, template: str = \"technical\"):\n",
    "    \"\"\"\n",
    "    Generate a report using a predefined template\n",
    "    \"\"\"\n",
    "    if template not in REPORT_TEMPLATES:\n",
    "        raise ValueError(f\"Unknown template: {template}\")\n",
    "    \n",
    "    generator = EnhancedReportGenerator()\n",
    "    sections = REPORT_TEMPLATES[template][\"sections\"]\n",
    "    \n",
    "    return generator.generate_report(topic, sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate a technical report\n",
    "technical_report = generate_templated_report(\n",
    "    \"Quantum Computing Applications\",\n",
    "    template=\"technical\"\n",
    ")\n",
    "\n",
    "# Export to markdown\n",
    "def export_to_markdown(report: StructuredReport, filename: str):\n",
    "    \"\"\"\n",
    "    Export report to markdown format\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"# {report.title}\\n\\n\")\n",
    "        f.write(f\"*Generated: {report.generated_at}*\\n\\n\")\n",
    "        f.write(f\"## Executive Summary\\n\\n{report.executive_summary}\\n\\n\")\n",
    "        \n",
    "        for section in report.sections:\n",
    "            f.write(f\"## {section.title}\\n\\n\")\n",
    "            f.write(f\"{section.content}\\n\\n\")\n",
    "            \n",
    "            if section.key_points:\n",
    "                f.write(\"### Key Points\\n\\n\")\n",
    "                for point in section.key_points:\n",
    "                    f.write(f\"- {point}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        f.write(f\"## Conclusion\\n\\n{report.conclusion}\\n\")\n",
    "\n",
    "export_to_markdown(technical_report, \"quantum_computing_report.md\")\n",
    "print(\"Report exported to quantum_computing_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "We've built a complete structured report generation system using basic Python and Ollama. Here's what we accomplished:\n",
    "\n",
    "1. **Structured Output**: Used Pydantic schemas to ensure consistent report formatting\n",
    "2. **Web Search Integration**: Added the ability to research topics using DuckDuckGo\n",
    "3. **Tool Calling**: Implemented a simple but effective tool calling system\n",
    "4. **Report Pipeline**: Created a complete pipeline for generating multi-section reports\n",
    "5. **Advanced Features**: Added validation, retry logic, and templates\n",
    "\n",
    "### Ideas for Extension:\n",
    "\n",
    "- Add more tools (Wikipedia search, arxiv papers, news APIs)\n",
    "- Implement citation tracking and source verification\n",
    "- Add support for generating charts and visualizations\n",
    "- Create a web interface for the report generator\n",
    "- Add support for different output formats (PDF, HTML)\n",
    "- Implement collaborative report generation with multiple LLMs\n",
    "- Add fact-checking and consistency validation\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- Local LLMs can be powerful tools for structured content generation\n",
    "- You don't need complex frameworks to build useful AI applications\n",
    "- Combining web search with LLMs creates more accurate and up-to-date content\n",
    "- Structured output ensures consistency and makes integration easier\n",
    "\n",
    "Happy report generating! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
