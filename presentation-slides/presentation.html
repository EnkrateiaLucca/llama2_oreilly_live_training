<!DOCTYPE html>
<html>
<head>
    <title>Presentation</title>
    <meta charset="utf-8">
    <style>
        @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
        @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
        @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

        body { font-family: 'Droid Serif'; }
        h1, h2, h3 {
            font-family: 'Yanone Kaffeesatz';
            font-weight: normal;
        }
        .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
</head>
<body>
<textarea id="source">
    class: center, middle
    
    # Getting Started with Llama3
    ## Lucas Soares
    ### 12-03-2025
    
    ---
    # Methodology Notes

    --
    1. Presentation Block

    --

    2. Notebook Demo

    --

    3. Quick Q&A + Summary

    --

    4. Optional Exercise During Q&A

    --

    5. Repeat

    ---
    
    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/text-prediction-blnks.png" style="width: 100%"> 

    ---
    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/chatgpt-text-prediction-1.png" style="width: 80%; margin-top: -10pt; margin-left: 80px; margin-top: -10pt; margin-left: 80px">

    ---

    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/chatgpt-text-prediction-2.png" style="width: 80%; margin-top: -10pt; margin-left: 80px">
    ---

    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/chatgpt-text-prediction-3.png" style="width: 80%; margin-top: -10pt; margin-left: 80px">
    ---

    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/chatgpt-text-prediction-4.png" style="width: 80%; margin-top: -10pt; margin-left: 80px">
    ---

    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/chatgpt-text-prediction-5.png" style="width: 80%; margin-top: -10pt; margin-left: 80px">
    ---

    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/chatgpt-text-prediction-6.png" style="width: 80%; margin-top: -10pt; margin-left: 80px">
    
    ---
    # LLMs Predict the Next Word

    <img src="../notebooks/assets-resources/chatgpt-text-prediction-7.png" style="width: 80%; margin-top: -10pt; margin-left: 80px">

    ---

    <img src="../notebooks/assets-resources/llm_predicts_pancakes.png" style="width: 100%;margin-top: 100pt;">

    ---
    
    <img src="../notebooks/assets-resources/llm-prob-distributions-context.png" style="width:100%; margin-top: 40pt;">

    ---
    # Introduction to Llama 3

    <img src="../notebooks/assets-resources/llama3-intro.png" width="90%">

    

    
    

    ---
    # Llama3 Releases
    
    - Meta Released Llama 3 in April of 2024

    - Llama 3.1 was released in July of 2024

    - Llama 3.2 was released in September of 2024

    - Llama 3.3 was released in December of 2024

    ---

    # Llama 3 Series

    - Open source with a Commercial license

    --

    - The latest and greatest: Llama3.3

    <img src="../notebooks/assets-resources/llama33-params.png" style="width: 100%;">

    <p style="font-size: 14px; margin-top: 200px; margin-left: 0px">
      <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct'>Hugging Face - Llama 3.3 70B Instruct</a>
    </p>

    ---
    # Llama 3.3 Evaluation Results

    <img src="../notebooks/assets-resources/llama33-evals.png" style="width: 70%;">

    <p style="font-size: 14px; margin-top: 0px; margin-left: 0px">
      <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct'>Hugging Face - Llama 3.3 70B Instruct</a>
    </p>

    ---
    # Llama 3.3 Evaluation Results

    <img src="../notebooks/assets-resources/llama33-evals2.png" style="width: 90%;">

    <p style="font-size: 14px; margin-top: 0px; margin-left: 0px">
      <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct'>Hugging Face - Llama 3.3 70B Instruct</a>
    </p>

    ---

    # Llama 3.3 Evaluation Results

    <img src="../notebooks/assets-resources/llama33-evals3.png" style="width: 90%;">

    <p style="font-size: 14px; margin-top: 0px; margin-left: 0px">
      <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct'>Hugging Face - Llama 3.3 70B Instruct</a>
    </p>

    ---

    # Llama 3.3 Evaluation Results

    <img src="../notebooks/assets-resources/llama33-evals4.png" style="width: 90%;">

    <p style="font-size: 14px; margin-top: 0px; margin-left: 0px">
      <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct'>Hugging Face - Llama 3.3 70B Instruct</a>
    </p>

    ---

    # Llama 3.3 Evaluation Results

    <img src="../notebooks/assets-resources/llama33-evals5.png" style="width: 90%;">

    <p style="font-size: 14px; margin-top: 0px; margin-left: 0px">
      <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct'>Hugging Face - Llama 3.3 70B Instruct</a>
    </p>

    ---
    # Llama 3 Resources


    - [Meta Llama Resources](https://ai.meta.com/llama/#resources)

    --

    - [Llama 3 Repo](https://github.com/meta-llama/llama-recipes)
    
    ---
    
    # Llama 3.1 405B Rivals Closed Source Models
    
    - System level approach for responsible development

    <p style="font-size: 14px; margin-top:50pt;">
      <a href='https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/'>Meta AI Llama3.2 Release Blog Post</a>
    </p>

    ---

    # From Llama 3.1 to Llama 3.2 with Knowledge Distillation & Pruning 

    <div style="text-align: center;">
      <img src="../notebooks/assets-resources/llama32-training.png" style="width: 90%; margin-top: 30pt; display: block; margin-left: auto; margin-right: auto;">
      <p style="font-size: 14px; margin-top: 20pt;">
        <a href='https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/'>Meta AI Llama3.2 Release Blog Post</a>
      </p>
    </div>
    
    ---

    # Training Llama 3.2 Vision Models

    --

    1. Architecture Adaptation:
      
      - Integrated image processing capabilities while preserving text abilities through adapter weights, cross-attention layers, and frozen language model parameters.

    --

    2. Training Pipeline:
      
      - Built upon Llama 3.1 models through pretraining on image-text pairs followed by domain-specific fine-tuning.

    --

    3. Post-Training Alignment:
        
      - Enhanced model safety and performance through supervised fine-tuning with rejection sampling and safety-focused data.


    
    ---
    class: center,middle

    <h1>
    <span style="background-color: lightgreen">
      Notebook Demo - Introduction to Llama3.2
    </span>
    </h1>

    ---

    # Query Your Docs Locally with Llama3.2

    - Need for LLMs with access to context-relevant data.

    <img src="../notebooks/assets-resources/private-qa-llama2.png" width="50%" style="margin-left: 30pt;">

    ---
    # Query Your Docs Locall with Llama3.2

    - Privacy concern with closed source LLMs.

    <img src="../notebooks/assets-resources/llama2-rag-intro.png" width="50%" style="margin-left: 30pt;">
    
    ---
    # Query Your Docs Locall with Llama3.2
    
    - Solution? Llama3.2!

    <img src="../notebooks/assets-resources/llama2-rag-limitations.png" width="50%" style="margin-left: 30pt;">

    ---
    # RAG with Llama3.2
    - RAG - Retrieval Augmented Generation
    <img src="../notebooks/assets-resources/llama2-rag-what-why.png" width="80%">

    ---
    # RAG with Llama3.2

    <img src="../notebooks/assets-resources/llama2-embeddings-content.png" width="80%">
    
    ---
    <img src="../notebooks/assets-resources/rag-llama3/rag-llama3.001.jpeg" style="width: 130%; margin-left:-100pt";>
    <p style="font-size: 14px; margin-top:50pt;">
        <a href='https://python.langchain.com/docs/use_cases/question_answering/'>Langchain Docs</a>
    </p>
    ---
    <img src="../notebooks/assets-resources/rag-llama3/rag-llama3.002.jpeg" style="width: 130%; margin-left:-100pt";>
    <p style="font-size: 14px; margin-top:50pt;">
        <a href='https://python.langchain.com/docs/use_cases/question_answering/'>Langchain Docs</a>
    </p>
    ---
    <img src="../notebooks/assets-resources/rag-llama3/rag-llama3.004.jpeg" style="width: 130%; margin-left:-100pt";>
    <p style="font-size: 14px; margin-top:50pt;">
        <a href='https://python.langchain.com/docs/use_cases/question_answering/'>Langchain Docs</a>
    </p>
    ---
    <img src="../notebooks/assets-resources/rag-llama3/rag-llama3.005.jpeg" style="width: 130%; margin-left:-100pt";>
    <p style="font-size: 14px; margin-top:50pt;">
        <a href='https://python.langchain.com/docs/use_cases/question_answering/'>Langchain Docs</a>
    </p>
    ---
    <img src="../notebooks/assets-resources/rag-llama3/rag-llama3.006.jpeg" style="width: 130%; margin-left:-100pt";>
    <p style="font-size: 14px; margin-top:50pt;">
        <a href='https://python.langchain.com/docs/use_cases/question_answering/'>Langchain Docs</a>
    </p>
    ---
    <img src="../notebooks/assets-resources/rag-llama3/rag-llama3.007.jpeg" style="width: 130%; margin-left:-100pt";>

    <p style="font-size: 14px; margin-top:50pt;">
      <a href='https://python.langchain.com/docs/use_cases/question_answering/'>Langchain Docs</a>
    </p>

    ---

    <img src="../notebooks/assets-resources/rag-llama3-pipeline/rag-llama3-pipeline.001.jpeg" style="width: 130%; margin-left:-100pt";>
    ---
    <img src="../notebooks/assets-resources/rag-llama3-pipeline/rag-llama3-pipeline.002.jpeg" style="width: 130%; margin-left:-100pt";>
    ---
    <img src="../notebooks/assets-resources/rag-llama3-pipeline/rag-llama3-pipeline.003.jpeg" style="width: 130%; margin-left:-100pt";>
    ---
    <img src="../notebooks/assets-resources/rag-llama3-pipeline/rag-llama3-pipeline.004.jpeg" style="width: 130%; margin-left:-100pt";>
    ---
    <img src="../notebooks/assets-resources/rag-llama3-pipeline/rag-llama3-pipeline.005.jpeg" style="width: 130%; margin-left:-100pt";>
    ---
        <img src="../notebooks/assets-resources/rag-llama3-pipeline/rag-llama3-pipeline.006.jpeg" style="width: 130%; margin-left:-100pt";>

    ---
    class: center, middle
    
    <h1>
    <span style="background-color: lightgreen">
      Notebook Demo - Local RAG with Llama 3.2
    </span>
    </h1>
    
    ---
    class: center, middle
    
    # Q&A / Break
    
    ---
    # Local Agents with Llama 3.2

    --
      
    <img src="../notebooks/assets-resources/agent-loop.png" width="1500">

    <p class="footnote">
        <a href='https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/'> Explanation of the agent loop in cognitive architectures. </a>
    </p>

    ---

    # Practical Use Case: Customer Support Agent

    --
    
    - **Scenario**: An LLM-powered customer support agent.

    --
    
    - **User Input**: Customer asks about order status.

    --
    
    - **LLM Decision**: Determines if it can provide the status directly or if it needs to fetch data from the database.

    --
    
    - **Action Taken**: If data fetch is needed, the agent queries the database and updates the user with the order status.

    <p class="footnote">
        <a href='https://langchain-ai.github.io/langgraph/concepts/high_level/#deployment'> Practical use case of LLM agents in customer support. </a>
    </p>

    ---
    class: center, middle

      <h1>
      <span style="background-color: lightgreen">
        Notebook Demo - Tool Calling and local agents with Llama 3.2
      </span>
      </h1>
    ---
    class: center, middle

    # Q&A / Break

    ---
    class: center, middle

    # Fine Tunning Llama3.2

    ---
    <img src="../notebooks/assets-resources/finetuning-llama3/finetuning-llama3.001.jpeg" style="width: 130%; margin-left:-100pt";>

    <p style="font-size: 14px; margin-top: 50pt;">
        <a href='https://arxiv.org/pdf/2307.09288'>LLama2 Paper</a>
      </p>
    ---
    <img src="../notebooks/assets-resources/finetuning-llama3/finetuning-llama3.002.jpeg" style="width: 130%; margin-left:-100pt";>

    <p style="font-size: 14px; margin-top: 50pt;">
        <a href='https://www.youtube.com/watch?v=g68qlo9Izf0&t=2935s'>Efficient Fine-Tuning for Llama-v2-7b on a Single GPU</a>
      </p>
    ---
    <img src="../notebooks/assets-resources/finetuning-llama3/finetuning-llama3.003.jpeg" style="width: 130%; margin-left:-100pt";>

    <p style="font-size: 14px; margin-top: 50pt;">
        <a href='https://www.youtube.com/watch?v=g68qlo9Izf0&t=2935s'>Efficient Fine-Tuning for Llama-v2-7b on a Single GPU</a>
      </p>
    ---
    <img src="../notebooks/assets-resources/finetuning-llama3/finetuning-llama3.004.jpeg" style="width: 130%; margin-left:-100pt";>

    <p style="font-size: 14px; margin-top: 50pt;">
      <a href='https://www.youtube.com/watch?v=g68qlo9Izf0&t=2935s'>Efficient Fine-Tuning for Llama-v2-7b on a Single GPU</a>
    </p>
    ---
    <img src="../notebooks/assets-resources/finetuning-llama3/finetuning-llama3.005.jpeg" style="width: 130%; margin-left:-100pt";>
    ---
    <img src="../notebooks/assets-resources/finetuning-llama3/finetuning-llama3.006.jpeg" style="width: 130%; margin-left:-100pt";>

    <p style="font-size: 14px; margin-top: 50pt;">
      <a href='https://arxiv.org/pdf/2106.09685'>Lora Paper</a>
    </p>
    ---
    class: center, middle

      <h1>
      <span style="background-color: lightgreen">
        Notebook Demo - Fine-Tuning Llama3.1 - Walkthrough
      </span>
      </h1>



</textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js">
</script>
<script>
var slideshow = remark.create();
</script>
</body>
</html>